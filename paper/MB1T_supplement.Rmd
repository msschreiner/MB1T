---
title             : "Manybabies1 Test-Retest Supplementary Materials"
shorttitle        : "MB1T supplementary"

figsintext        : yes
floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
toc: true 

bibliography      : ["r-references.bib"]

documentclass     : "apa6"
classoption       : "man, donotrepeattitle"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library(papaja)
library(dplyr)
library(tidyr)
library(readxl)
library(lme4)
library(readr)
library(langcog)
library(ggthemes)
library(tidyverse)
library(here)
library(knitr)
library(kableExtra)
library("langcog")
library(lmerTest)
library(ggplot2)
library(cowplot)
library(tidymodels)
library(car)
library(metafor)

read_path <- here("data","processed")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed,echo = FALSE, warning = FALSE, message = FALSE)
```

\newpage
# S1. Notes on and deviations from the preregistration

Below, we have compiled a list of notes on and deviations from the preregistered methods and analyses [https://osf.io/v5f8t](https://osf.io/v5f8t).

- All infants with usable data for both test and retest session were included in the analyses, regardless of the number of total of infants a lab was able to contribute after exclusion. This decision is consistent with past decisions in ManyBabies projects to be as inclusive about data inclusion as possible [@manybabies2020quantifying].
- A small number of infants with a time between sessions above 31 days were also included in the analyses (*n* = 3).
- Consistent with analytic decisions in ManyBabies 1 [@manybabies2020quantifying], total looking times were truncated at 18 seconds (the maximum trial time) in the small number of cases where recorded looking times were slightly greater than 18s (presumably due to small measurement error in recording infant looking times).
- In assessing differences in IDS preference between test and retest sessions, we preregistered an additional linear mixed-effects model including a by-lab random slope for session. This model yielded qualitatively equivalent results (see R markdown analysis script for the main manuscript). However, the model resulted in a singular fit, suggesting that the model specification may be overly complex and that its estimates should be interpreted with caution. We therefore focused only on the first preregistered model (including only by-lab and by-participant random intercepts) in reporting the analyses in the main manuscript.
- In assessing the reliability of IDS using a linear-mixed-effects model predicting IDS preference in session 2 from IDS preference in session 1, we also assessed the robustness of the results by fitting a second preregistered model with more complex random effects structure, including a by-lab random slope for IDS preference in session 1. This model is included in the main R markdown script and yields qualitatively equivalent results to the model reported in the manuscript that includes a by-lab random intercept only.
- We report a series of secondary planned analyses in the Supplementary Materials exploring potential moderating variables of time between test sessions (S2.1), the language background of the participants (S2.2.), and participant age (S2.3.). 
- We did not fit all models (in particular, the models investigating interactions between moderators) described in the secondary analyses of the preregistration, because our final sample size was smaller than we anticipated, which made it less feasible to investigate more complex relationships between moderators.

\newpage
# S2. Secondary analyses investigating possible moderating variables

```{r}

#read in cleaned and aggregated data
data_clean <- read_csv(here(read_path,"clean_data_minimum_trials_per_type_1.csv"))
agg_subjects <- read_csv(here(read_path,"all_agg_subjects_paired_minimum_trials_per_type_1.csv"))
all_agg_subjects_paired <- read_csv(here(read_path,"all_agg_subjects_paired_minimum_trials_per_type_1.csv")) 
all_agg_subjects_paired_retest <- read_csv(here(read_path,"all_agg_subjects_paired_retest_minimum_trials_per_type_1.csv")) %>%
  mutate(Pref_Test=case_when(
             Diff_1 > 0 ~ "IDS_preference",
             Diff_1 < 0 ~ "ADS_preference",
             Diff_1 == 0 ~ "no_preference"
             ),
         Pref_Retest=case_when(
             Diff_2 > 0 ~ "IDS_preference",
             Diff_2 < 0 ~ "ADS_preference",
             Diff_2 == 0 ~ "no_preference"
             )) %>%
  mutate(reversal_binary = case_when((Pref_Test == "IDS_preference" & Pref_Retest == "IDS_preference")  ~ 0,
                              (Pref_Test == "ADS_preference" & Pref_Retest == "ADS_preference")  ~ 0,
                              (Pref_Test == "IDS_preference" & Pref_Retest == "ADS_preference")  ~ 1,
                              (Pref_Test == "ADS_preference" & Pref_Retest == "IDS_preference")  ~ 1)) %>%
  unite(reversal_type,Pref_Test,Pref_Retest,remove=FALSE) %>%
  mutate(reversal_type=str_remove_all(reversal_type,pattern="_preference"))
```

## S2.1. Time between test sessions

```{r}
#mean-center Diff_1 and age_between_sessions to make main effects interpretable
all_agg_subjects_paired_retest <- all_agg_subjects_paired_retest %>%
  ungroup() %>%
  mutate(Diff_1_c = Diff_1-mean(Diff_1),
         days_between_sessions_c = days_between_sessions-mean(days_between_sessions))

# Adding days between sessions to model
mod_lmer_2.1 <- lmer(Diff_2 ~ Diff_1_c*days_between_sessions_c+ (1+ Diff_1_c|Lab), # Effect of days between sessions
              data=all_agg_subjects_paired_retest,REML=T)

# Note models with more complex random effects structure did not converge (singular boundary fit), though they yielded qualitatively equivalent results.
## random effects structure: (1+ Diff_1_c*days_between_sessions_c|Lab) --> singular boundary fit
## random effects structure: (1+ Diff_1_c+days_between_sessions_c|Lab) --> singular boundary fit

days_coefs<-summary(mod_lmer_2.1)$coefficients

```

The number of days between the first and second testing session varied widely across participants (mean: `r round(mean(all_agg_subjects_paired_retest$days_between_sessions),1)` days; range: `r min(all_agg_subjects_paired_retest$days_between_sessions)` - `r max(all_agg_subjects_paired_retest$days_between_sessions)` days). We therefore tested for the possibility that the time between sessions might have an impact on the reliability. We fit a linear mixed-effects model predicting IDS preference in session 2 from IDS preference in session 1 (mean-centered), number of days between testing sessions (mean-centered), and their interaction, including a by-lab random intercept and random slope for IDS preference in test session 1 (more complex random effects structure including additional random slopes for number of days between test sessions and its interaction with IDS preference in session 1 did not converge). We found no evidence that number of days between test sessions moderated the relationship between IDS preference at test session 1 and 2. Neither the main effect of time between sessions, $\beta$=`r round(days_coefs[3,1],3)`, *SE*=`r round(days_coefs[3,2],2)`, *t*(`r round(days_coefs[3,3],1)`)=`r round(days_coefs[3,4],2)`, *p*=`r printp(days_coefs[3,5])`, nor the interaction term, $\beta$=`r round(days_coefs[4,1],3)`, *SE*=`r round(days_coefs[4,2],2)`, *t*(`r round(days_coefs[4,3],1)`)=`r round(days_coefs[4,4],2)`, *p*=`r printp(days_coefs[4,5])`, showed significant effects.

## S2.2. Language Background

```{r}
# Differences in test-retest reliability between English and non-English learning infants
# Assigning 1 to UBC and Madison as NAE labs. Remaining labs are non-NAE and are coded as 0.
all_agg_subjects_paired_retest$NAE<-ifelse(all_agg_subjects_paired_retest$Lab=="InfantCog-UBC"| all_agg_subjects_paired_retest$Lab=="infantll-madison", 1, 0)

#center NAE to make main effects interpretable
all_agg_subjects_paired_retest <- all_agg_subjects_paired_retest %>%
  ungroup() %>%
  mutate(NAE_c = ifelse(NAE==1,0.5,-0.5))

# Adding NAE to model
mod_lmer_2.2 <- lmer(Diff_2 ~ Diff_1_c*NAE_c+(1|Lab),
              data=all_agg_subjects_paired_retest,REML=T) 

# Note models with more complex random effects structure did not converge (singular boundary fit), though they yielded qualitatively equivalent results.
## random effects structure: (1+ Diff_1_c|Lab) --> singular boundary fit

nae_coefs<-summary(mod_lmer_2.2)$coefficients
```

```{r fig1, fig.cap = "Infants' preference in Session 1 and Session 2 with individual data points and regression lines color-coded by method (central fixation, eye-tracking, or HPP). Results are plotted separately for North American English-learning infants (right panel) and infants learning other languages and dialects (right panel)."}

# Plotting IDS preference in Session 1 and Session 2 split by NAE
NAE.labs <- c("non-NAE", "NAE")
names(NAE.labs) <- c("0", "1")

all_agg_subjects_paired_retest$NAE <- as.factor(all_agg_subjects_paired_retest$NAE)

ggplot(all_agg_subjects_paired_retest, aes(x = Diff_1, y = Diff_2, col = Method)) + 
  geom_point() +
  geom_smooth(method = "lm", se=FALSE) + 
 # geom_hline(yintercept = 0, lty = 2) + 
  facet_grid(~NAE, labeller=labeller(NAE=NAE.labs))+
  ylab("IDS preference in second session") + 
  scale_color_ptol(name = "Method") + 
  #scale_linetype(name = "North American English") + 
  xlab("IDS preference in first session") + 
 # ylim(-3, 5) + 
  theme(legend.position = "bottom", legend.box = "vertical",panel.background = element_rect(fill = "white",
                                colour = "black",
                                size = 0.5, linetype = "solid"),
  panel.grid.major = element_blank(), 
  panel.grid.minor = element_blank(),
  strip.background = element_blank())

```

NAE-learning infants showed greater IDS preferences than their non-NAE counterparts in MB1.
We therefore also assessed if test-retest reliability interacted with children’s language background.
A linear mixed-effects model predicting IDS preference in Session 2 based on IDS preference in Session 1 (mean-centered), NAE (centered) and their interaction, including Lab as a random intercept, revealed no interaction, $\beta$=`r round(nae_coefs[4,1],2)`, *SE*=`r round(nae_coefs[4,2],2)`, *t*(`r round(nae_coefs[4,3],1)`)=`r round(nae_coefs[4,4],2)`, *p*=`r printp(nae_coefs[4,5])` (Figure 1).

## S2.3. Participant age

```{r}
# Age model
#mean-center age to make main effects interpretable
all_agg_subjects_paired_retest <- all_agg_subjects_paired_retest %>%
  ungroup() %>%
  mutate(Age_c = Age-mean(Age))

# Adding days between sessions to model
mod_lmer_2.3 <- lmer(Diff_2 ~ Diff_1_c*Age_c+ (1+ Diff_1_c|Lab), # Effect of days between sessions
              data=all_agg_subjects_paired_retest,REML=T)

# Note models with more complex random effects structure did not converge (singular boundary fit), though they yielded qualitatively equivalent results.
## random effects structure: (1+ Diff_1_c*Age_c|Lab) --> singular boundary fit
## random effects structure: (1+ Diff_1_c+Age_c|Lab) --> singular boundary fit

age_coefs<-summary(mod_lmer_2.3)$coefficients

```

To investigate the possibility that age moderated test-retest reliability, we fit a linear mixed-effects model predicting predicting IDS preference in Session 2 from on IDS preference in Session 1 (mean-centered), participant age (mean-centered) and their interaction.
The model included a by-lab random intercept and a by-lab random slope for IDS preference in Session 1.
We found no evidence that age influenced test-retest reliability as indicated by the interaction between IDS preference in Session 1 and age, $\beta$=`r round(age_coefs[4,1],4)`, *SE*=`r round(age_coefs[4,2],4)`, *t*(`r round(age_coefs[4,3],1)`)=`r round(age_coefs[4,4],2)`, *p*=`r printp(age_coefs[4,5])`.

\newpage
# S3. Meta-analysis of test-retest reliability

```{r}
#compute correlation
diff_correlation_estimate <- function(df) {
  return(as.numeric(cor.test(df$Diff_1,df$Diff_2)$estimate))
}

#summarize correlations by lab
lab_correlations <- all_agg_subjects_paired_retest %>%
  select(Lab,Method, Diff_1,Diff_2) %>%
  ungroup() %>%
  group_by(Lab,Method) %>%
  nest() %>%
  mutate(
    correlation=purrr::map(data,diff_correlation_estimate),
  ) %>%
  unnest(cols=c(data,correlation)) %>%
  group_by(Lab,Method,correlation) %>%
  summarize(
    N=n()
  ) %>%
  mutate(
    Method=case_when(
      Method=="Eyetracking" ~ "ET",
      Method=="central fixation" ~ "CF",
      TRUE ~ Method
    )
  )

z_transformed_es <- escalc(measure = "ZCOR",ri=correlation,ni=N,data=lab_correlations,slab=paste(Lab,Method,sep=", "))
meta_model <- rma.mv(yi=yi, V=vi,random=list(~ 1 | Lab, ~ 1 | Method),data=z_transformed_es)
```

```{r fig2, fig.cap = "Forest plot of test-retest reliability effect sizes. Each row represents Fisher's z transformed correlation coefficient and 95% CI for a given lab and method (HPP = head-turn preference procedure; ET = eye-tracking; CF = central fixation). The black diamond represents the overall estimated effect size from the mixed-effects meta-analytic model." }
forest(meta_model,header="Lab and Method")
```

In addition to the methods for assessing test-retest reliability reported in the main manuscript, we also investigated test-retest reliability across labs using a meta-analytic approach.
We used the metafor package [@R-metafor] to fit a mixed-effects meta-analytic model on z-transformed correlations for each combination of lab and method using sample size weighting. 
The model included random intercepts for lab and method.
The overall effect size estimate was not significantly different from zero, *b* = `r round(meta_model$beta[1],2)`, 95% CI = [`r round(meta_model$ci.lb,2)`, `r round(meta_model$ci.ub,2)`], *p* = `r round(meta_model$pval,2)`. 
A forest plot of the effect sizes for each lab and method is shown in Figure 2.

\newpage
# S4. Alternative Dependent Variables

To check the robustness of our results, we also investigated whether we obtained similar results with other possible dependent measures: average log-transformed looking times and a proportion-based preference measure. For each alternative dependent variable, we conducted the main analyses of test-retest reliability reported in the manuscript: the overall Pearson correlation, the test-retest linear mixed-effects model, and an inspection of applying stricter inclusion criteria for number of trials contributed.

## S4.1. Log-transformed looking times

```{r}
# calculate simple correlation of log-looking-time-based difference score of session 1 and difference score of session 2
simplyOverallCorr_log_lt<-cor.test(all_agg_subjects_paired_retest$Diff_log_lt_session_1,
         all_agg_subjects_paired_retest$Diff_log_lt_session_2)
```

In these analyses, we calculated IDS preference by first log-transforming looking times for each trial, computing the average log-transformed looking time for IDS and ADS for each participant, and calculating the difference between average IDS and ADS log-transforming looking times.
We fit a linear mixed-effects model predicting IDS preference in Session 2 from IDS preference in Session 1, including a by-lab random intercept. 
As in the analyses using average raw looking times, the results revealed no significant relationship between IDS preference in Session 1 and 2 (Table 1). 
The Pearson correlation coefficient was also not statistically significant, `r apa_print(simplyOverallCorr_log_lt)$full_result`.
Applying successively stricter inclusion criteria --- by requiring a higher number of valid trials per condition in each session --- showed a similar pattern to the main manuscript, such that correlations increased somewhat with stricter inclusion criteria, but substantially reduced the sample size at the same time (Figure 3).

```{r results="asis",warning=FALSE,message=FALSE}
# Linear mixed model predicting log-looking-time-based difference score of session 2 from difference score of session 1
mod_lmer_log_lt <- lmer(Diff_log_lt_session_2 ~ Diff_log_lt_session_1 + (1|Lab),
              data=all_agg_subjects_paired_retest,REML=T)

# Coefficients for linear mixed model predicting log-looking-time-based difference score of session 2 from difference score of session 1
coefs <- summary(mod_lmer_log_lt)$coef %>%
  as_data_frame %>%
  mutate_at(c("Estimate","Std. Error","df", "t value", "Pr(>|t|)"), 
            function (x) signif(x, digits = 3)) %>%
  rename(SE = `Std. Error`, 
         t = `t value`,
         p = `Pr(>|t|)`) %>%
  select(-df)
         
rownames(coefs) <- c("Intercept", "Log LT IDS Preference Session 1")

papaja::apa_table(coefs, 
                  caption = "Coefficient estimates from a linear mixed effects model predicting Log LT IDS preference in Session 2.", 
                  format.args = list(digits = 2))
```


```{r fig.cap="IDS preferences (based on average log-looking times) of both sessions plotted against each other for each inclusion criterion. n indicates the number of included infants, r is the Pearson correlation coefficient as the indicator for reliability.", fig.align="center",out.width = "5in",fig.height = 8, fig.width = 8}

#read datasets with different inclusion criteria
all_agg_subjects_paired_retest_2 <- read_csv(here(read_path,"all_agg_subjects_paired_retest_minimum_trials_per_type_2.csv"))
all_agg_subjects_paired_retest_4 <- read_csv(here(read_path,"all_agg_subjects_paired_retest_minimum_trials_per_type_4.csv"))
all_agg_subjects_paired_retest_6 <- read_csv(here(read_path,"all_agg_subjects_paired_retest_minimum_trials_per_type_6.csv"))
all_agg_subjects_paired_retest_8 <- read_csv(here(read_path,"all_agg_subjects_paired_retest_minimum_trials_per_type_8.csv"))

# Creating scatterplot for each inclusion criterion
plot_exclusion_dataset <- function(current_dataset,minimum_trials=1,
                                   Limaxes=c(-4,10),
                                   session_1_column="Diff_1",
                                   session_2_column="Diff_2",
                                   xlab = "IDS preference in first session",
                                   ylab = "IDS preference in second session"
                                   ) {
  current_correlation <- cor.test(current_dataset[[session_1_column]],current_dataset[[session_2_column]])
  main_message <- paste0("Inclusion criterion: ",minimum_trials," trial pairs")
  plot(current_dataset[[session_1_column]],current_dataset[[session_2_column]],xlab = xlab,ylab=ylab,main = main_message,xlim = Limaxes,ylim = Limaxes)
abline(lm(current_dataset[[session_2_column]]~current_dataset[[session_1_column]]),col="red")
legend("topleft",legend = paste("r = ",round(current_correlation$estimate,2),"; n = ",current_correlation$parameter+2,sep=""))
}

par(mfrow = c(2, 2))
plot_exclusion_dataset(all_agg_subjects_paired_retest_2,minimum_trials=2,Limaxes=c(-1,1.5),session_1_column="Diff_log_lt_session_1",session_2_column="Diff_log_lt_session_2",xlab = "IDS preference (log looking time) in Session 1", ylab="IDS preference (log looking time) in Session 2")
plot_exclusion_dataset(all_agg_subjects_paired_retest_4,minimum_trials=4,Limaxes=c(-1,1.5),session_1_column="Diff_log_lt_session_1",session_2_column="Diff_log_lt_session_2",xlab = "IDS preference (log looking time) in Session 1", ylab="IDS preference (log looking time) in Session 2")
plot_exclusion_dataset(all_agg_subjects_paired_retest_6,minimum_trials=6,Limaxes=c(-1,1.5),session_1_column="Diff_log_lt_session_1",session_2_column="Diff_log_lt_session_2",xlab = "IDS preference (log looking time) in Session 1", ylab="IDS preference (log looking time) in Session 2")
plot_exclusion_dataset(all_agg_subjects_paired_retest_8,minimum_trials=8,Limaxes=c(-1,1.5),session_1_column="Diff_log_lt_session_1",session_2_column="Diff_log_lt_session_2",xlab = "IDS preference (log looking time) in Session 1", ylab="IDS preference (log looking time) in Session 2")
```


## S4.2. Proportion looking to IDS

```{r results="asis",warning=FALSE,message=FALSE}
# Linear mixed model predicting proportion-based IDS preference of session 2 from proportion novelty preference of session 1
mod_lmer_prop <- lmer(Prop_session_2 ~ Prop_session_1 + (1|Lab),
              data=all_agg_subjects_paired_retest,REML=T)

# Coefficients for linear mixed model
coefs <- summary(mod_lmer_prop)$coef %>%
  as_data_frame %>%
  mutate_at(c("Estimate","Std. Error","df", "t value", "Pr(>|t|)"), 
            function (x) signif(x, digits = 3)) %>%
  rename(SE = `Std. Error`, 
         t = `t value`,
         p = `Pr(>|t|)`) %>%
  select(-df)
         
rownames(coefs) <- c("Intercept", "IDS Preference (proportion measure) Session 1")

papaja::apa_table(coefs, 
                  caption = "Coefficient estimates from a linear mixed effects model predicting IDS preference (based on proportion IDS looking) in Session 2.", 
                  format.args = list(digits = 2))
```

```{r}
# calculate simple correlation of difference score of session 1 and difference score of session 2
simplyOverallCorr_prop<-cor.test(all_agg_subjects_paired_retest$Prop_session_1,
         all_agg_subjects_paired_retest$Prop_session_2)
```

Next, we calculated a proportion-based IDS preference measure by computing the average proportion (raw) looking time to IDS relative to total (raw) looking time to IDS and ADS for each subject (i.e., IDS looking time / (ADS looking time + IDS looking time)).
We fit a linear mixed-effects model predicting proportion-based IDS preference in Session 2 from propotion-based IDS preference in Session 1, including a by-lab random intercept. 
As in the analyses using other measure of IDS preference, the results revealed no significant relationship between IDS preference in Session 1 and 2 (Table 2). 
The Pearson correlation coefficient based on proportional IDS looking was also not statistically significant, `r apa_print(simplyOverallCorr_prop)$full_result`.
Stricter inclusion criteria increased the correlation somewhat, as in previous analyses (Figure 4).

```{r fig.cap="IDS preferences (based on average log-looking times) of both sessions plotted against each other for each inclusion criterion. n indicates the number of included infants, r is the Pearson correlation coefficient as the indicator for reliability.", fig.align="center",out.width = "5in",fig.height = 8, fig.width = 8}
par(mfrow = c(2, 2))
plot_exclusion_dataset(all_agg_subjects_paired_retest_2,minimum_trials=2,Limaxes=c(0.25,0.8),session_1_column="Prop_session_1",session_2_column="Prop_session_2",xlab = "IDS preference (proportion-based) in Session 1", ylab="IDS preference (proportion-based) in Session 2")
plot_exclusion_dataset(all_agg_subjects_paired_retest_4,minimum_trials=4,Limaxes=c(0.25,0.8),session_1_column="Prop_session_1",session_2_column="Prop_session_2",xlab = "IDS preference (proportion-based) in Session 1", ylab="IDS preference (proportion-based) in Session 2")
plot_exclusion_dataset(all_agg_subjects_paired_retest_6,minimum_trials=6,Limaxes=c(0.25,0.8),session_1_column="Prop_session_1",session_2_column="Prop_session_2",xlab = "IDS preference (proportion-based) in Session 1", ylab="IDS preference (proportion-based) in Session 2")
plot_exclusion_dataset(all_agg_subjects_paired_retest_8,minimum_trials=8,Limaxes=c(0.25,0.8),session_1_column="Prop_session_1",session_2_column="Prop_session_2",xlab = "IDS preference (proportion-based) in Session 1", ylab="IDS preference (proportion-based) in Session 2")
```

\newpage

# S5. Sensitivity of test-retest reliability to trial number inclusion criteria

To conduct a more fine-grained analysis of how stricter trial inclusion criteria affect test-retest reliability, we computed correlations while gradually increasing the number of total valid trials required for inclusion.
For this analysis, we required a minimum of 1 IDS and 1 ADS trial and gradually increased the number of total valid trials required in both sessions (irrespective of IDS and ADS condition) from 2 to 16 (the maximum number of total trials).
Figure 5 depicts the Pearson correlation coefficients for increasingly stricter requirements for the overall trial numbers of a given participant in both sessions.
Correlations only increase and reach conventional levels of significance once the number of total required trials for both sessions is greater than 12.


```{r}
#function for filter based on number of overall trials and compute dataframe of correlation information
correlation_test_retest_min_trials <- function(dataset,minimum_trials=2) {
  #filter dataset
  current_dataset <- filter(dataset,total_trial_n_session_1>=minimum_trials&total_trial_n_session_2>=minimum_trials)
  #compute the test-retest correlation
  correlation <- cor.test(current_dataset$Diff_1,current_dataset$Diff_2)
  correlation_df <- data.frame(
    r = correlation$estimate, 
    t = correlation$statistic, 
    df=correlation$parameter, 
    p = correlation$p.value, 
    lower_ci = correlation$conf.int[1],
    upper_ci = correlation$conf.int[2],
    N = length(current_dataset$Subject_Unique))
  return(correlation_df)
}

# create correlations for each minimum number of trial exlcusion level
min_trials<-seq(2,16)
correlation_trial_exclusion_seq <- all_agg_subjects_paired_retest %>%
  #expand and then nest data frame for each level of minimum trial pairs
  expand_grid(min_trials=min_trials) %>%
  group_by(min_trials) %>%
  nest() %>%
  #now aggregate data for each minimum trial exclusion level
  mutate(
    correlation = purrr::map2(data,min_trials,correlation_test_retest_min_trials)
  ) %>%
  select(-data) %>%
  unnest(cols=correlation)
```

```{r sfig5, fig.cap = "Pearson correlation coefficient with increasingly strict trial-level inclusion criteria. The x-axis depicts the required number of overall valid trials in both session 1 and session 2. Dots represent corresponding correlation coefficients, with 95 percent CIs. The sample size is shown above each dot."}
ggplot(correlation_trial_exclusion_seq,aes(x=min_trials,y=r))+
  geom_hline(yintercept=0,linetype="dashed")+
  geom_point(size=4)+
  geom_errorbar(aes(ymin=lower_ci,ymax=upper_ci),width=0)+
  xlab("Number of overall trials in each session required for inclusion")+
  ylab("Pearson correlation coefficient")+
  geom_text(aes(label=N,y=0.75),size=5)+
  ylim(-0.15,1)+
  scale_x_continuous(breaks=seq(2,16))+
  theme_cowplot()
                  
```


# S6. Patterns of preference across sessions

```{r,message=FALSE,warning=FALSE}

#overall percent reversals
reversal_tally <- all_agg_subjects_paired_retest %>%
  ungroup() %>%
  group_by(reversal_type) %>%
  tally()
total_consistent <- sum(filter(reversal_tally, reversal_type %in% c("IDS_IDS","ADS_ADS"))$n)
total_n <- sum(reversal_tally$n)
percent_consistent <- total_consistent/total_n 
```

We also conducted analyses to explore whether there were any patterns of preference reversal across test sessions.
While there was no strong correlation in the magnitude of IDS preference between test session 1 and test session 2, here we asked whether infants consistently expressed the same preference across test sessions.
Overall, `r round(percent_consistent,3)*100 `% of the infants had a consistent preference from test to retest session, indicating that infants were not more likely than chance to maintain their preference from test session 1 to test session 2 (exact binomial test; *p* = 
`r round(binom.test(total_consistent,total_n)$p.value,2)`).
Of the `r sum(reversal_tally$n)` total infants, `r round(filter(reversal_tally,reversal_type=="IDS_IDS")$n/sum(reversal_tally$n),3)*100`% of infants showed a consistent infant-directed speech preference and `r round(filter(reversal_tally,reversal_type=="ADS_ADS")$n/sum(reversal_tally$n),3)*100`% showed a consistent adult-directed speech preference.
`r round(filter(reversal_tally,reversal_type=="IDS_ADS")$n/sum(reversal_tally$n),3)*100`% of infants switched from an infant-directed speech preference at test session 1 to an adult-directed speech preference at test session 2 and `r round(filter(reversal_tally,reversal_type=="ADS_IDS")$n/sum(reversal_tally$n),3)*100`% switched from an adult-directed speech preference to an infant-directed speech preference. 


Next, we explored whether we could detect any systematic clustering of infants with distinct patterns of preference across the test and retest session.
We took a bottom-up approach and conducted a *k*-means clustering of the test-retest difference data (here using log-transformed looking time data).
We found little evidence of distinct clusters emerging from these groupings: the clusterings ranging from *k*=2 (2 clusters) to *k*=4 (4 clusters) appear to mainly track  whether participants are approximately above or below the mean looking time difference for test session 1 and test session 2 (Figure 6A).
The diagnostic elbow plot shows little evidence of a qualitative improvement as the number of clusters is increased, which suggests little evidence for a distinctive set of clusters of participants who showed similar patterns of looking across the test and retest sessions (Figure 6B).

```{r}
d <- all_agg_subjects_paired_retest %>% 
  column_to_rownames(var = "Subject_Unique") %>% 
  filter(!is.na(Diff_log_lt_session_1)) %>%
  ungroup() %>% 
  select(Diff_log_lt_session_1,Diff_log_lt_session_2)

kclusts <- 
  tibble(k = 1:6) %>%
  mutate(
    kclust = map(k, ~kmeans(d, .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, d)
  )

clusters <- 
  kclusts %>%
  unnest(cols = c(tidied))

assignments <- 
  kclusts %>% 
  unnest(cols = c(augmented)) %>%
  mutate(
    mean_diff_1=mean(Diff_log_lt_session_1),
    mean_diff_2=mean(Diff_log_lt_session_2),
  )

clusterings <- 
  kclusts %>%
  unnest(cols = c(glanced))
```

```{r fig6, fig.cap = " (A) Results from the k-means clustering analysis of IDS preference (based on average log looking times) in session 1 and 2 for different numbers of k  and (B) the corresponding elbow plot of the total within-cluster sum of squares. In (A), points represent indvidual participants' magnitude of looking time difference at test sessions 1 (x-axis) and 2 (y-axis). The solid line indicates no preference for IDS vs. ADS, the dotted lines indicate mean IDS preference at test session 1 and 2, respectively. Colors indicate clusters from the k-means clustering for different values of k."}
p1 <- 
  ggplot(assignments, aes(x = Diff_log_lt_session_1, y = Diff_log_lt_session_2)) +
  geom_point(aes(color = .cluster), alpha = 0.8) + 
  facet_wrap(~ k,ncol =2)+
  geom_hline(yintercept=0)+
  geom_vline(xintercept=0)+
  geom_hline(yintercept=assignments$mean_diff_2[1],linetype="dotted")+
  geom_vline(xintercept=assignments$mean_diff_1[1],linetype="dotted")+
  xlab("IDS preference (log-based) in first session")+
  ylab("IDS preference (log-based) in second session")+
  scale_color_brewer(name ="cluster", palette="Set1")+
  theme_cowplot()+
  theme(axis.text=element_text(size=11),axis.title=element_text(size=11))

p2 <- ggplot(clusterings, aes(k, tot.withinss)) +
  geom_line() +
  geom_point()+
  xlab("Number of clusters")+
  scale_x_continuous(breaks=seq(1,10))+
  ylab("Total within-cluster sum of squares")+
  theme_cowplot()

plot_grid(p1,p2,labels=c("A","B"),rel_widths=c(1.5,1))
```


# S7. Relationship between number of contributed trials in each session

```{r}
# importing dataset (after excluding session and trial errors)
df_all <- read_csv(here(read_path,"df_all_after_exclusions.csv"))

#some minor data cleaning
#not cleaning based on minimum number of trials in order to see even infants contributing at least one trial
df_all <- df_all %>%
  filter(Condition != "training") %>%
  filter(LT >= 2) # minimum LT for inclusion

# calculate number of trials that each participant contributes per session
df_all_trials <- df_all %>%
  group_by(Subject_Unique,Session, Method) %>%
  summarise(N_trials = length(trial_num)) %>%
  mutate(
    N_trials = as.numeric(N_trials), #probably not needed, just to be safe
    Method = as.factor(Method)) %>%
  mutate(
    Session = case_when(
      Session==1 ~ "Test",
      Session==2 ~ "Retest"
    )
  )

df_all_trials <- df_all_trials %>% 
  pivot_wider(names_from = Session,values_from = N_trials)

#compute correlation
cor_trials <- cor.test(df_all_trials$Test,df_all_trials$Retest)
cor_trials_out <- apa_print(cor_trials)
```

```{r sfig7, fig.cap = "Correlation between the number of trials contributed in session 1 and session 2. Each data point represents one infant. Colored lines represent linear fits for each method."}
# jittering individual data points in order to make overlapping data points visible
trials <- ggplot(df_all_trials, aes(x=Test, y=Retest))+
  geom_jitter(width=0.5,height=0.5,aes(colour = Method),alpha=0.5)+
  geom_smooth(aes(colour = Method,group=Method),method=lm, se=FALSE,alpha=0.3)+
  geom_smooth(method=lm, colour= "black")+
  scale_x_continuous(seq(0,16,2),name="Number of Trials During Session 1")+
  scale_y_continuous(seq(0,16,2),name="Number of Trials During Session 2")+
  theme_cowplot()+
  theme(legend.position=c(0.02,0.8))
#plot
trials
```

Are there stable individual differences in how likely an infant is to contribute a high number of trials?
To answer this question, we conducted an exploratory analysis investigating whether there is a relationship between the number of trials an infant contributed in session 1 and session 2. 
Do infants who contribute a higher number of trials during their first testing session also tend to contribute more trials during their second testing session? 
A positive correlation between trial numbers during the first and second session would indicate that their is some stability in a given infants' likelihood of remaining attentive throughout the experiment.
On the other hand, the absence of a correlation would indicate that the number of trials a given infant contributes is not predictive of how many trials they might contribute during their next session.

We found a strong positive correlation between number of trials contributed during the first and the second session `r cor_trials_out$full_result` (Figure 7). 
This result suggests that if infants contribute a higher number of trials in one session, compared to other infants, they are likely to contribute a higher number of trials in their next session.
This finding is consistent with the hypothesis that how attentive infants are throughout an experiment (and hence how many trials they contribute) is a stable individual difference, at least for some infant looking time tasks.
Researchers should therefore be mindful of the fact that decisions about including or excluding infants based on trials contributed may selectively sample a specific sub-set of the infant population they are studying [@debolt2020robust;@byers2021six].

# S8. Correlations in average looking times between sessions

```{r}
agg_by_subj_paired <- data_clean %>%
  select(Subject,Subject_Unique,Gender,Method, preterm,Session, Condition, trial_num,LT,Lab,days_between_sessions) %>% 
  group_by(Method, Lab,Subject, Subject_Unique, Gender, preterm,days_between_sessions,Session) %>%
  summarize(mean_lt=mean(LT),
            N=n()) %>%
  pivot_wider(names_from = Session, values_from = c(mean_lt,N))

corr_lt_p1 <- ggplot(agg_by_subj_paired,aes(mean_lt_1,mean_lt_2))+
  geom_point()+
  geom_smooth(method="lm",color="black")+
  theme_cowplot()+
  theme(legend.position="none")+
  xlab("Average Looking Time (ms) in first session")+
  ylab("Average Looking Time (ms) in second session")

correlation_avg_looking <- cor.test(agg_by_subj_paired$mean_lt_1,agg_by_subj_paired$mean_lt_2)

#similar results when controlling for trial number
m <- lm(mean_lt_2~mean_lt_1+N_1+N_2,data=agg_by_subj_paired)
results_control_model <- summary(m)
#car::vif(m) #no really worrying variance inflation here
```

To what extent are participants looking times between the two sessions related?
To test this question, we first investigated whether participants' overall looking times --- irrespective of condition --- were correlated between the first and second session.
There was a robust correlation between average looking time in Session 1 and Session 2: infants with longer looking times during their first session also tended to look longer during their second session, `r apa_print(correlation_avg_looking)$full_result`.
This relationship held even after controlling for number of trials in the first and second session, suggesting that the relationship in between average looking could not be entirely explained by correlation between number of trials contributed between the two sessions (S7), `r apa_print(m)$full_result[["mean_lt_1"]]` (Figure 8A).

```{r}
agg_by_subj_condition_paired <- data_clean %>%
  select(Subject,Subject_Unique,Gender,Method, preterm,Session, Condition, LT,Lab,days_between_sessions) %>% 
  group_by(Method, Lab,Subject, Subject_Unique, Gender, preterm,days_between_sessions,Condition,Session) %>%
  summarize(mean_lt=mean(LT)) %>%
  pivot_wider(names_from = Session, values_from = mean_lt) %>%
  rename(LT_Test = `1`,LT_Retest=`2`)

corr_lt_p2 <- ggplot(agg_by_subj_condition_paired,aes(LT_Test,LT_Retest,color=Condition))+
  geom_point()+
  geom_smooth(method="lm",color="black")+
  facet_wrap(~Condition)+
  theme_cowplot()+
  theme(legend.position="none")+
  xlab("Average Looking Time (in ms) in first session")+
  ylab("Average Looking Time (in ms) in second session")

agg_by_subj_condition_paired_wide <- agg_by_subj_condition_paired %>%
  pivot_wider(names_from = Condition,values_from = c(LT_Test,LT_Retest))

#condition-specific correlations
correlation_IDS_LT <- cor.test(agg_by_subj_condition_paired_wide$LT_Retest_IDS,agg_by_subj_condition_paired_wide$LT_Test_IDS)
correlation_ADS_LT <- cor.test(agg_by_subj_condition_paired_wide$LT_Retest_ADS,agg_by_subj_condition_paired_wide$LT_Test_ADS)

#predicting condition-specific session 2 looking from session 1 looking to IDS and ADS
predicting_Retest_IDS <- lm(LT_Retest_IDS ~ LT_Test_IDS + LT_Test_ADS,data=agg_by_subj_condition_paired_wide)
predicting_Retest_ADS <- lm(LT_Retest_ADS ~ LT_Test_IDS + LT_Test_ADS,data=agg_by_subj_condition_paired_wide)
```

Next, we explored the extent to which average looking times for IDS and ADS stimuli specifically were related.
First, we found similar correlations in average looking time to IDS stimuli in Session 1 and 2 (`r apa_print(correlation_IDS_LT)$full_result`; Figure 8B) and ADS stimuli in Session 1 and 2 (`r apa_print(correlation_ADS_LT)$full_result`).
To test whether these correlations were specific to looking times for IDS or ADS stimuli alone, we fit linear regression models predicting average looking to IDS (or ADS) stimuli in Session 2 from average looking to IDS and ADS stimuli in Session 1.
We found that average looking to IDS stimuli in Session 2 could be predicted from average looking to IDS stimuli in Session 1, even after controlling for average looking to ADS stimuli in Session 1, `r apa_print(predicting_Retest_IDS)$full_result[["LT_Test_IDS"]]`.
Conversely, average looking to ADS stimuli in Session 2 could be predicted from average looking to ADS stimuli in Session 1, even after controlling for average looking to IDS stimuli in Session 1, `r apa_print(predicting_Retest_ADS)$full_result[["LT_Test_ADS"]]`.
These results suggest that the condition-specific correlations in average looking time cannot be fully explained by the fact that infants' overall looking times between sessions are correlated.

```{r}
agg_by_subj_item_paired <- data_clean %>%
    select(Subject,Subject_Unique,Gender,Method, preterm,Session, Condition, stimulus, LT,Lab,days_between_sessions) %>% 
  group_by(Method, Lab,Subject, Subject_Unique, Gender, preterm,days_between_sessions,Condition,stimulus) %>%
  mutate(stimulus = str_remove(stimulus,"Retest")) %>%
  mutate(stimulus = str_remove(stimulus,"retest")) %>%
  filter(stimulus %in% c("ADS1","IDS1","ADS2","IDS2","ADS3","IDS3","ADS4", "IDS4","IDS5","ADS5","ADS6","IDS6","ADS7","IDS7","ADS8","IDS8")) %>%
  pivot_wider(names_from = Session, values_from = LT) %>%
  rename(LT_Test = `1`,LT_Retest=`2`)

corr_lt_p3 <- ggplot(agg_by_subj_item_paired,aes(LT_Test,LT_Retest,color=Condition))+
  geom_point()+
  geom_smooth(method="lm",color="black")+
  facet_wrap(~stimulus)+
  theme_cowplot()+
  theme(legend.position="none")+
  xlab("Looking Time (in ms) in first session")+
  ylab("Looking Time (in ms) in second session")

item_lt_relationship <- lmer(LT_Retest~LT_Test+(1+LT_Test|Subject_Unique)+(1|stimulus)+(1+LT_Test|Lab),data=agg_by_subj_item_paired, control=lmerControl(optimizer="bobyqa"))
#model including a by-stimulus slope for LT_Test has a singular fit but yields similar results
```

Finally, we inspected item-level correlations between the two test sessions.
Specifically, we investigated the relation between items composed of the same recording clips in Session 1 and Session 2 (but with a reversed order of clips between the two sessions).
We fit a linear mixed-effects model predicting item-level looking time in Session 2 from item-level looking time in Session 1, including random intercepts for participant, item, and lab, as well as an  random slope for item-level looking time in Session 1 for participant and lab.
Item-level looking in Session 2 was related to item-level looking in Session 1, `r apa_print(item_lt_relationship)$full_result[["LT_Test"]]` (Figure 8C).
Similar results hold if looking times are log-transformed

```{r fig.cap="Correlations in average looking time (in ms) between Session 1 and 2 (A) overall, (B) by condition, and (C) by item.", fig.align="center",fig.height = 16, fig.width = 12}
top_row <- plot_grid(corr_lt_p1,corr_lt_p2, labels=c("A","B"),ncol=2)
plot_grid(top_row,corr_lt_p3,labels=c("","C"),nrow=2,rel_heights=c(1,2))
```

# S9. By-item-pair preference scores across sessions

```{r}
agg_by_subj_item_paired_pref <- data_clean  %>%
  select(Subject,Subject_Unique,Gender,Method, preterm,Session, Condition, stimulus, LT,Lab) %>% 
  mutate(stimulus = str_remove(stimulus,"Retest")) %>%
  mutate(stimulus = str_remove(stimulus,"retest")) %>%
  filter(stimulus %in% c("ADS1","IDS1","ADS2","IDS2","ADS3","IDS3","ADS4", "IDS4","IDS5","ADS5","ADS6","IDS6","ADS7","IDS7","ADS8","IDS8")) %>%
  separate(stimulus,into = c("cond","stimulus_num"),sep="(?<=[A-Za-z])(?=[0-9])",remove=TRUE) %>%
  select(-cond) %>%
  group_by(Method, Lab,Subject, Subject_Unique, Gender, preterm,Session,stimulus_num) %>%
  pivot_wider(names_from = Condition,values_from=LT) %>%
  mutate(Diff = IDS - ADS, 
           Prop = IDS / (IDS + ADS)) %>%
  pivot_wider(names_from = Session, values_from =c(IDS,ADS,Diff,Prop))

# ggplot(agg_by_subj_item_paired_pref ,aes(Prop_1,Prop_2))+
#   geom_point()+
#   geom_smooth(method="lm",color="black")+
#   facet_wrap(~stimulus_num,ncol=4)+
#   theme_cowplot()+
#   theme(legend.position="none")+
#   xlab("IDS preference in first session")+
#   ylab("IDS preference in second session")

ggplot(agg_by_subj_item_paired_pref ,aes(Diff_1,Diff_2))+
  geom_point()+
  geom_smooth(method="lm",color="black")+
  facet_wrap(~stimulus_num,ncol=4)+
  theme_cowplot()+
  theme(legend.position="none")+
  xlab("IDS difference score in first session")+
  ylab("IDS difference score in second session")

m <- lmer(Diff_2~Diff_1+(1+Diff_1|Subject_Unique)+(1+Diff_1|stimulus_num)+(1+Diff_1|Lab),data=agg_by_subj_item_paired_pref)
apa_lm <- apa_print(m)
apa_table(
  apa_lm$table
  , caption = "Mixed-effects model results predicting IDS preference during session 1 from IDS preference at session 2 at the stimulus level."
)
```


\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup

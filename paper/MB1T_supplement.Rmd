---
title             : "ManyBabies1 Test-Retest Supplementary Materials"
shorttitle        : "MB1T supplementary"

figsintext        : yes
floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
toc: true 

bibliography      : ["r-references.bib"]

documentclass     : "apa6"
classoption       : "man, donotrepeattitle"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library(papaja)
library(dplyr)
library(tidyr)
library(readxl)
library(lme4)
library(readr)
library(langcog)
library(ggthemes)
library(tidyverse)
library(here)
library(knitr)
library(kableExtra)
library("langcog")
library(lmerTest)
library(ggplot2)
library(cowplot)
library(tidymodels)
library(car)
library(metafor)
library(ggExtra)
library(corx)
library(cower)
library(r2mlm)
library(performance)
read_path <- here("data","processed")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed,echo = FALSE, warning = FALSE, message = FALSE)
RUN_SIMULATIONS = FALSE
```

\newpage
# S1. Notes on and deviations from the preregistration

## S1.1. Deviations from the preregistration

Below, we have compiled a list of deviations from the preregistered methods and analyses available at [https://osf.io/v5f8t](https://osf.io/v5f8t).

- All infants with usable data for both test and retest session were included in the analyses, regardless of the number of total infants a lab was able to contribute after exclusion. This decision is consistent with past decisions in ManyBabies projects to be as inclusive about data retention as possible [@manybabies2020quantifying].
- A small number of infants whose time between sessions exceeded 31 days were still included in the analyses (*n* = 3). We included these participants for two reasons. First, the general philosophy in ManyBabies studies has been to err on the side of being inclusive, as long as the data from a given participant adds valid information to the study in question. Secondly, time between test session varied continuously across participants and we planned to assess the impact of time between test on reliability. We expected that including these participants should (if anything) provide additional information (and statistical power) by extending the range of a continuous predictor variable (time between test sessions) in our moderator analyses.
- Consistent with analytic decisions in ManyBabies1 [hereafter, MB1, @manybabies2020quantifying], total looking times were truncated at 18 seconds (the maximum trial time) in the small number of cases where recorded looking times were slightly greater than 18s (presumably due to small measurement error in recording infant looking times).
- In assessing differences in IDS preference between test and retest sessions, we preregistered an additional linear mixed-effects model including a by-lab random slope for session. This model yielded qualitatively equivalent results (see R markdown of the main manuscript). However, the model resulted in a singular fit, suggesting that the model specification may be overly complex and that its estimates should be interpreted with caution. We therefore focused only on the first preregistered model (including only by-lab and by-participant random intercepts) in reporting the analyses in the main manuscript.
- In assessing the reliability of IDS using a linear mixed-effects model predicting IDS preference in Session 2 from IDS preference in Session 1, we also assessed the robustness of the results by fitting a second preregistered model with more complex random effects structure, including a by-lab random slope for IDS preference in Session 1. This model is included in the main R markdown script and yields qualitatively equivalent results to the model reported in the manuscript that includes a by-lab random intercept only.
- We report a series of secondary planned analyses in the Supplementary Materials exploring potential moderating variables of time between test sessions (S2.1.), participant age (S2.2.), method (S2.3.), and the language background of the participants (S2.4.).
- While we fit all models described in the secondary analyses of the preregistration, including models investigating interactions between moderators, we interpret the more complex, three-way interaction models with caution. Our final sample size was smaller than we anticipated, which made our sample less well-powered to investigate more complex relationships between moderators. Moreover, the baseline model for these secondary interaction models was incorrectly specified in the preregistration (lower-order terms for the moderator were incorrectly removed in the planned baseline model), and we opt instead to report estimates using the more conventional method of comparing parameters of interest to models including all predictors except the main predictor of interest (e.g., estimating significance of three-way interaction terms by comparing the model fit to a model including only all lower-order predictors).
- In the by-lab meta-analysis of test-retest reliability (S3), we also separated the data by method, such that the data from one lab was split into data from the head-turn preference procedure and the central fixation method. We separated the data by both lab and method because differences in IDS preference across methods were observed in MB1 and because this approach was more consistent with the analytic approach in the rest of the manuscript. This decision does not qualitatively affect the conclusions of the meta-analysis, namely that there was no consistent evidence of test-retest reliability.

## S1.2. Additional notes

While the original idea was to retest infants that contributed data to the original study, some labs had already finished data collection for MB1 but nevertheless agreed to collect a new set of data for the MB1 test-retest spin-off project. In addition, one lab already started data collection prior to the preregistration (both Session 1 and Session 2 data), however, this data had not been inspected or analyzed prior to the preregistration. We here present a detailed list of our collected data in relation to the original MB1 study and the preregistration (see Table 1).

```{r}
#read in meta data with additional information on each lab's data collection status in relation to the preregistration and to MB1
additional.notes <- read.csv(here(read_path,"mb1t_data_collection_info.csv"))

names(additional.notes)[c(3,4)]<-c("Collection prior to preregistration","MB1 as Session 1")
papaja::apa_table(additional.notes, 
                  caption = "Additional notes on data collection status of each lab in relation to preregistration and MB1.")
                  #format.args = list(digits = 0))
```

\newpage
# S2. Secondary analyses investigating possible moderating variables

```{r}

#read in cleaned and aggregated data
data_clean <- read_csv(here(read_path,"clean_data_minimum_trials_per_type_1.csv"))
agg_subjects <- read_csv(here(read_path,"all_agg_subjects_paired_minimum_trials_per_type_1.csv"))
all_agg_subjects_paired <- read_csv(here(read_path,"all_agg_subjects_paired_minimum_trials_per_type_1.csv")) 
all_agg_subjects_paired_retest <- read_csv(here(read_path,"all_agg_subjects_paired_retest_minimum_trials_per_type_1.csv")) %>%
  mutate(Pref_Test=case_when(
             Diff_1 > 0 ~ "IDS_preference",
             Diff_1 < 0 ~ "ADS_preference",
             Diff_1 == 0 ~ "no_preference"
             ),
         Pref_Retest=case_when(
             Diff_2 > 0 ~ "IDS_preference",
             Diff_2 < 0 ~ "ADS_preference",
             Diff_2 == 0 ~ "no_preference"
             )) %>%
  mutate(reversal_binary = case_when((Pref_Test == "IDS_preference" & Pref_Retest == "IDS_preference")  ~ 0,
                              (Pref_Test == "ADS_preference" & Pref_Retest == "ADS_preference")  ~ 0,
                              (Pref_Test == "IDS_preference" & Pref_Retest == "ADS_preference")  ~ 1,
                              (Pref_Test == "ADS_preference" & Pref_Retest == "IDS_preference")  ~ 1)) %>%
  unite(reversal_type,Pref_Test,Pref_Retest,remove=FALSE) %>%
  mutate(reversal_type=str_remove_all(reversal_type,pattern="_preference"))
```

## S2.1. Descriptives and power

### S2.1.1. Additional descriptive information

```{r fig.cap="Distribution of participant age for each lab and method. Method is highlighted by color and  language status is indicated by line type (solid = North American English; dashed = non-North American English).", fig.align="center"}
all_agg_subjects_paired_retest <- all_agg_subjects_paired_retest  %>%
  mutate(
    Method_short=case_when(
      Method=="Eyetracking" ~ "ET",
      Method=="central fixation" ~ "CF",
      TRUE ~ Method
    )
  ) %>%
  mutate(
    Method=case_when(
      Method=="Eyetracking" ~ "eye-tracking",
      TRUE ~ Method
    )
  ) %>%
  unite(lab_method,Lab,Method_short,sep=", ", remove=FALSE)
# Differences in test-retest reliability between English and non-English learning infants
# Assigning 1 to UBC and Madison as NAE labs. Remaining labs are non-NAE and are coded as 0.
all_agg_subjects_paired_retest$NAE<-ifelse(all_agg_subjects_paired_retest$Lab=="InfantCog-UBC"| all_agg_subjects_paired_retest$Lab=="infantll-madison", 1, 0)

all_agg_subjects_paired_retest <- all_agg_subjects_paired_retest %>%
  group_by(Lab,Method) %>%
  mutate(
    N = n(),
    `NAE Language` = ifelse(NAE==1,"NAE","non-NAE")
  )


ggplot(all_agg_subjects_paired_retest,aes(Age,color=Method))+
  theme_cowplot()+
  geom_density(aes(linetype=`NAE Language`))+
  facet_wrap(~lab_method)+
  scale_color_ptol(name="Method")+
  geom_label(aes(label=paste0("n = ",N)),x=300,y=0.0125,size=2,show.legend=FALSE)+
  theme(strip.text.x = element_text(size = 8))
```

To highlight the distributions of the key moderators of interest, we include an additional plot representing the distribution of infant age among the 7 participating labs, split by method and language background (Figure 1).

### S2.1.2. A note on (post-hoc) power

```{r}
#side analyses for approximate estimate of power to detect differences in two correlations
#used in response to reviewers but not reported in the supplement - retaining here for ease of reference
language_background_power <- power.indep.cor(r1 = 0.5, r2 = 0, n1 =37,n2=121)
method_power <- power.indep.cor(r1 = 0.43, r2 = 0, n1 =68,n2=90)
```

Our final sample size (*N* = 158) --- although quite large for typical infant looking time studies --- had limited power to detect moderation effects. As a heuristic for approximate post-hoc power, we can consider the power to detect differences between correlations for our final sample. For the moderator of language background, we had *n* = 37 participants with a North American English (hereafter, NAE) language background and *n* = 121 participants with non-North American English (hereafter, non-NAE) backgrounds. Given this sample size, differences between the two samples would have to be substantial in order to have reasonable power to detect a difference: assuming *r* = 0 for one sample, we would only reach 80% power to detect a difference if *r* ~ 0.5 for the other sample. We had slightly more power to detect differences for method, where we had *n* = 68 HPP observations and *n* = 90 non-HPP observations. For example, again assuming *r* = 0 for one sample, we would reach 80% power to detect differences once *r* ~ 0.43 for the second sample. Given the limited power to detect all but large effect sizes in our moderation analyses, we planned to treat any significant results from the moderator analyses with caution.

## S2.2. Time between test sessions

### S2.2.1. Reliability moderated by time between test sessions

```{r}
#mean-center Diff_1 and age_between_sessions to make main effects interpretable
all_agg_subjects_paired_retest <- all_agg_subjects_paired_retest %>%
  ungroup() %>%
  mutate(Diff_1_c = Diff_1-mean(Diff_1),
         days_between_sessions_c = days_between_sessions-mean(days_between_sessions))

# Adding days between sessions to model
mod_lmer_2.1.1 <- lmer(Diff_2 ~ Diff_1_c*days_between_sessions_c+ (1+ Diff_1_c|Lab), # Effect of days between sessions
              data=all_agg_subjects_paired_retest,REML=T)

# Note models with more complex random effects structure did not converge (singular boundary fit), though they yielded qualitatively equivalent results.
## random effects structure: (1+ Diff_1_c*days_between_sessions_c|Lab) --> singular boundary fit
## random effects structure: (1+ Diff_1_c+days_between_sessions_c|Lab) --> singular boundary fit

days_coefs<-summary(mod_lmer_2.1.1)$coefficients

```

The number of days between the first and second testing session varied widely across participants (mean: `r round(mean(all_agg_subjects_paired_retest$days_between_sessions),1)` days; range: `r min(all_agg_subjects_paired_retest$days_between_sessions)` - `r max(all_agg_subjects_paired_retest$days_between_sessions)` days). We therefore tested for the possibility that the time between sessions might have an impact on test-retest reliability. We fit a linear mixed-effects model predicting IDS preference in Session 2 from IDS preference in Session 1 (mean-centered), number of days between testing sessions (mean-centered), and their interaction, including a by-lab random intercept and random slope for IDS preference in Session 1.
A more complex random effects structure including additional random slopes for number of days between test sessions and its interaction with IDS preference in Session 1 did not converge.
We found no evidence that the number of days between test sessions moderated the relationship between IDS preference in Session 1 and 2. Neither the main effect of time between sessions, $\beta$=`r round(days_coefs[3,1],3)`, *SE*=`r round(days_coefs[3,2],2)`, *t*(`r round(days_coefs[3,3],1)`)=`r round(days_coefs[3,4],2)`, *p*=`r printp(days_coefs[3,5])`, nor the interaction term, $\beta$=`r round(days_coefs[4,1],3)`, *SE*=`r round(days_coefs[4,2],2)`, *t*(`r round(days_coefs[4,3],1)`)=`r round(days_coefs[4,4],2)`, *p*=`r printp(days_coefs[4,5])`, showed significant effects.

### S2.2.2. Change in preferential looking moderated by time between test sessions

```{r}
#APLT ~ time * time_between + (1+ time+time_between| lab) + (1+subject)
agg_subjects <- agg_subjects %>%
  mutate(
    session_c=Session-1.5
  ) %>%
  group_by(Subject_Unique) %>%
  mutate(
    days_between_sessions = Age[Session==2]-Age[Session==1]
  ) %>%
  ungroup() %>%
  mutate(
    days_between_sessions_c = days_between_sessions-mean(days_between_sessions)
  )

mod_lmer_2.1.2 <- lmer(Diff ~ session_c*days_between_sessions_c + (1|Lab)+(1|Subject_Unique),
                       data=agg_subjects)
# Note models with more complex random effects structure did not converge (singular boundary fit)
## random effects structure: (1+session_c*days_between_sessions_c|Lab) --> singular boundary fit
## random effects structure: (1+ session_c+days_between_sessions_c|Lab) --> singular boundary fit
## random effects structure: (1+ days_between_sessions_c|Lab) --> singular boundary fit
## random effects structure: (1+ session_c|Lab) --> singular boundary fit

change_days_coefs<-summary(mod_lmer_2.1.2)$coefficients
```

In addition to assessing the influence of moderators on test-retest reliability, we also tested whether the difference in magnitude of the IDS preference between Session 1 and Session 2 depended on moderators of interest.
To investigate the influence of time between test sessions, we fit a linear mixed-effects model predicting average IDS preference from Session (centered; Session 1 vs. Session 2), days between test sessions (mean-centered), and their interaction.
We included by-lab and by-participant random intercepts (more complex random effects structures did not converge due to singular fits).
We found no evidence that the change in preferential looking to IDS between Session 1 and Session 2 was moderated by days between test sessions, $\beta$=`r round(change_days_coefs[4,1],3)`, *SE*=`r round(change_days_coefs[4,2],3)`, *t*(`r round(change_days_coefs[4,3],1)`)=`r round(change_days_coefs[4,4],2)`, *p*=`r printp(change_days_coefs[4,5])`.

## S2.3. Participant age

### S2.3.1. Reliability moderated by participant age

```{r}
# Age model
#mean-center age to make main effects interpretable
all_agg_subjects_paired_retest <- all_agg_subjects_paired_retest %>%
  ungroup() %>%
  mutate(Age_c = Age-mean(Age))

# Adding age to model
mod_lmer_2.2.1 <- lmer(Diff_2 ~ Diff_1_c*Age_c+ (1+ Diff_1_c|Lab), # effect of age
              data=all_agg_subjects_paired_retest,REML=T)

# Note models with more complex random effects structure did not converge (singular boundary fit), though they yielded qualitatively equivalent results.
## random effects structure: (1+ Diff_1_c*Age_c|Lab) --> singular boundary fit
## random effects structure: (1+ Diff_1_c+Age_c|Lab) --> singular boundary fit

age_coefs<-summary(mod_lmer_2.2.1)$coefficients

```

To investigate the possibility that age moderated test-retest reliability, we fit a linear mixed-effects model predicting IDS preference in Session 2 from IDS preference in Session 1 (mean-centered), participant age (mean-centered), and their interaction.
The model included a by-lab random intercept and a by-lab random slope for IDS preference in Session 1.
We found no evidence that age influenced test-retest reliability as indicated by the interaction between IDS preference in Session 1 and age, $\beta$=`r round(age_coefs[4,1],3)`, *SE*=`r round(age_coefs[4,2],3)`, *t*(`r round(age_coefs[4,3],1)`)=`r round(age_coefs[4,4],2)`, *p*=`r printp(age_coefs[4,5])`.

### S2.3.2. Change in preferential looking moderated by participant age

```{r}
#APLT ~ time * ageC + (1+ time+ageC| lab) + (1+subject)
agg_subjects <- agg_subjects %>%
  mutate(
    Age_c=Age-mean(Age)
  )


mod_lmer_2.2.2 <- lmer(Diff ~ session_c*Age_c + (1|Lab)+(1|Subject_Unique),
                       data=agg_subjects)
# Note models with more complex random effects structure did not converge (singular boundary fit)
## random effects structure: (1+session_c*Age_c|Lab) --> singular boundary fit
## random effects structure: (1+ session_c+Age_c|Lab) --> singular boundary fit
## random effects structure: (1+ session_c|Lab) --> singular boundary fit
## random effects structure: (1+ Age_c|Lab) --> singular boundary fit

change_age_coefs<-summary(mod_lmer_2.2.2)$coefficients
```

To investigate the potential of moderators to influence the overall magnitude of the IDS effect between Session 1 and 2, we fit a linear mixed-effects model predicting average IDS preference from Session (centered; Session 1 vs. Session 2), participant age (mean-centered), and their interaction.
We included by-lab and by-participant random intercepts (more complex random effects structures did not converge due to singular fits).
We found no evidence that the change in preferential looking to IDS between Session 1 and Session 2 was moderated by participant age, $\beta$=`r round(change_age_coefs[4,1],3)`, *SE*=`r round(change_age_coefs[4,2],3)`, *t*(`r round(change_age_coefs[4,3],1)`)=`r round(change_age_coefs[4,4],2)`, *p*=`r printp(change_age_coefs[4,5])`.

## S2.4. Method

### S2.4.1. Differences in IDS preference across method

```{r}
diff_session_method <- lmer(Diff ~ Session + Method+
                (1 | Lab) + (1 | Subject), 
              data=all_agg_subjects_paired, REML=FALSE)
chi_sq_test_session_method <- Anova(diff_session_method,type="III")
```

In MB1, infants who participated in the head-turn preference procedure (hereafter, HPP) showed a significantly larger magnitude of IDS preference, compared to central fixation (hereafter, CF) and eye-tracking (hereafter, ET) methods. Therefore, in the current study, we also explored whether the magnitude of IDS preference differed as a function of method. We fit a linear mixed-effects model predicting IDS preference from Session and Method (dummy-coded, with central fixation as the reference level), including by-lab and by-participant random intercepts. We found no significant difference in IDS preference across methods, ${\chi}^2$(2)=`r round(chi_sq_test_session_method$Chisq[3],2)`, *p*=`r printp(chi_sq_test_session_method[3,3])`.

### S2.4.2. Reliability moderated by method

```{r}
all_agg_subjects_paired_retest <- all_agg_subjects_paired_retest %>%
  mutate(
    Method=case_when(
      Method=="Eyetracking" ~ "eye-tracking",
      TRUE ~ Method
    )
  )

# Adding method to the model
mod_lmer_2.3.1 <- lmer(Diff_2 ~ Diff_1_c*Method + (1+ Diff_1_c|Lab), # effect of method
              data=all_agg_subjects_paired_retest,REML=T)

# Note models with more complex random effects structure did not converge (singular boundary fit), though they yielded qualitatively equivalent results.
## random effects structure: (1+ Diff_1_c*Method|Lab) --> singular boundary fit
## random effects structure: (1+ Diff_1_c+Method|Lab) --> singular boundary fit

#overall chi-squared test to assess the omnibus interaction effect between test at time 1 and method
overall_chisq_test_method <- Anova(mod_lmer_2.3.1,type="III")
method_coefs<-summary(mod_lmer_2.3.1)$coefficients
```

We tested whether method (ET vs. CF vs. HPP) moderated test-retest reliability by fitting a linear mixed-effects model predicting IDS preference in Session 2 from IDS preference in Session 1 (mean-centered), Method (dummy-coded, with central fixation as the reference level) and their interaction.
The model included a by-lab random intercept and a by-lab random slope for IDS preference in Session 1 (models with more complex random effects structure including by-lab random effects for Method did not converge).
We found no evidence that Method influenced test-retest reliability as indicated by the interaction between IDS preference in Session 1 and age, ${\chi}^2$(2)=`r round(overall_chisq_test_method$Chisq[4],2)`, *p*=`r printp(overall_chisq_test_method[4,3])`.

### S2.4.3. Reliability and its interaction with both method and age

```{r}
#interaction including both age and method
mod_lmer_2.3.2 <- lmer(Diff_2 ~ Diff_1_c*Method*Age_c + (1+ Diff_1_c|Lab),
              data=all_agg_subjects_paired_retest,REML=T)

#more complex random effects structures fail to converge
method_age_coefs<-summary(mod_lmer_2.3.2)$coefficients
overall_chisq_test_method_age <- Anova(mod_lmer_2.3.2,type="III")
```

```{r}
#compute reliability for older and younger infants by Method
#function for computing coefficient table from lmer model and correlation reliability
compute_coefs_reliability <- function(dataset) {
  #fit lmer reliability model
  mod_lmer <- lmer(Diff_2 ~ Diff_1 + (1|Lab),data=dataset,REML=T)
  #extract coefficients
  coef <- summary(mod_lmer)$coef %>%
    as_data_frame %>%
    mutate_at(c("Estimate","Std. Error","df", "t value", "Pr(>|t|)"), 
              function (x) signif(x, digits = 3)) %>%
    rename(SE = `Std. Error`, 
           t = `t value`,
           pvalue = `Pr(>|t|)`,
           estimate = Estimate) %>%
    #keep just main coefficient estimate
    slice(2) %>%
    #keep most important columns
    select(estimate, SE, pvalue)
  
  #compute correlation and add to summarized data
  correlation<-cor.test(dataset$Diff_1,dataset$Diff_2) 
  summarized_data <- coef %>%
    mutate(cor=as.numeric(correlation$estimate),
           df=as.numeric(correlation$parameter),
           pvalue2=as.numeric(correlation$p.value))
  
  #return
  summarized_data
}

#split by older and younger kids
all_agg_subjects_paired_retest <- all_agg_subjects_paired_retest %>%
  mutate(age_bin = cut(Age, breaks = 2, labels = c("younger","older")))

coef_table_method_age <- all_agg_subjects_paired_retest %>%
  group_by(Method,age_bin) %>%
  nest() %>%
  #compute coefficients
  mutate(coef_table = purrr::map(data, compute_coefs_reliability)) %>%
  #remove data
  select(-data) %>%
  #unpack
  unnest(cols=c(coef_table)) 
```

In a more complex linear mixed-effects model (preregistered as part of our planned secondary analyses) including the interaction between IDS preference in Session 1 (mean-centered), Method (dummy-coded, with central fixation as the reference level), participant age (mean-centered), and all lower order interactions, we find evidence for an interaction between method and age in predicting reliability, ${\chi}^2$(2)=`r round(overall_chisq_test_method_age$Chisq[8],2)`, *p*=`r printp(overall_chisq_test_method_age[8,3])`.
This effect appears to be mainly driven by older infants showing some evidence of test-retest reliability for HPP, *r* = `r round(filter(coef_table_method_age, Method=="HPP"&age_bin=="older")$cor,2)`, *p* = `r round(filter(coef_table_method_age, Method=="HPP"&age_bin=="older")$pvalue2,2)` (see Figure 3B).
However, we believe these tentative findings should be treated with caution, due to the small size of our infant sample once binned by multiple moderating factors.

### S2.4.4. Change in preferential looking moderated by age and method

```{r}
#APLT ~ time * ageC*method + (1+ test+ageC+method| lab) + (1+subject)
mod_lmer_2.3.3 <- lmer(Diff ~ session_c*Age_c*Method + (1|Subject_Unique),
                       data=agg_subjects)
# Note models with more complex random effects structure did not converge (singular boundary fit)
## random effects structure: (1+session_c*Age_c*Method|Lab) --> singular boundary fit
## random effects structure: (1+ session_c*Age_c+Method|Lab) --> singular boundary fit
## random effects structure: (1+ session_c+Age_c*Method|Lab) --> singular boundary fit
## random effects structure: (1+ session_c+Age_c+Method|Lab) --> singular boundary fit
## random effects structure: (1+ session_c*Age_c|Lab) --> singular boundary fit
## random effects structure: (1+ session_c*Method|Lab) --> singular boundary fit
## random effects structure: (1+ Age_c*Method|Lab) --> singular boundary fit
## random effects structure: (1+ session_c+Age_c|Lab) --> singular boundary fit
## random effects structure: (1+ session_c+Method|Lab) --> singular boundary fit
## random effects structure: (1+ Age_c+Method|Lab) --> singular boundary fit
## random effects structure: (1+ session_c|Lab) --> singular boundary fit
## random effects structure: (1+ Age_c|Lab) --> singular boundary fit
## random effects structure: (1+ Method|Lab) --> singular boundary fit
## random effects structure: (1|Lab) --> singular boundary fit

overall_chisq_test_change_age_method <- Anova(mod_lmer_2.3.3,type="III")
change_age_method_coefs<-summary(mod_lmer_2.3.3)$coefficients
```

We fit a linear mixed-effects model predicting average IDS preference from the three-way interaction of Session (centered; Session 1 vs. Session 2), participant age (mean-centered), Method (dummy-coded, with central fixation as the reference level), and all lower order predictors.
We included a by-participant random intercept (more complex random effects structures did not converge due to singular fits).
We found no evidence that the change in preferential looking to IDS between Session 1 and Session 2 was moderated by participant age and Method, $\beta$=`r round(change_age_method_coefs[12,1],3)`, *SE*=`r round(change_age_method_coefs[12,2],3)`, *t*(`r round(change_age_method_coefs[12,3],1)`)=`r round(change_age_method_coefs[12,4],2)`, *p*=`r printp(change_age_method_coefs[12,5])`.

## S2.5. Language background

### S2.5.1. Reliability moderated by language background

```{r}
#center NAE to make main effects interpretable
all_agg_subjects_paired_retest <- all_agg_subjects_paired_retest %>%
  ungroup() %>%
  mutate(NAE_c = ifelse(NAE==1,0.5,-0.5))

# Adding NAE to model
mod_lmer_2.4.1 <- lmer(Diff_2 ~ Diff_1_c*NAE_c+(1|Lab),
              data=all_agg_subjects_paired_retest,REML=T) 

# Note models with more complex random effects structure did not converge (singular boundary fit), though they yielded qualitatively equivalent results.
## random effects structure: (1+ Diff_1_c|Lab) --> singular boundary fit

nae_coefs<-summary(mod_lmer_2.4.1)$coefficients
```

```{r fig.cap = "Infants' preference in Session 1 and Session 2 with individual data points and regression lines color-coded by method (CF, ET, or HPP). Results are plotted separately for North American English-learning infants (right panel) and infants learning other languages and dialects (left panel)."}

# Plotting IDS preference in Session 1 and Session 2 split by NAE
NAE.labs <- c("non-NAE", "NAE")
names(NAE.labs) <- c("0", "1")

all_agg_subjects_paired_retest$NAE <- as.factor(all_agg_subjects_paired_retest$NAE)

ggplot(all_agg_subjects_paired_retest, aes(x = Diff_1, y = Diff_2, col = Method)) + 
  geom_point() +
  geom_smooth(method = "lm", se=FALSE) + 
 # geom_hline(yintercept = 0, lty = 2) + 
  facet_grid(~NAE, labeller=labeller(NAE=NAE.labs))+
  ylab("IDS preference in second session") + 
  scale_color_ptol(name = "Method") + 
  #scale_linetype(name = "North American English") + 
  xlab("IDS preference in first session") + 
 # ylim(-3, 5) + 
  theme(legend.position = "bottom", legend.box = "vertical",panel.background = element_rect(fill = "white",
                                colour = "black",
                                size = 0.5, linetype = "solid"),
  panel.grid.major = element_blank(), 
  panel.grid.minor = element_blank(),
  strip.background = element_blank())

```

NAE-learning infants showed greater IDS preferences than their non-NAE counterparts in MB1.
We therefore also assessed whether test-retest reliability interacted with children’s language background.
A linear mixed-effects model predicting IDS preference in Session 2 based on IDS preference in Session 1 (mean-centered), NAE (centered), and their interaction, including Lab as a random intercept, revealed no interaction, $\beta$=`r round(nae_coefs[4,1],2)`, *SE*=`r round(nae_coefs[4,2],2)`, *t*(`r round(nae_coefs[4,3],1)`)=`r round(nae_coefs[4,4],2)`, *p*=`r printp(nae_coefs[4,5])` (Figure 2).

### S2.5.2. Reliability and its interaction between language background and age

```{r}
# Adding NAE to model
mod_lmer_2.4.2 <- lmer(Diff_2 ~ Diff_1_c*NAE_c*Age_c+(1+Diff_1_c|Lab),
              data=all_agg_subjects_paired_retest,REML=T) 

# Note models with more complex random effects structure did not converge
nae_age_coefs<-summary(mod_lmer_2.4.2)$coefficients
```


```{r fig.cap = "Infants' preference in Session 1 and Session 2 with individual data points and regression lines color-coded by method for (A) younger and (B) older infants (median-split). Results are plotted separately for North American English-learning infants and infants learning other languages and dialects."}
p1 <- ggplot(filter(all_agg_subjects_paired_retest,age_bin=="younger"), aes(x = Diff_1, y = Diff_2, col = Method)) +
  geom_point() +
  geom_smooth(method = "lm", se=FALSE) + 
 # geom_hline(yintercept = 0, lty = 2) + 
  facet_grid(~NAE, labeller=labeller(NAE=NAE.labs))+
  ylab("IDS preference in second session") + 
  scale_color_ptol(name = "Method") + 
  xlab("IDS preference in first session") + 
 # ylim(-3, 5) + 
  theme(
    #legend.position = "bottom", 
    #legend.box = "vertical",
    panel.background = element_rect(fill = "white",
                                colour = "black",
                                size = 0.5, linetype = "solid"),
  #legend.title = element_text(size=8),
  #legend.text = element_text(size=7),
  panel.grid.major = element_blank(), 
  panel.grid.minor = element_blank(),
  strip.background = element_blank())+
  ggtitle("YOUNGER INFANTS") +
  theme(plot.title = element_text(hjust = 0.5))

p2 <- ggplot(filter(all_agg_subjects_paired_retest,age_bin=="older"), aes(x = Diff_1, y = Diff_2, col = Method)) +
  geom_point() +
  geom_smooth(method = "lm", se=FALSE) + 
 # geom_hline(yintercept = 0, lty = 2) + 
  facet_grid(~NAE, labeller=labeller(NAE=NAE.labs))+
  ylab("IDS preference in second session") + 
  scale_color_ptol(name = "Method") + 
  xlab("IDS preference in first session") + 
 # ylim(-3, 5) + 
  theme(
    #legend.position = "bottom", 
    #legend.box = "vertical",
        panel.background = element_rect(fill = "white",
                                colour = "black",
                                size = 0.5, linetype = "solid"),
  #legend.title = element_text(size=8),
  #legend.text = element_text(size=7),
  panel.grid.major = element_blank(), 
  panel.grid.minor = element_blank(),
  strip.background = element_blank())+
  ggtitle("OLDER INFANTS") +
  theme(plot.title = element_text(hjust = 0.5))

main_plot <- plot_grid(
  p1+theme(legend.position="none"),
  p2+theme(legend.position="none"),
  labels=c("A","B"))

legend <- get_legend(
  p1 + 
    guides(color = guide_legend(nrow = 1)) +
    theme(legend.position = "bottom",legend.box = "vertical",legend.title = element_text(size=12),legend.text = element_text(size=10))
)

plot_grid(main_plot, legend, ncol = 1, rel_heights = c(1, .1))
```

We also fit a preregistered linear mixed-effects model predicting IDS preference in Session 2 from the three-way interaction between IDS preference in Session 1 (mean-centered), NAE (centered), participant age (mean-centered), and all lower order interactions.
We find evidence for an interaction between language background and age in predicting reliability, $\beta$=`r round(nae_age_coefs[8,1],2)`, *SE*=`r round(nae_age_coefs[8,2],3)`, *t*(`r round(nae_age_coefs[8,3],1)`)=`r round(nae_age_coefs[8,4],2)`, *p*=`r printp(nae_age_coefs[8,5])`.
Figure 3 illustrates that this interaction was driven by a small set of older infants (all from a single lab and participating in the HPP method) showing a somewhat more reliable relationship between Session 1 and Session 2 looking.
Note that the mixed-effects analyses use Age as a continuous predictor --- age is median-split in Figure 3 to ease visualization.
Given the small number of infants driving the three-way interaction and the confounded nature of this sample (with method and lab), we do not draw strong conclusions from the existence of this three-way interaction, but report it here to spur future investigations into how age and experience interacts with test-retest reliability.

### S2.5.3. Change in preferential looking moderated by age and language background

```{r}
# Assigning 1 to UBC and Madison as NAE labs. Remaining labs are non-NAE and are coded as 0.
agg_subjects$NAE<-ifelse(agg_subjects$Lab=="InfantCog-UBC"| agg_subjects$Lab=="infantll-madison", 1, 0)

#center NAE to make main effects interpretable
agg_subjects <- agg_subjects %>%
  ungroup() %>%
  mutate(NAE_c = ifelse(NAE==1,0.5,-0.5))

#APLT ~ time * ageC*native + (1+ test+ageC+native| lab) + (1+subject)
mod_lmer_2.4.3 <- lmer(Diff ~ session_c*Age_c*NAE_c + (1+ session_c|Lab) + (1|Subject_Unique),
                       data=agg_subjects)
# Note models with more complex random effects structure did not converge (singular boundary fit)
## random effects structure: (1+session_c*Age_c*NAE_c|Lab) --> singular boundary fit
## random effects structure: (1+ session_c*Age_c+NAE_c|Lab) --> negative eigenvalue
## random effects structure: (1+ session_c+Age_c*NAE_c|Lab) --> negative eigenvalue
## random effects structure: (1+ session_c+Age_c+NAE_c|Lab) --> negative eigenvalue
## random effects structure: (1+ session_c*Age_c|Lab) --> singular boundary fit
## random effects structure: (1+ session_c*NAE_c|Lab) --> singular boundary fit
## random effects structure: (1+ Age_c*NAE_c|Lab) --> singular boundary fit/ negative EV
## random effects structure: (1+ session_c+Age_c|Lab) --> singular boundary fit
## random effects structure: (1+ session_c+NAE_c|Lab) --> singular boundary fit
## random effects structure: (1+ Age_c+NAE_c|Lab) --> singular boundary fit
## random effects structure: (1+ Age_c|Lab) --> singular boundary fit
## random effects structure: (1+ NAE_c|Lab) --> singular boundary fit, negative EV

change_age_language_coefs<-summary(mod_lmer_2.4.3)$coefficients
```

We fit a linear mixed-effects model predicting average IDS preference from the three-way interaction of Session (centered; Session 1 vs. Session 2), participant age (mean-centered), NAE (centered), and all lower order predictors.
We included by-lab and by-participant random intercepts and by-lab random slope for Session (more complex random effects structures did not converge due to singular fits).
We found no evidence that the change in preferential looking to IDS between Session 1 and Session 2 was moderated by participant age and language background, $\beta$=`r round(change_age_language_coefs[8,1],3)`, *SE*=`r round(change_age_language_coefs[8,2],3)`, *t*(`r round(change_age_language_coefs[8,3],1)`)=`r round(change_age_language_coefs[8,4],2)`, *p*=`r printp(change_age_language_coefs[8,5])`.

# S3. Meta-analysis of test-retest reliability

```{r}
#compute correlation
diff_correlation_estimate <- function(df) {
  return(as.numeric(cor.test(df$Diff_1,df$Diff_2)$estimate))
}

#summarize correlations by lab
lab_correlations <- all_agg_subjects_paired_retest %>%
  select(Lab,Method, Diff_1,Diff_2) %>%
  ungroup() %>%
  group_by(Lab,Method) %>%
  nest() %>%
  mutate(
    correlation=purrr::map(data,diff_correlation_estimate),
  ) %>%
  unnest(cols=c(data,correlation)) %>%
  group_by(Lab,Method,correlation) %>%
  summarize(
    N=n()
  ) %>%
  mutate(
    Method=case_when(
      Method=="eye-tracking" ~ "ET",
      Method=="central fixation" ~ "CF",
      TRUE ~ Method
    )
  )

z_transformed_es <- escalc(measure = "ZCOR",ri=correlation,ni=N,data=lab_correlations,slab=paste(Lab,Method,sep=", "))
meta_model <- rma.mv(yi=yi, V=vi,random=list(~ 1 | Lab, ~ 1 | Method),data=z_transformed_es)
```

```{r fig.cap = "Forest plot of test-retest reliability effect sizes. Each row represents Fisher's z transformed correlation coefficient and 95% CI for a given lab and method (HPP, ET, and CF). The black diamond represents the overall estimated effect size from the mixed-effects meta-analytic model." }
forest(meta_model,header="Lab and Method")
```

In addition to the methods for assessing test-retest reliability reported in the main manuscript, we also investigated test-retest reliability across labs using a meta-analytic approach.
We used the metafor package [@R-metafor] to fit a mixed-effects meta-analytic model on z-transformed correlations for each combination of lab and method using sample size weighting. 
The model included random intercepts for lab and method.
The overall effect size estimate was not significantly different from zero, *b* = `r round(meta_model$beta[1],2)`, 95% CI = [`r round(meta_model$ci.lb,2)`, `r round(meta_model$ci.ub,2)`], *p* = `r round(meta_model$pval,2)`. 
A forest plot of the effect sizes for each lab and method is shown in Figure 4.

# S4. Analyses including a more restricted sample

Given that we found that restricting the sample to participants contributing at least six ADS and IDS trials in both sessions increased test-retest reliability, we conducted the central analyses with this more restricted infant sample.

```{r}
agg_subjects_6 <- read_csv(here(read_path,"agg_subjects_minimum_trials_per_type_6.csv"))
all_agg_subjects_paired_6 <- read_csv(here(read_path,"all_agg_subjects_paired_minimum_trials_per_type_6.csv")) 
all_agg_subjects_paired_retest_6 <- read_csv(here(read_path,"all_agg_subjects_paired_retest_minimum_trials_per_type_6.csv")) %>%
  mutate(Pref_Test=case_when(
             Diff_1 > 0 ~ "IDS_preference",
             Diff_1 < 0 ~ "ADS_preference",
             Diff_1 == 0 ~ "no_preference"
             ),
         Pref_Retest=case_when(
             Diff_2 > 0 ~ "IDS_preference",
             Diff_2 < 0 ~ "ADS_preference",
             Diff_2 == 0 ~ "no_preference"
             )) %>%
  mutate(reversal_binary = case_when((Pref_Test == "IDS_preference" & Pref_Retest == "IDS_preference")  ~ 0,
                              (Pref_Test == "ADS_preference" & Pref_Retest == "ADS_preference")  ~ 0,
                              (Pref_Test == "IDS_preference" & Pref_Retest == "ADS_preference")  ~ 1,
                              (Pref_Test == "ADS_preference" & Pref_Retest == "IDS_preference")  ~ 1)) %>%
  unite(reversal_type,Pref_Test,Pref_Retest,remove=FALSE) %>%
  mutate(reversal_type=str_remove_all(reversal_type,pattern="_preference"))
```

## S4.1. Descriptives and IDS preference for the restricted sample

```{r labs6, results="asis", message=FALSE, warning=FALSE}
# Summarize mean Age and number of Subjects for each lab, method, and language
lab_stats_6<-all_agg_subjects_paired_retest_6 %>%
  select(Lab,Subject,Method,Age,Language) %>% 
  group_by(Lab,Method,Language) %>% 
  summarise(Age=mean(Age),N=length(unique(Subject)))

# Assign column label to column 4
names(lab_stats_6)[4]<-"Mean age (days)"

#slight edit to method naming
lab_stats_6 <- lab_stats_6 %>%
  mutate(Method = case_when(
    Method=="Eyetracking" ~ "ET",
    Method=="central fixation" ~ "CF",
    TRUE ~ Method
  ))

papaja::apa_table(lab_stats_6, 
                  caption = "Statistics of the included labs for the restricted sample (min six trials contributed per session). N refers to the number of infants included in the analysis.", 
                  format.args = list(digits = 0))

# Are the ages for the full sample and the restricted sample similar?
t_test_age_restricted_sample <- t.test(all_agg_subjects_paired_retest_6$Age,all_agg_subjects_paired_retest$Age)
```

```{r,message=FALSE,warning=FALSE}
# Linear mixed model assessing if session predicts differences in LT 
diff_session_1_6 <- lmer(Diff ~ Session + 
                (1 | Lab) + (1 | Subject), 
              data=all_agg_subjects_paired_6, REML=FALSE)
diff_session_6_coefs<-summary(diff_session_1_6)$coefficients

# Subsetting to session 1 for t-test
all_agg_subjects_paired_s1_6 <- subset(all_agg_subjects_paired_6, all_agg_subjects_paired_6$Session=="1")
ttestSession1_6<-t.test(all_agg_subjects_paired_s1_6$Diff)

# Subsetting to session 2 for t-test
all_agg_subjects_paired_s2_6 <- subset(all_agg_subjects_paired_6, all_agg_subjects_paired_6$Session=="2")
ttestSession2_6 <- t.test(all_agg_subjects_paired_s2_6$Diff)
```

The participants in the restricted sample --- contributing at least six IDS and ADS trials for both sessions --- were distributed across the contributing labs, methods, and language backgrounds (Table 2). There was no difference in average age between the main sample and the restricted sample, `r apa_print(t_test_age_restricted_sample)$statistic`. There was a robust preference for IDS in both Session 1, `r apa_print(ttestSession1_6)$statistic`, and Session 2, `r apa_print(ttestSession2_6)$statistic`. We observed no difference in IDS preference between the two sessions, $\beta$=`r round(diff_session_6_coefs[2,1],2)`, *SE*=`r round(diff_session_6_coefs[2,2],2)`, *p*=`r printp(diff_session_6_coefs[2,5])`. 

```{r}
# Linear mixed model predicting difference score of session 2 from difference score of session 1
mod_lmer_6 <- lmer(Diff_2 ~ Diff_1 + (1|Lab),
              data=all_agg_subjects_paired_retest_6,REML=T)
mod_lmer_6_coefs<-summary(mod_lmer_6)$coefficients

# calculate simple correlation of difference score of session 1 and difference score of session 2
simplyOverallCorr_6<-cor.test(all_agg_subjects_paired_retest_6$Diff_1,
         all_agg_subjects_paired_retest_6$Diff_2)
```

Interestingly, while there was a significant simple correlation between IDS preference in Session 1 and Session 2, `r apa_print(simplyOverallCorr_6)$full_result`, we found that IDS preference in Session 1 did not significantly predict IDS preference in Session 2 in a linear mixed-effects model including a by-lab random intercept, $\beta$=`r round(mod_lmer_6_coefs[2,1],2)`, *SE*=`r round(mod_lmer_6_coefs[2,2],2)`, *p*=`r printp(mod_lmer_6_coefs[2,5])`. 

## S4.2. Moderator analyses including a more restricted sample

### S4.2.1. Time between test sessions

```{r}
#mean-center Diff_1 and age_between_sessions to make main effects interpretable
all_agg_subjects_paired_retest_6 <- all_agg_subjects_paired_retest_6 %>%
  ungroup() %>%
  mutate(Diff_1_c = Diff_1-mean(Diff_1),
         days_between_sessions_c = days_between_sessions-mean(days_between_sessions))

# Adding days between sessions to model
mod_lmer_2.1_6trials <- lmer(Diff_2 ~ Diff_1_c*days_between_sessions_c+ (1|Lab), # Effect of days between sessions
              data=all_agg_subjects_paired_retest_6,REML=T)

days_coefs_6trials<-summary(mod_lmer_2.1_6trials)$coefficients

```

As in the analyses with the full dataset, we found no evidence that the number of days between test sessions moderated the relationship between IDS preference in Session 1 and 2. Neither the main effect of time between sessions, $\beta$=`r round(days_coefs_6trials[3,1],3)`, *SE*=`r round(days_coefs_6trials[3,2],2)`, *t*(`r round(days_coefs_6trials[3,3],1)`)=`r round(days_coefs_6trials[3,4],2)`, *p*=`r printp(days_coefs_6trials[3,5])`, nor the interaction term, $\beta$=`r round(days_coefs_6trials[4,1],3)`, *SE*=`r round(days_coefs_6trials[4,2],2)`, *t*(`r round(days_coefs_6trials[4,3],1)`)=`r round(days_coefs_6trials[4,4],2)`, *p*=`r printp(days_coefs_6trials[4,5])`, showed significant effects.

## S4.2.2. Participant age

```{r}
# Age model
#mean-center age to make main effects interpretable
all_agg_subjects_paired_retest_6 <- all_agg_subjects_paired_retest_6 %>%
  ungroup() %>%
  mutate(Age_c = Age-mean(Age))

# Adding days between sessions to model
mod_lmer_2.2_6 <- lmer(Diff_2 ~ Diff_1_c*Age_c+ (1+ Diff_1_c|Lab), # Effect of days between sessions
              data=all_agg_subjects_paired_retest_6,REML=T)

age_coefs_6<-summary(mod_lmer_2.2_6)$coefficients

```

To investigate the possibility that age moderated test-retest reliability in the restricted sample, we fit a linear mixed-effects model predicting IDS preference in Session 2 from IDS preference in Session 1 (mean-centered), participant age (mean-centered), and their interaction.
The model included a by-lab random intercept and a by-lab random slope for IDS preference in Session 1.
We found no evidence that age influenced test-retest reliability as indicated by the interaction between IDS preference in Session 1 and age, $\beta$=`r round(age_coefs_6[4,1],3)`, *SE*=`r round(age_coefs_6[4,2],3)`, *t*(`r round(age_coefs_6[4,3],1)`)=`r round(age_coefs_6[4,4],2)`, *p*=`r printp(age_coefs_6[4,5])`.

## S4.2.3. Method

```{r}
all_agg_subjects_paired_retest_6 <- all_agg_subjects_paired_retest_6 %>%
  mutate(
    Method=case_when(
      Method=="Eyetracking" ~ "eye-tracking",
      TRUE ~ Method
    )
  )

# Adding method to the model
mod_lmer_2.3_6 <- lmer(Diff_2 ~ Diff_1_c*Method + (1+ Diff_1_c|Lab), # effect of method
              data=all_agg_subjects_paired_retest_6,REML=T)

#overall chi-squared test to assess the omnibus interaction effect between test at time 1 and method
overall_chisq_test_method_6 <- Anova(mod_lmer_2.3_6,type="III")
method_coefs_6<-summary(mod_lmer_2.3_6)$coefficients
```

```{r results="asis",warning=FALSE,message=FALSE}
#function for computing coefficient table from lmer model and correlation reliability
compute_coefs_reliability <- function(dataset) {
  #fit lmer reliability model
  mod_lmer <- lmer(Diff_2 ~ Diff_1 + (1|Lab),data=dataset,REML=T)
  #extract coefficients
  coef <- summary(mod_lmer)$coef %>%
    as_data_frame %>%
    mutate_at(c("Estimate","Std. Error","df", "t value", "Pr(>|t|)"), 
              function (x) signif(x, digits = 3)) %>%
    rename(SE = `Std. Error`, 
           t = `t value`,
           pvalue = `Pr(>|t|)`,
           estimate = Estimate) %>%
    #keep just main coefficient estimate
    slice(2) %>%
    #keep most important columns
    select(estimate, SE, pvalue)
  
  #compute correlation and add to summarized data
  correlation<-cor.test(dataset$Diff_1,dataset$Diff_2) 
  summarized_data <- coef %>%
    mutate(cor=as.numeric(correlation$estimate),
           df=as.numeric(correlation$parameter),
           pvalue2=as.numeric(correlation$p.value))
  
  #return
  summarized_data
}
coef_table_6 <- all_agg_subjects_paired_retest_6 %>%
  group_by(Method) %>%
  nest() %>%
  #compute coefficients
  mutate(coef_table = purrr::map(data, compute_coefs_reliability)) %>%
  #remove data
  select(-data) %>%
  #unpack
  unnest(cols=c(coef_table)) %>%
  select(-pvalue2,-df) %>%
  mutate(
    Method=case_when(
      Method=="Eyetracking" ~ "eye-tracking",
      Method=="central fixation" ~ "central fixation",
      TRUE ~ Method
    )
  )
```

We tested whether method (ET vs. CF vs. HPP) moderated test-retest reliability by fitting a linear mixed-effects model predicting IDS preference in Session 2 from IDS preference in Session 1 (mean-centered), Method (dummy-coded, with CF as the reference level), and their interaction.
The model included a by-lab random intercept and a by-lab random slope for IDS preference in Session 1.
We found no evidence that Method influenced test-retest reliability as indicated by the interaction between IDS preference in Session 1 and age, ${\chi}^2$(2)=`r round(overall_chisq_test_method_6$Chisq[4],2)`, *p*=`r printp(overall_chisq_test_method_6[4,3])`. There was no significant relationship between IDS preference for Session 1 and Session 2 for each method considered separately, CF: $\beta$=`r round(coef_table_6$estimate[1],3)`, *SE*=`r round(coef_table_6$SE[1],2)`, *p*=`r printp(coef_table_6$pvalue[1])`, HPP: $\beta$=`r round(coef_table_6$estimate[2],3)`, *SE*=`r round(coef_table_6$SE[2],2)`, *p*=`r printp(coef_table_6$pvalue[2])`, ET: $\beta$=`r round(coef_table_6$estimate[3],3)`, *SE*=`r round(coef_table_6$SE[3],2)`, *p*=`r printp(coef_table_6$pvalue[3])`.



## S4.2.4. Language background

```{r}
# Differences in test-retest reliability between English and non-English learning infants
# Assigning 1 to UBC and Madison as NAE labs. Remaining labs are non-NAE and are coded as 0.
all_agg_subjects_paired_retest_6$NAE<-ifelse(all_agg_subjects_paired_retest_6$Lab=="InfantCog-UBC"| all_agg_subjects_paired_retest_6$Lab=="infantll-madison", 1, 0)

#center NAE to make main effects interpretable
all_agg_subjects_paired_retest_6 <- all_agg_subjects_paired_retest_6 %>%
  ungroup() %>%
  mutate(NAE_c = ifelse(NAE==1,0.5,-0.5))

# Adding NAE to model
mod_lmer_2.4_6 <- lmer(Diff_2 ~ Diff_1_c*NAE_c+(1|Lab),
              data=all_agg_subjects_paired_retest_6,REML=T) 

nae_coefs_6<-summary(mod_lmer_2.4_6)$coefficients
```

As in the main sample, a linear mixed-effects model predicting IDS preference in Session 2 based on IDS preference in Session 1 (mean-centered), NAE (centered), and their interaction, including Lab as a random intercept, revealed no interaction, $\beta$=`r round(nae_coefs_6[4,1],2)`, *SE*=`r round(nae_coefs_6[4,2],2)`, *t*(`r round(nae_coefs_6[4,3],1)`)=`r round(nae_coefs_6[4,4],2)`, *p*=`r printp(nae_coefs_6[4,5])`.

\newpage
# S5. Alternative dependent variables

To check the robustness of our results, we also investigated whether we obtained similar results with other possible dependent measures: average log-transformed looking times and a proportion-based preference measure. For each alternative dependent variable, we conducted the main analyses of test-retest reliability reported in the manuscript: the overall Pearson correlation, the test-retest linear mixed-effects model, and an inspection of applying stricter inclusion criteria for number of trials contributed.

## S5.1. Correlations between alternative dependent variables

```{r, results="asis"}
dv_corr_dataframe <- all_agg_subjects_paired_6 %>%
  select(Diff, Diff_log_lt, Prop) %>%
  as.data.frame()

dv_corr_table <- corx(dv_corr_dataframe,
          triangle = "lower",
          stars = c(0.05, 0.01, 0.001),
          describe = c(`M` = mean, `SD` = sd))

papaja::apa_table(dv_corr_table$apa, # apa contains the data.frame needed for apa_table
                  caption = "Correlations between alternative dependent measures.",
                  note = "* p < 0.05; ** p < 0.01; *** p < 0.001",
                  escape = T)
```

First, we consider the correlations between the three dependent measures we considered for IDS preference: (a) a simple difference score between average IDS and ADS looking times (main manuscript), (b) a difference score between average log-transformed looking times, and (c) the proportion-based preference measure. As expected, the correlations between the alternative dependent measures were very high, all *r*-values >= `r round(min(dv_corr_table$r),2)` (Table 3).

## S5.2. Log-transformed looking times

```{r}
# calculate simple correlation of log-looking-time-based difference score of session 1 and difference score of session 2
simplyOverallCorr_log_lt<-cor.test(all_agg_subjects_paired_retest$Diff_log_lt_session_1,
         all_agg_subjects_paired_retest$Diff_log_lt_session_2)
```

```{r results="asis",warning=FALSE,message=FALSE}
# Linear mixed model predicting log-looking-time-based difference score of session 2 from difference score of session 1
mod_lmer_log_lt <- lmer(Diff_log_lt_session_2 ~ Diff_log_lt_session_1 + (1|Lab),
              data=all_agg_subjects_paired_retest,REML=T)

# Coefficients for linear mixed model predicting log-looking-time-based difference score of session 2 from difference score of session 1
coefs <- summary(mod_lmer_log_lt)$coef %>%
  as_data_frame %>%
  mutate_at(c("Estimate","Std. Error","df", "t value", "Pr(>|t|)"), 
            function (x) signif(x, digits = 3)) %>%
  rename(SE = `Std. Error`, 
         t = `t value`,
         p = `Pr(>|t|)`) %>%
  select(-df)
         
rownames(coefs) <- c("Intercept", "Log LT IDS Preference Session 1")

papaja::apa_table(coefs, 
                  caption = "Coefficient estimates from a linear mixed-effects model predicting Log LT IDS preference in Session 2.", 
                  col.names=c("", "Estimate","$SE$", "$t$", "$p$"),
                  format.args = list(digits = 2))
```


```{r fig.cap="IDS preferences (based on average log-looking times) of both sessions plotted against each other for each inclusion criterion. \\textit{n} indicates the number of included infants, \\textit{r} is the Pearson correlation coefficient as the indicator for reliability.", fig.align="center",out.width = "5in",fig.height = 8, fig.width = 8}

#read datasets with different inclusion criteria
all_agg_subjects_paired_retest_2 <- read_csv(here(read_path,"all_agg_subjects_paired_retest_minimum_trials_per_type_2.csv"))
all_agg_subjects_paired_retest_4 <- read_csv(here(read_path,"all_agg_subjects_paired_retest_minimum_trials_per_type_4.csv"))
all_agg_subjects_paired_retest_6 <- read_csv(here(read_path,"all_agg_subjects_paired_retest_minimum_trials_per_type_6.csv"))
all_agg_subjects_paired_retest_8 <- read_csv(here(read_path,"all_agg_subjects_paired_retest_minimum_trials_per_type_8.csv"))

# Creating scatterplot for each inclusion criterion
plot_exclusion_dataset <- function(current_dataset,minimum_trials=1,
                                   Limaxes=c(-4,10),
                                   session_1_column="Diff_1",
                                   session_2_column="Diff_2",
                                   xlab = "IDS preference in first session",
                                   ylab = "IDS preference in second session"
                                   ) {
  current_correlation <- cor.test(current_dataset[[session_1_column]],current_dataset[[session_2_column]])
  main_message <- paste0("Inclusion criterion: ",minimum_trials," trial pairs")
  plot(current_dataset[[session_1_column]],current_dataset[[session_2_column]],xlab = xlab,ylab=ylab,main = main_message,xlim = Limaxes,ylim = Limaxes)
abline(lm(current_dataset[[session_2_column]]~current_dataset[[session_1_column]]),col="red")
legend("topleft",legend = paste("r = ",round(current_correlation$estimate,2),"; n = ",current_correlation$parameter+2,sep=""))
}

par(mfrow = c(2, 2))
plot_exclusion_dataset(all_agg_subjects_paired_retest_2,minimum_trials=2,Limaxes=c(-1,1.5),session_1_column="Diff_log_lt_session_1",session_2_column="Diff_log_lt_session_2",xlab = "IDS preference (log looking time) in Session 1", ylab="IDS preference (log looking time) in Session 2")
plot_exclusion_dataset(all_agg_subjects_paired_retest_4,minimum_trials=4,Limaxes=c(-1,1.5),session_1_column="Diff_log_lt_session_1",session_2_column="Diff_log_lt_session_2",xlab = "IDS preference (log looking time) in Session 1", ylab="IDS preference (log looking time) in Session 2")
plot_exclusion_dataset(all_agg_subjects_paired_retest_6,minimum_trials=6,Limaxes=c(-1,1.5),session_1_column="Diff_log_lt_session_1",session_2_column="Diff_log_lt_session_2",xlab = "IDS preference (log looking time) in Session 1", ylab="IDS preference (log looking time) in Session 2")
plot_exclusion_dataset(all_agg_subjects_paired_retest_8,minimum_trials=8,Limaxes=c(-1,1.5),session_1_column="Diff_log_lt_session_1",session_2_column="Diff_log_lt_session_2",xlab = "IDS preference (log looking time) in Session 1", ylab="IDS preference (log looking time) in Session 2")
```

In these analyses, we calculated IDS preference by first log-transforming looking times for each trial, computing the average log-transformed looking time for IDS and ADS for each participant, and calculating the difference between average IDS and ADS log-transformed looking times.
We fit a linear mixed-effects model predicting IDS preference in Session 2 from IDS preference in Session 1, including a by-lab random intercept. 
As in the analyses using average raw looking times, the results revealed no significant relationship between IDS preference in Session 1 and 2 (Table 4). 
The Pearson correlation coefficient was also not statistically significant, `r apa_print(simplyOverallCorr_log_lt)$full_result`.
Applying successively stricter inclusion criteria --- by requiring a higher number of valid trials per condition in each session --- showed a similar pattern to the main manuscript, such that correlations increased somewhat with stricter inclusion criteria, but substantially reduced the sample size at the same time (Figure 5).

## S5.3. Proportion looking to IDS

```{r}
# calculate simple correlation of difference score of session 1 and difference score of session 2
simplyOverallCorr_prop<-cor.test(all_agg_subjects_paired_retest$Prop_session_1,
         all_agg_subjects_paired_retest$Prop_session_2)
```

Next, we calculated a proportion-based IDS preference measure by computing the average proportion (raw) looking time to IDS relative to total (raw) looking time to IDS and ADS for each subject (i.e., IDS looking time / (ADS looking time + IDS looking time)).
We fit a linear mixed-effects model predicting proportion-based IDS preference in Session 2 from proportion-based IDS preference in Session 1, including a by-lab random intercept. 
As in the analyses using other measures of IDS preference, the results revealed no significant relationship between IDS preference in Session 1 and 2 (Table 5). 
The Pearson correlation coefficient based on proportional IDS looking was also not statistically significant, `r apa_print(simplyOverallCorr_prop)$full_result`.
Stricter inclusion criteria increased the correlation somewhat, as in previous analyses (Figure 6).

```{r fig.cap="IDS preferences (based on proportion IDS looking) of both sessions plotted against each other for each inclusion criterion. n indicates the number of included infants, r is the Pearson correlation coefficient as the indicator for reliability.", fig.align="center",out.width = "5in",fig.height = 8, fig.width = 8}
par(mfrow = c(2, 2))
plot_exclusion_dataset(all_agg_subjects_paired_retest_2,minimum_trials=2,Limaxes=c(0.25,0.8),session_1_column="Prop_session_1",session_2_column="Prop_session_2",xlab = "IDS preference (proportion-based) in Session 1", ylab="IDS preference (proportion-based) in Session 2")
plot_exclusion_dataset(all_agg_subjects_paired_retest_4,minimum_trials=4,Limaxes=c(0.25,0.8),session_1_column="Prop_session_1",session_2_column="Prop_session_2",xlab = "IDS preference (proportion-based) in Session 1", ylab="IDS preference (proportion-based) in Session 2")
plot_exclusion_dataset(all_agg_subjects_paired_retest_6,minimum_trials=6,Limaxes=c(0.25,0.8),session_1_column="Prop_session_1",session_2_column="Prop_session_2",xlab = "IDS preference (proportion-based) in Session 1", ylab="IDS preference (proportion-based) in Session 2")
plot_exclusion_dataset(all_agg_subjects_paired_retest_8,minimum_trials=8,Limaxes=c(0.25,0.8),session_1_column="Prop_session_1",session_2_column="Prop_session_2",xlab = "IDS preference (proportion-based) in Session 1", ylab="IDS preference (proportion-based) in Session 2")
```

```{r results="asis",warning=FALSE,message=FALSE}
# Linear mixed model predicting proportion-based IDS preference of session 2 from proportion novelty preference of session 1
mod_lmer_prop <- lmer(Prop_session_2 ~ Prop_session_1 + (1|Lab),
              data=all_agg_subjects_paired_retest,REML=T)

# Coefficients for linear mixed model
coefs <- summary(mod_lmer_prop)$coef %>%
  as_data_frame %>%
  mutate_at(c("Estimate","Std. Error","df", "t value", "Pr(>|t|)"), 
            function (x) signif(x, digits = 3)) %>%
  rename(SE = `Std. Error`, 
         t = `t value`,
         p = `Pr(>|t|)`) %>%
  select(-df)
         
rownames(coefs) <- c("Intercept", "IDS Preference (proportion measure) Session 1")

papaja::apa_table(coefs, 
                  caption = "Coefficient estimates from a linear mixed-effects model predicting IDS preference (based on proportion IDS looking) in Session 2.", 
                  col.names=c("", "Estimate","$SE$", "$t$", "$p$"),
                  format.args = list(digits = 2))
```

\newpage

# S6. Sensitivity of test-retest reliability to trial number inclusion criteria

## S6.1. Pearson Correlation with increasingly stricter trial-level inclusion criteria

To conduct a more fine-grained analysis of how stricter trial inclusion criteria affect test-retest reliability, we computed correlations while gradually increasing the number of total valid trials required for inclusion.
For this analysis, we required a minimum of one IDS and one ADS trial and gradually increased the number of total valid trials required in both sessions (irrespective of IDS and ADS condition) from two to 16 (the maximum number of total trials).
Figure 7 depicts the Pearson correlation coefficients for increasingly stricter requirements for the overall trial numbers of a given participant in both sessions.
Correlations only increase and reach conventional levels of significance once the number of total required trials for both sessions is greater than 12.


```{r}
#function for filter based on number of overall trials and compute dataframe of correlation information
correlation_test_retest_min_trials <- function(dataset,minimum_trials=2) {
  #filter dataset
  current_dataset <- filter(dataset,total_trial_n_session_1>=minimum_trials&total_trial_n_session_2>=minimum_trials)
  #compute the test-retest correlation
  correlation <- cor.test(current_dataset$Diff_1,current_dataset$Diff_2)
  correlation_df <- data.frame(
    r = correlation$estimate, 
    t = correlation$statistic, 
    df=correlation$parameter, 
    p = correlation$p.value, 
    lower_ci = correlation$conf.int[1],
    upper_ci = correlation$conf.int[2],
    N = length(current_dataset$Subject_Unique))
  return(correlation_df)
}

# create correlations for each minimum number of trial exlcusion level
min_trials<-seq(2,16)
correlation_trial_exclusion_seq <- all_agg_subjects_paired_retest %>%
  #expand and then nest data frame for each level of minimum trial pairs
  expand_grid(min_trials=min_trials) %>%
  group_by(min_trials) %>%
  nest() %>%
  #now aggregate data for each minimum trial exclusion level
  mutate(
    correlation = purrr::map2(data,min_trials,correlation_test_retest_min_trials)
  ) %>%
  select(-data) %>%
  unnest(cols=correlation)
```

```{r sfig7, fig.cap = "Pearson correlation coefficient with increasingly strict trial-level inclusion criteria. The x-axis depicts the required number of overall valid trials in both Session 1 and Session 2. Dots represent corresponding correlation coefficients, with 95% CIs. The sample size is shown above each dot."}
ggplot(correlation_trial_exclusion_seq,aes(x=min_trials,y=r))+
  geom_hline(yintercept=0,linetype="dashed")+
  geom_point(size=4)+
  geom_errorbar(aes(ymin=lower_ci,ymax=upper_ci),width=0)+
  xlab("Number of overall trials in each session required for inclusion")+
  ylab("Pearson correlation coefficient")+
  geom_text(aes(label=N,y=0.75),size=5)+
  ylim(-0.15,1)+
  scale_x_continuous(breaks=seq(2,16))+
  theme_cowplot()
                  
```

## S6.2. Simulation with subset of participants

While increasing the number of trials required for inclusion, we are also drastically reducing the sample size and selecting a specific set of participants.
One resulting question is whether the rise in reliability is driven primarily by the increasing number of trials or by the selection of a particular set of participants (e.g., with a more consistent distribution of responses), or both.
To address this question, we conducted a simulation in which, for each subset of participants based on an increasing number of trials needed for inclusion (2-16; see Figure 7), we resampled increasingly larger numbers of trials for the given subset of participants (in increments of pairs of IDS and ADS trials, 1-8 trial pairs) and computed the average resulting test-retest correlation (Figure 8).



```{r, warning=FALSE, message=FALSE, echo=FALSE, results= 'hide', eval=RUN_SIMULATIONS}
# function for random selection of trial pairs per Session and Condition of overall trials and computing correlations
correlation_test_retest_sampled_trials <- function(dataset,num_slices=1) {
  #filter dataset
  current_dataset <- dataset %>% 
      group_by(Subject_Unique,Session,Condition) %>% 
      slice_sample(n = num_slices) %>%
      summarise(mean_lt=mean(LT),.groups="drop_last") %>% 
      pivot_wider(names_from = Condition,values_from = mean_lt) %>% 
      mutate(Diff=IDS-ADS) %>%
      select(-(c("ADS","IDS"))) %>%  
      pivot_wider(names_from = Session,values_from = Diff, names_prefix = "Diff_") %>% 
      ungroup()
    
  #compute the test-retest correlation
  correlation <- cor.test(current_dataset$Diff_1,current_dataset$Diff_2)
  return(list(correlation=correlation$estimate,N=length(unique(current_dataset$Subject_Unique))))
}

#compute correlation for a specific trial number
compute_retest_correlation_subset <- function(min_trial_number,num_slices,index=1,print_message=TRUE) {
  
  # create subset datasets based on minumum trial number
  all_agg_subjects_paired_retest_atleast_t_trials <- all_agg_subjects_paired_retest %>%
    filter(total_trial_n_session_1>=min_trial_number&total_trial_n_session_2>=min_trial_number)
  all_subjects_atleast_t_trials <- data_clean %>% 
    filter(Subject_Unique %in% all_agg_subjects_paired_retest_atleast_t_trials$Subject_Unique)
  
  if (print_message) {
    #print information about the run
    print(paste("run index:", index,"| min trials:",min_trial_number,"| num slices:",num_slices))
  }
  
  #compute resampled correlation
  return(correlation_test_retest_sampled_trials(all_subjects_atleast_t_trials,num_slices))
  
}


# Start simulation for participants with at least 12 valid trials (6 valid trials pairs) for both test and retest session

N = seq(1,100) # Number of iterations

Trials = seq(2,16) # Select the number of required trials

num_slices = c(1,2,3,4,5,6,7,8) # Select the number of needed trial pairs for inclusion

#run sumpulations
simulation_outcome <-expand.grid(N = N,Trials = Trials,num_slices = num_slices) %>%
  mutate(
    results = pmap(list(Trials,num_slices,N),compute_retest_correlation_subset),
    correlation = map_dbl(results,'correlation'),
    sample_size = map_dbl(results,'N')
  ) %>%
  select(-results)

#save the resulting data frame
write_csv(simulation_outcome, here("data","processed", "resampling_retest_correlation_simulation_outcome.csv"))
```

```{r fig8, fig.cap = "Results of the simulation showing how test-rest correlations increase with the number of trial pairs included in the analysis. The data is facetted by the number of overall trials required for inclusion (11-16 overall trials; compare Figure 7), including. The x-axis depicts the number of trial pairs (randomly resampled per participant with replacement) in the analysis, and the y-axis shows the mean Pearson correlation coefficient between test and retest IDS preference (difference score). Error bars represent 95% CIs. The number of included participants is shown above each data point."}

#read in simulation data
simulation_outcome <- read_csv(here("data","processed", "resampling_retest_correlation_simulation_outcome.csv"))

#summarize simulation data
simulation_outcome_summarized <-simulation_outcome %>% 
  group_by(sample_size,Trials,num_slices) %>% 
  summarise(N=n(),r_mean=mean(correlation),r_sd=sd(correlation)) %>%
  mutate(
    lower_ci = r_mean - (1.96 * r_sd / sqrt(N)),
    upper_ci = r_mean + (1.96 * r_sd / sqrt(N))
    ) %>%
  #just in case this comes in handy to use
  mutate(
    trials_squared = sqrt(num_slices*2)
  )

#plot
ggplot(simulation_outcome_summarized,aes(x=num_slices,y=r_mean,color=as.factor(Trials)))+
  geom_smooth(method="lm",alpha=0.5, fill="#d3d4d3")+
  geom_hline(yintercept=0,linetype="dashed")+
  geom_point(size=1.5)+
  geom_errorbar(aes(ymin=lower_ci,ymax=upper_ci),width=0)+
  geom_label(aes(label=paste0("n = ",sample_size)),x=3,y=0.4,size=4,show.legend=FALSE)+
  xlab("Number of randomly resampled trial pairs (with replacement)")+
  ylab("Average test-retest correlation coefficient")+
  scale_color_viridis_d(name="Minimum number of \ncontributed trials in subset")+
  ylim(-0.1,0.5)+
  scale_x_continuous(breaks=c(2,4,6,8))+
  theme_cowplot()+
  theme(legend.position="none")+
  facet_wrap(~Trials,nrow=3)

#plot
#just in case useful for showing the r ~ sqrt(N) relationship
# ggplot(simulation_outcome_summarized,aes(x=trials_squared,y=r_mean,color=as.factor(Trials)))+
#   geom_smooth(method="lm",alpha=0.5, fill="#d3d4d3")+
#   geom_hline(yintercept=0,linetype="dashed")+
#   geom_point(size=1.5)+
#   geom_errorbar(aes(ymin=lower_ci,ymax=upper_ci),width=0)+
#   geom_label(aes(label=paste0("n = ",sample_size)),x=3,y=0.4,size=4,show.legend=FALSE)+
#   xlab("Squared number of randomly resampled trials (with replacement)")+
#   ylab("Average test-retest correlation coefficient")+
#   scale_color_viridis_d(name="Minimum number of \ncontributed trials in subset")+
#   ylim(-0.1,0.5)+
#   #scale_x_continuous(breaks=c(2,4,6,8))+
#   theme_cowplot()+
#   theme(legend.position="none")+
#   facet_wrap(~Trials,nrow=3)
```


\newpage

# S7. Patterns of preference across sessions

```{r,message=FALSE,warning=FALSE}

#overall percent reversals
reversal_tally <- all_agg_subjects_paired_retest %>%
  ungroup() %>%
  group_by(reversal_type) %>%
  tally()
total_consistent <- sum(filter(reversal_tally, reversal_type %in% c("IDS_IDS","ADS_ADS"))$n)
total_n <- sum(reversal_tally$n)
percent_consistent <- total_consistent/total_n 
```

We also conducted analyses to explore whether there were any patterns of preference reversal across test sessions.
While there was no strong correlation in the magnitude of IDS preference between Session 1 and Session 2, here we asked whether infants consistently expressed the same preference across test sessions.
Overall, `r round(percent_consistent,3)*100 `% of the infants had a consistent preference from test to retest session.
Of the `r sum(reversal_tally$n)` total infants, `r round(filter(reversal_tally,reversal_type=="IDS_IDS")$n/sum(reversal_tally$n),3)*100`% of infants showed a consistent IDS preference and `r round(filter(reversal_tally,reversal_type=="ADS_ADS")$n/sum(reversal_tally$n),3)*100`% showed a consistent ADS preference.
`r round(filter(reversal_tally,reversal_type=="IDS_ADS")$n/sum(reversal_tally$n),3)*100`% of infants switched from an IDS preference at Session 1 to an ADS preference at Session 2 and `r round(filter(reversal_tally,reversal_type=="ADS_IDS")$n/sum(reversal_tally$n),3)*100`% switched from an ADS preference to an IDS preference. 


Next, we explored whether we could detect any systematic clustering of infants with distinct patterns of preference across the test and retest session.
We took a bottom-up approach and conducted a *k*-means clustering of the test-retest difference data (here using log-transformed looking time data).
We found little evidence of distinct clusters emerging from these groupings: the clusterings ranging from *k*=2 (2 clusters) to *k*=4 (4 clusters) appear to mainly track  whether participants are approximately above or below the mean looking time difference for Session 1 and Session 2 (Figure 9A).
The diagnostic elbow plot shows little evidence of a qualitative improvement as the number of clusters is increased, which suggests little evidence for a distinctive set of clusters of participants who showed similar patterns of looking across the test and retest sessions (Figure 9B).

```{r}
d <- all_agg_subjects_paired_retest %>% 
  column_to_rownames(var = "Subject_Unique") %>% 
  filter(!is.na(Diff_log_lt_session_1)) %>%
  ungroup() %>% 
  select(Diff_log_lt_session_1,Diff_log_lt_session_2)

kclusts <- 
  tibble(k = 1:6) %>%
  mutate(
    kclust = map(k, ~kmeans(d, .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, d)
  )

clusters <- 
  kclusts %>%
  unnest(cols = c(tidied))

assignments <- 
  kclusts %>% 
  unnest(cols = c(augmented)) %>%
  mutate(
    mean_diff_1=mean(Diff_log_lt_session_1),
    mean_diff_2=mean(Diff_log_lt_session_2),
  )

clusterings <- 
  kclusts %>%
  unnest(cols = c(glanced))
```

```{r fig9, fig.cap = " (A) Results from the k-means clustering analysis of IDS preference (based on average log looking times) in Session 1 and 2 for different numbers of k,  and (B) the corresponding elbow plot of the total within-cluster sum of squares. In (A), points represent indvidual participants' magnitude of looking time difference at Sessions 1 (x-axis) and 2 (y-axis). The solid line indicates no preference for IDS vs. ADS, the dotted lines indicate mean IDS preference at Session 1 and 2, respectively. Colors indicate clusters from the k-means clustering for different values of k."}
p1 <- 
  ggplot(assignments, aes(x = Diff_log_lt_session_1, y = Diff_log_lt_session_2)) +
  geom_point(aes(color = .cluster), alpha = 0.8) + 
  facet_wrap(~ k,ncol =2)+
  geom_hline(yintercept=0)+
  geom_vline(xintercept=0)+
  geom_hline(yintercept=assignments$mean_diff_2[1],linetype="dotted")+
  geom_vline(xintercept=assignments$mean_diff_1[1],linetype="dotted")+
  xlab("IDS preference (log-based) in first session")+
  ylab("IDS preference (log-based) in second session")+
  scale_color_brewer(name ="cluster", palette="Set1")+
  theme_cowplot()+
  theme(axis.text=element_text(size=11),axis.title=element_text(size=11))

p2 <- ggplot(clusterings, aes(k, tot.withinss)) +
  geom_line() +
  geom_point()+
  xlab("Number of clusters")+
  scale_x_continuous(breaks=seq(1,10))+
  ylab("Total within-cluster sum of squares")+
  theme_cowplot()

plot_grid(p1,p2,labels=c("A","B"),rel_widths=c(1.5,1))
```

\newpage

# S8. Correlations in average looking times between sessions

```{r}
agg_by_subj_paired <- data_clean %>%
  select(Subject,Subject_Unique,Age,Gender,Method, preterm,Session, Condition, trial_num,LT,Lab,days_between_sessions) %>% 
  group_by(Method, Lab,Subject, Subject_Unique, Age,Gender, preterm,days_between_sessions,Session) %>%
  mutate(
    Method=case_when(
      Method=="Eyetracking" ~ "eye-tracking",
      TRUE ~ Method
    )
  )%>%
  summarize(mean_lt=mean(LT),
            N=n()) %>%
  pivot_wider(names_from = Session, values_from = c(mean_lt,N,Age)) %>%
  rowwise() %>%
  mutate(
    mean_age = mean(c(Age_1,Age_2),na.rm=TRUE)
  )

agg_by_subj_condition_paired <- data_clean %>%
  select(Subject,Subject_Unique,Age,Gender,Method, preterm,Session, Condition, LT,Lab,days_between_sessions) %>% 
  group_by(Method, Lab,Subject, Subject_Unique, Age,Gender, preterm,days_between_sessions,Condition,Session) %>%
  summarize(mean_lt=mean(LT)) %>%
  pivot_wider(names_from = Session, values_from =  c(mean_lt, Age)) %>%
  rename(LT_Test = mean_lt_1,LT_Retest=mean_lt_2) %>%
  rowwise() %>%
  mutate(
    mean_age = mean(c(Age_1,Age_2),na.rm=TRUE)
  )

corr_lt_p2 <- ggplot(agg_by_subj_condition_paired,aes(LT_Test,LT_Retest,color=Condition))+
  geom_point()+
  geom_smooth(method="lm",color="black")+
  facet_wrap(~Condition)+
  theme_cowplot()+
  theme(legend.position="none")+
  scale_color_brewer(palette="Set1",direction=-1)+
  xlab("Average Looking Time (in s) in first session")+
  ylab("Average Looking Time (in s) in second session")

agg_by_subj_condition_paired_wide <- agg_by_subj_condition_paired %>%
  pivot_wider(names_from = Condition,values_from = c(LT_Test,LT_Retest))

#condition-specific correlations
correlation_IDS_LT <- cor.test(agg_by_subj_condition_paired_wide$LT_Retest_IDS,agg_by_subj_condition_paired_wide$LT_Test_IDS)
correlation_ADS_LT <- cor.test(agg_by_subj_condition_paired_wide$LT_Retest_ADS,agg_by_subj_condition_paired_wide$LT_Test_ADS)

#predicting condition-specific session 2 looking from session 1 looking to IDS and ADS
predicting_Retest_IDS <- lm(LT_Retest_IDS ~ LT_Test_IDS + LT_Test_ADS,data=agg_by_subj_condition_paired_wide)
predicting_Retest_ADS <- lm(LT_Retest_ADS ~ LT_Test_IDS + LT_Test_ADS,data=agg_by_subj_condition_paired_wide)

#controlling for age
predicting_Retest_IDS_age <- lm(LT_Retest_IDS ~ LT_Test_IDS + mean_age,data=agg_by_subj_condition_paired_wide)
predicting_Retest_ADS_age <- lm(LT_Retest_ADS ~ LT_Test_ADS + mean_age,data=agg_by_subj_condition_paired_wide)

#both controlling for age and IDS and ADS looking
#predicting condition-specific session 2 looking from session 1 looking to IDS and ADS
predicting_Retest_IDS_full <- lm(LT_Retest_IDS ~ LT_Test_IDS + LT_Test_ADS+mean_age,data=agg_by_subj_condition_paired_wide)
predicting_Retest_ADS_full <- lm(LT_Retest_ADS ~ LT_Test_IDS + LT_Test_ADS+mean_age,data=agg_by_subj_condition_paired_wide)
```

As reported in the main manuscript, we found that infants' average looking time was correlated between their two testing sessions.
We also found similar correlations in average looking time to IDS stimuli in Session 1 and 2, `r apa_print(correlation_IDS_LT)$full_result`, and ADS stimuli in Session 1 and 2, `r apa_print(correlation_ADS_LT)$full_result`.
To test whether these correlations were specific to looking times for IDS or ADS stimuli alone, we fit linear regression models predicting average looking to IDS (or ADS) stimuli in Session 2 from average looking to IDS and ADS stimuli in Session 1.
We found that average looking to IDS stimuli in Session 2 could be predicted from average looking to IDS stimuli in Session 1, even after controlling for average looking to ADS stimuli in Session 1, `r apa_print(predicting_Retest_IDS)$full_result[["LT_Test_IDS"]]`.
Conversely, average looking to ADS stimuli in Session 2 could be predicted from average looking to ADS stimuli in Session 1, even after controlling for average looking to IDS stimuli in Session 1, `r apa_print(predicting_Retest_ADS)$full_result[["LT_Test_ADS"]]`.
These results suggest that the condition-specific correlations in average looking time cannot be fully explained by the fact that infants' overall looking times between sessions are correlated.

```{r}
agg_by_subj_item_paired <- data_clean %>%
    select(Subject,Subject_Unique,Gender,Method, preterm,Session, Condition, stimulus,trial_num, LT,Lab,days_between_sessions) %>% 
  group_by(Method, Lab,Subject, Subject_Unique, Gender, preterm,days_between_sessions,Condition,stimulus) %>%
  mutate(stimulus = str_remove(stimulus,"Retest")) %>%
  mutate(stimulus = str_remove(stimulus,"retest")) %>%
  filter(stimulus %in% c("ADS1","IDS1","ADS2","IDS2","ADS3","IDS3","ADS4", "IDS4","IDS5","ADS5","ADS6","IDS6","ADS7","IDS7","ADS8","IDS8")) %>%
  pivot_wider(names_from = Session, values_from = c(LT,trial_num)) %>%
  rename(LT_Test = LT_1,LT_Retest=LT_2)

corr_lt_p3 <- ggplot(agg_by_subj_item_paired,aes(LT_Test,LT_Retest,color=Condition))+
  geom_point()+
  geom_smooth(method="lm",color="black")+
  facet_wrap(~stimulus)+
  theme_cowplot()+
  theme(legend.position="none")+
  scale_color_brewer(palette="Set1",direction=-1)+
  xlab("Looking Time (in s) in first session")+
  ylab("Looking Time (in s) in second session")

item_lt_relationship <- lmer(LT_Retest~LT_Test+(1+LT_Test|Subject_Unique)+(1|stimulus)+(1+LT_Test|Lab),data=agg_by_subj_item_paired, control=lmerControl(optimizer="bobyqa"))
#model including a by-stimulus slope for LT_Test has a singular fit but yields similar results

#same model but including the interaction with trial number in session 1
agg_by_subj_item_paired <- agg_by_subj_item_paired %>%
  ungroup() %>%
  #center
  mutate(
    LT_Test_c = LT_Test - mean(LT_Test,na.rm=TRUE),
    trial_num_1_c = trial_num_1 - mean(trial_num_1,na.rm=TRUE)
  )
item_lt_relationship_control_trial_num <- lmer(LT_Retest~LT_Test_c*trial_num_1_c+(1+LT_Test_c+trial_num_1_c|Subject_Unique)+(1|stimulus)+(1+LT_Test_c+trial_num_1_c|Lab),data=agg_by_subj_item_paired, control=lmerControl(optimizer="bobyqa"))
#LT_Test is significant with a by-lab random slope for trial number, but model fit is singular
```

We next inspected item-level correlations between the two test sessions.
Specifically, we investigated the relation between items composed of the same recording clips in Session 1 and Session 2 (but with a reversed order of clips between the two sessions).
We fit a linear mixed-effects model predicting item-level looking time in Session 2 from item-level looking time in Session 1, including random intercepts for participant, item, and lab, as well as a random slope for item-level looking time in Session 1 for participant and lab.
Item-level looking in Session 2 was related to item-level looking in Session 1, `r apa_print(item_lt_relationship)$full_result[["LT_Test"]]` (Figure 10).
Similar results hold if looking times are log-transformed.

In MB1, the ordering of stimuli was counterbalanced, but some stimuli still appeared earlier in the experiment than others. For example, the IDS1 and ADS1 speech stimuli appeared on trials 1, 2, 5, or 6, while the IDS8 and ADS8 speech stimuli always occurred on the final two trials (trial number 15 or 16). This means that the interpretation of the correlations between individual speech stimuli must also take into account that these stimuli tend to be occurring in earlier or later portions of the experiment (when infants are more or less attentive and show longer looking times in general). To further investigate the impact of trial number on by-item correlations in looking time, we fit an interaction model testing whether the magnitude of the item-level correlation depended on the trial number for a given session. We fit a linear mixed-effects model predicting item-level looking time in Session 2 from the interaction between item-level looking time in Session 1 and trial number in Session 1 (trial numbers across sessions are almost always identical). The model included random intercepts for participant, item, and lab, as well as random slopes for item-level looking time and trial number in Session 1 for participant and lab. We indeed found that the magnitude of the item-level correlations in looking time between sessions depended on trial number, `r apa_print(item_lt_relationship_control_trial_num)$full_result[["LT_Test_c_trial_num_1_c"]]`, with the strength of the relation between sessions declining as trial number increased. While trial number was a strong predictor of Session 2 looking time, `r apa_print(item_lt_relationship_control_trial_num)$full_result[["trial_num_1_c"]]`, item-level looking in Session 1 only marginally predicted Session 2 looking when controlling for trial number, `r apa_print(item_lt_relationship_control_trial_num)$full_result[["LT_Test_c"]]`. Variation in item-level correlations is therefore at least partially due to the ordering of the stimuli in the experiment, rather than a sole function of differences between the stimuli *per se*.

```{r  sfig10, fig.cap="Correlations in average looking time (in s) between Session 1 and 2 by item.", fig.align="center",fig.height = 16, fig.width = 12}
corr_lt_p3
```

\newpage

# S9. By-item-pair preference scores across sessions

```{r}
#organizing by stimulus number
#computes stimulus-level preference scores by pairing IDS1-ADS1, IDS2-ADS2, etc.
#these two trials always occurred consecutively across all lists and sessions
agg_by_subj_item_paired_pref <- data_clean  %>%
  mutate(
    trial_pair_num = case_when(
      trial_num %in% c(1,2) ~ 1,
      trial_num %in% c(3,4) ~ 2,
      trial_num %in% c(5,6) ~ 3,
      trial_num %in% c(7,8) ~ 4,
      trial_num %in% c(9,10) ~ 5,
      trial_num %in% c(11,12) ~ 6,
      trial_num %in% c(13,14) ~ 7,
      trial_num %in% c(15,16) ~ 8
    )
  ) %>%
  select(Subject,Subject_Unique,Gender,Method, preterm,Session, Condition, stimulus,trial_pair_num, LT,Lab) %>% 
  mutate(stimulus = str_remove(stimulus,"Retest")) %>%
  mutate(stimulus = str_remove(stimulus,"retest")) %>%
  filter(stimulus %in% c("ADS1","IDS1","ADS2","IDS2","ADS3","IDS3","ADS4", "IDS4","IDS5","ADS5","ADS6","IDS6","ADS7","IDS7","ADS8","IDS8")) %>%
  separate(stimulus,into = c("cond","stimulus_num"),sep="(?<=[A-Za-z])(?=[0-9])",remove=TRUE) %>%
  select(-cond) %>%
  group_by(Method, Lab,Subject, Subject_Unique, Gender, preterm,Session,stimulus_num,trial_pair_num) %>%
  pivot_wider(names_from = Condition,values_from=LT) %>%
  mutate(Diff = IDS - ADS, 
           Prop = IDS / (IDS + ADS)) %>%
  pivot_wider(names_from = Session, values_from =c(IDS,ADS,Diff,Prop,trial_pair_num))

#not plotted
optional_fig <- ggplot(agg_by_subj_item_paired_pref ,aes(Diff_1,Diff_2))+
  geom_point()+
  geom_smooth(method="lm",color="black")+
  facet_wrap(~stimulus_num,ncol=4)+
  theme_cowplot()+
  theme(legend.position="none")+
  xlab("IDS difference score in first session")+
  ylab("IDS difference score in second session")

#fit mixed-effects model predicting IDS preference in Session 2 from IDS preference in Session 1
#models including more complex random effects structure (slopes for IDS preference in Session 1 and by-item random effects) led to non-convergence
item_pref <- lmer(Diff_2~Diff_1+(1|Subject_Unique)+(1|Lab),data=agg_by_subj_item_paired_pref, control=lmerControl(optimizer="bobyqa"))
apa_lm_item_pref <- apa_print(item_pref)
```

We also inspected on a more fine-grained item level whether IDS preference in Session 1 was related to IDS preference in Session 2.
To do so, we exploited the fact the specific IDS and ADS stimuli were paired together in test orders in both sessions, such that one IDS stimulus (e.g., IDS1) always occurred adjacently to a specific ADS stimulus (e.g., ADS1).
We therefore computed stimulus-specific IDS preference scores by calculating the difference in raw looking time for each of the eight IDS-ADS stimulus pairs for each participant (whenever both trials in a given pair were available).
We then fit a linear mixed-effects model predicting stimulus-specific IDS preference in Session 2 from stimulus-specific IDS preference in Session 1, including by-participant and by-lab random intercepts (models with more complex random effects structure, including by-item random effects, failed to converge).
There was a marginal, but non-significant relation in stimulus-specific IDS preference between the two test sessions (Table 6).

```{r}
apa_table(
  apa_lm_item_pref$table
  , caption = "Linear mixed-effects model results predicting IDS preference in Session 2 from IDS preference in Session 1 at the stimulus level."
)
```

# S10. Overall looking times and test-retest IDS preference

## S10.1. Correlations between overall looking time and IDS preference

```{r}
#integrate mean looking time data with paired IDS preference dataset
all_agg_subjects_paired_retest <- all_agg_subjects_paired_retest %>%
  left_join(select(agg_by_subj_paired,Lab, Method, Subject, Subject_Unique,days_between_sessions,mean_lt_1, mean_lt_2))

#compute correlations
session1_lt_pref_correlation <-cor.test(all_agg_subjects_paired_retest$mean_lt_1,
         all_agg_subjects_paired_retest$Diff_1)
session2_lt_pref_correlation <-cor.test(all_agg_subjects_paired_retest$mean_lt_2,
         all_agg_subjects_paired_retest$Diff_2)
```

We also investigated whether preferential looking behavior varied as a function of infants' tendencies to look for longer or shorter periods of time on average across all stimuli. We found no evidence for a correlation between infants' average looking time and the magnitude of IDS preference (Figure 10), either in Session 1, `r apa_print(session1_lt_pref_correlation)$full_result`, or in Session 2, `r apa_print(session2_lt_pref_correlation)$full_result`.

```{r  fig10, fig.cap="Correlations between average looking time (in s) and IDS preference in (A) Session 1 and (B) Session 2.", fig.align="center"}

#plot correlations
p1 <- ggplot(all_agg_subjects_paired_retest,aes(mean_lt_1,Diff_1))+
  geom_hline(yintercept=0.5,linetype="dashed")+
  geom_point(size=1.5)+
  geom_smooth(method="lm")+
  theme_cowplot()+
  ggtitle("SESSION 1")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylab("IDS preference in Session 1")+
  xlab("Average looking time (in s) in Session 1")+
  coord_cartesian(xlim=c(0,16),ylim=c(-10,10))+
  theme(axis.title=element_text(size=11))
p2 <- ggplot(all_agg_subjects_paired_retest,aes(mean_lt_2,Diff_2))+
  geom_hline(yintercept=0.5,linetype="dashed")+
  geom_point(size=1.5)+
  geom_smooth(method="lm")+
  theme_cowplot()+
  ggtitle("SESSION 2")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylab("IDS preference in Session 2")+
  xlab("Average looking time (in s) in Session 2")+
  coord_cartesian(xlim=c(0,16),ylim=c(-10,10))+
  theme(axis.title=element_text(size=11))
plot_grid(p1,p2,labels=c("A","B"))
```

## S10.2. Does average looking time moderate test-retest reliability?

```{r}
#mean-centering average looking time
all_agg_subjects_paired_retest <- all_agg_subjects_paired_retest %>%
  ungroup() %>%
  mutate(
    mean_lt_1_c = mean_lt_1-mean(mean_lt_1),
    mean_lt_2_c = mean_lt_2-mean(mean_lt_2),
         )

# Adding age to model
mod_lmer_lt <- lmer(Diff_2 ~ Diff_1_c*mean_lt_1_c+(1+ Diff_1_c|Lab),
              data=all_agg_subjects_paired_retest,REML=T)

# Note models with more complex random effects structure did not converge (singular boundary fit), though they yielded qualitatively equivalent results.
## random effects structure: (1+ Diff_1_c*mean_lt_1_c|Lab) --> singular boundary fit
## random effects structure: (1+ Diff_1_c+mean_lt_1_c|Lab) --> singular boundary fit

lt_coefs<-summary(mod_lmer_lt)$coefficients
```

Do longer lookers or shorter lookers show a tendency towards higher test-retest reliability?
Next, we tested whether infants' tendency to look for longer or shorter periods (during Session 1) moderated test-retest reliability.
We fit a linear mixed-effects model predicting IDS preference in Session 2 from the interaction of IDS preference in Session 1 and average looking time in Session 1. The model included a by-lab random intercept and a by-lab random slope for IDS preference (Session 1). Average looking time during Session 1 did not significantly moderate test-retest reliability, $\beta$=`r round(lt_coefs[4,1],2)`, *SE*=`r round(lt_coefs[4,2],3)`, *t*(`r round(lt_coefs[4,3],1)`)=`r round(lt_coefs[4,4],2)`, *p*=`r printp(lt_coefs[4,5])`. The direction of this marginal, non-significant effect is consistent with a slight increase in test-retest reliability as average looking time increases (Figure 11). However, overall, we find no significant evidence for robust differences between long and short lookers.

```{r  fig11, fig.cap="Correlations between average looking time (in s) and IDS preference for infants with shorter looking (left panel) and longer looking (right panel).", fig.align="center"}
all_agg_subjects_paired_retest <- all_agg_subjects_paired_retest %>%
  mutate(looking_bin = cut(mean_lt_1, breaks = 2, labels = c("shorter looking","longer looking")))

ggplot(all_agg_subjects_paired_retest,aes(Diff_1,Diff_2))+
  geom_point(size=1.5)+
  geom_smooth(method="lm",color="red")+
  theme_cowplot()+
  facet_wrap(~looking_bin)+
  xlab("IDS preference in Session 1")+
  ylab("IDS preference in Session 2")
  
```

# S11. Changes in looking time and preferential looking across trials

## S11.1. Changes in looking time across trials

In this section, we explore how looking time changes across trials --- a major source of variation in infants' general looking behavior.
In general, and as expected, looking time shows a steep decline across trials, both in the first and second session (Figure 13).

```{r fig13, fig.cap="(A) Looking time (in s) across trials, facetted by Session (left panel: first Session; right panel: second session). The blue line represents a loess fit through the data. Error bands represent 95% CIs. Boxplots show the distribution of looking times within each trial number. (B) Looking time across trials split by condition (IDS vs. ADS) and Session (facetted). Each line represents a loess fit, with 95% CI error bands.", fig.align="center"}
data_clean <- data_clean %>%
  mutate(session_name = paste0("Session ",Session))
p1 <- ggplot(data_clean,aes(trial_num,LT))+
  geom_boxplot(aes(group=trial_num),alpha=0.1)+
  geom_smooth(method="loess")+
  theme_cowplot()+
  facet_wrap(~session_name)+
  xlab("Trial number")+
  scale_x_continuous(breaks=seq(2,16,2))+
  ylab("Looking time (in s)")
p2 <- ggplot(data_clean,aes(trial_num,LT,color=Condition))+
  geom_point(alpha=0.05)+
  geom_smooth(method="loess")+
  theme_cowplot()+
  facet_wrap(~session_name)+
  xlab("Trial number")+
  ylab("Looking time (in s)")+
  scale_x_continuous(breaks=seq(2,16,2))+
  scale_color_brewer(palette="Set1",direction=-1)+
  theme(legend.background = element_rect( fill = "white"),legend.position=c(0.8,0.7))
plot_grid(p1,p2,labels=c("A","B"),ncol=1)
```

## S11.2. Changes in preferential looking across trials

```{r warning=FALSE, message=FALSE, echo=FALSE, results= 'hide'}
#create a data frame of paired IDS preferences for each subject
agg_by_subj_paired_preference <- data_clean %>%
  #fix one issue in stimulus naming
  mutate(
    stimulus = case_when(
      Subject == "M328" & Session == 1 & trial_num == 5 ~ "IDS1",
      TRUE ~ stimulus
    )
  ) %>%
  #split the stimulus column into a column stimulus_name and stimulus_id based on where the numeric value appears
  separate(stimulus, into = c("stimulus_name", "stimulus_id"), sep = "(?<=[A-Za-z])(?=[0-9])",remove=FALSE) %>%
  ungroup() %>%
  #combine trial numbers into pairs with 1,2 being 1, 3,4 being 2, etc.
  mutate(trial_pair = ceiling(trial_num / 2)) %>%
  #relocate trial_pair column after trial_num
  relocate(trial_pair,.after = "trial_num") %>%
  #throw out the odd stimulus trials with MB in them
  filter(!grepl("MB",stimulus_name)) %>%
  select(Subject,Subject_Unique,Lab,Session,Age,days_between_sessions,total_trial_n,stimulus_id,stimulus_name,trial_pair,LT,log_lt) %>%
  group_by(Subject,Subject_Unique,Lab,Session,Age,days_between_sessions,total_trial_n,stimulus_id,trial_pair) %>%
  #pivot wider to get IDS and ADS looking times (LT) for each trial; also get log looking times for IDS and ADS from column log_lt
  pivot_wider(names_from = stimulus_name,values_from = c(LT,log_lt)) %>%
  mutate(
    Diff = LT_IDS - LT_ADS,
    Prop = LT_IDS / (LT_IDS + LT_ADS),
    Diff_log_lt = log_lt_IDS - log_lt_ADS,
    Prop_log_lt = log_lt_IDS / (log_lt_IDS + log_lt_ADS)
  ) %>%
  mutate(session_name = paste0("Session ",Session)) %>%
  mutate(
    trial_pair_c = trial_pair -4.5
  )

#Does IDS preference change across trial pairs?
# Fit linear mixed-effects model
m_diff_trial_pair <- lmer(Diff ~ trial_pair_c + (1+trial_pair_c|Subject_Unique) + (1+trial_pair_c|Lab), data = agg_by_subj_paired_preference)
```

```{r fig14, fig.cap="IDS preference (IDS-ADS) for pairs of (adjacent) trials, for Session 1 (left panel) and Session 2 (right panel). Error bars represent +1/-1 SEs. The blue line represents a loess fit through the data with 95% confidence bands.", fig.align="center"}
#plot average Diff by trial_pair with standard error bars
ggplot(agg_by_subj_paired_preference,aes(x=trial_pair,y=Diff))+
  geom_hline(yintercept=0, linetype="dashed")+
  stat_summary(fun=mean,geom="point",size=3)+
  #stat_summary(fun=mean,geom="line")+
  stat_summary(fun.data=mean_se,geom="errorbar",width=0)+
  geom_smooth(method="loess")+
  xlab("Trial pair")+
  ylab("Average IDS preference (Difference)")+
  theme_cowplot()+
  facet_wrap(~session_name)
```

The most important question for measuring IDS preference is whether infants' preferential looking to IDS shows systematic changes across the course of the experiment.
To investigate changes in preferential looking across the experiment, we grouped IDS and ADS trials into adjacent pairs (again exploiting the fact that IDS and ADS trials always occurred on adjacent trials, see S9) and computed difference scores for each pair.
Figure 14 depicts the change in average IDS preference across each session.
To test whether IDS preference changed across the experiment, we fit a linear mixed-effects model predicting IDS preference (difference score per trial pair) from trial pair (centered), including by-participant and by-lab random intercepts and by-participant and by-lab random slopes for trial pair. 
We did not find a significant effect of trial pair, `r apa_print(m_diff_trial_pair)$full_result$trial_pair_c`.

# S12. Decomposing sources of variance

```{r, warning=F, message=F,results='hide'}
#preparing the data
data_clean <- data_clean %>%
  mutate(condition_num = ifelse(Condition=="ADS",-0.5,0.5)) %>%
  group_by(Subject_Unique) %>%
  #centering predictors using the average within participant
  #because this is required by r2mlm
  mutate(condition_c= condition_num-mean(condition_num,na.rm=TRUE),
         trial_num_c=trial_num-mean(trial_num),
         session_c = Session-mean(Session),
         age_c = Age-mean(Age,na.rm=TRUE)) %>%
  mutate(average_age=mean(Age,na.rm=TRUE)) %>%
  mutate(
    method_hpp_et = ifelse(Method=="Eyetracking",1,0),
    method_hpp_cf = ifelse(Method=="central fixation",1,0)
  ) %>%
  mutate(
    NAE = ifelse(Lab=="InfantCog-UBC"|Lab=="infantll-madison", 1, 0)
  )

model_formulas <- c(
  "log_lt ~ 1+trial_num_c+(1+trial_num_c|Subject_Unique)",
  "log_lt ~ 1+condition_c+trial_num_c+(1+condition_c+trial_num_c|Subject_Unique)",
  "log_lt ~ 1+session_c+condition_c+trial_num_c+(1+session_c+condition_c+trial_num_c|Subject_Unique)",
  "log_lt ~ 1+session_c+condition_c+trial_num_c+average_age+NAE+method_hpp_et+method_hpp_cf+days_between_sessions+(1+session_c+condition_c+trial_num_c|Subject_Unique)"
)

#helper function to take a formula string and fit an lmer model
fit_model <- function(formula_string){
  model <- lmer(formula_string,data=data_clean, REML=TRUE,control=lmerControl(optimizer="bobyqa"))
  return(model)
}

model_data <- data.frame(model_formulas = model_formulas,formula_name=c("Trial Number","Trial Number + Condition","Trial Number + Session + Condition", "Full Model")) %>%
  mutate(model = map(model_formulas,fit_model)) %>%
  mutate(summary = map(model,summary)) %>%
  mutate(r2mlm = map(model, r2mlm,bargraph=FALSE)) %>%
  mutate(icc = map(model,performance::icc))

#make sure multicollinearity is ok in the final model
check_collinearity(model_data$model[[4]])
```

Finally, to understand the main sources of variation in infants' looking behavior in more depth, we fit successively more complex linear mixed-effects models predicting (log-transformed) looking times including the main predictors that explained substantial variance or were of primary interest.
We fit these models on the level of individual trials, in order to explore sources of variation at the trial-, session-, and participant-level simultaneously.
Due to limitations of the `r2mlm` package, we could only include a single random effect and so chose to include by-participant random effects (and no by-lab random effects), given our primary interest in understanding sources of variance within and between participants.
First, we sequentially included the within-participant predictors (trial number, then condition, then session) that explained the most variation in infant looking behavior, including a by-participant random intercept and random slopes for each predictor.
The final model included fixed effects for the within-participant predictors for trial number, condition, and session (including a by-participant random intercept and random slopes for these predictors) and fixed effects for the between-participant predictors age, method, language background, and days between test sessions.
Table XX shows the results of this final model - note that no evidence of multi-collinearity was observed.
We then used the R packages `r2mlm` and `performance` to decompose variance in looking behavior explained within each model.
We show the results of variance decomposition in Figure XX and Table XX.
In general, the model including the full set of predictors could explain a substantial amount of total variance in infants' looking behavior (Figure XX), mainly driven by variance explained by mean variation between participants and the variation explained by the slopes varying within participant (in particular trial number, as well as condition and session).
The between-participant fixed effects in the model explained only a small proportion of additional variance.
Table XX shows the intra-class correlation (ICC) for each mixed-effects model.
The ICC represents the proportion of variance in looking behavior that can be attributed to participant grouping in the data.
It can be interpreted as indicating to what extent individual measurements of looking behavior within the same participant resemble one another (i.e., how reliable they are).
We find that ultimately about a third of the variance (~0.3) could be attributed to participant grouping in the data once including the within-participant predictors related to trial number, session, and condition.

\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup

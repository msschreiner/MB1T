---
title             : "Limited evidence of test-retest reliability in infant-directed speech preference in a large preregistered infant experiment"
shorttitle        : "test-retest reliability of infant-directed speech preference"

author: 
  - name          : "Melanie S. Schreiner"
    affiliation   : "1,2"
    corresponding : yes    # Define only one corresponding author
    address       : "Gosslerstr. 14, 37073 Göttingen"
    email         : "melanie.schreiner@psych.uni-goettingen.de"
  - name          : "Martin	Zettersten"
    affiliation   : "3,4"
  - name          : "Christina Bergmann"
    affiliation   : "5"
  - name          : "Michael C. Frank"
    affiliation   : "6"
  - name          : "Tom	Fritzsche"
    affiliation   : "7"
  - name          : "Nayeli	Gonzalez-Gomez"
    affiliation   : "8" 
  - name          : "Kiley	Hamlin"
    affiliation   : "9"
  - name          : "Natalia	Kartushina"
    affiliation   : "10"
  - name          : "Danielle J.	Kellier"
    affiliation   : "11"
  - name          : "Nivedita	Mani"
    affiliation   : "1,2"
  - name          : "Julien	Mayor"
    affiliation   : "10"
  - name          : "Jenny	Saffran"
    affiliation   : "3"
  - name          : "Mohinish	Shukla"
    affiliation   : "12"
  - name          : "Priya	Silverstein"
    affiliation   : "13, 14"
  - name          : "Melanie	Soderstrom"
    affiliation   : "15"
  - name          : "Matthias	Lippold"
    affiliation   : "1,2"

affiliation:
  - id            : "1"
    institution   : "University of Goettingen"
  - id            : "2"
    institution   : "Leibniz Science Campus PrimateCognition"
  - id            : "3"
    institution   : "University of Wisconsin-Madison"
  - id            : "4"
    institution   : "Princeton University"
  - id            : "5"
    institution   : "Max Planck Insitute for Psycholinguistics"
  - id            : "6"
    institution   : "Stanford University"
  - id            : "7"
    institution   : "University of Potsdam"
  - id            : "8"
    institution   : "Oxford Brookes University"
  - id            : "9"
    institution   : "University of British Columbia"  
  - id            : "10"
    institution   : "University of Oslo"
  - id            : "11"
    institution   : "University of Pennsylvania"
  - id            : "12"
    institution   : "Università di Padova"
  - id            : "13"
    institution   : "Institute for Globally Distributed Open Research"
  - id            : "14"
    institution   : "Ashland University"
  - id            : "15"
    institution   : "University of Manitoba"

    
authornote: |
  **Acknowledgements.** This work was supported in part by a Leibniz ScienceCampus Primate Cognition seed fund awarded to MSc and ML, a grant from the Research Council of Norway (project number 301625) and its Centres of Excellence funding scheme (project number 223265) awarded to NK, an ERC Grant (agreement number 773202 – ERC 2017, "BabyRhythm") awarded to MSh, a ManyBabies SSHRC Partnership Development Grant awarded to MSo, and a grant from the NSF awarded to MZ (NSF DGE-1747503). 
  
  **Conflict of Interest Statement.** The authors declare that there are no conflicts of interest for this work. 
  
  **Data Availability Statement.** All code for reproducing the paper is available at https://github.com/msschreiner/MB1T. Data and materials are available on OSF (https://osf.io/zeqka/?view_only=e027502f4e7f49408cfb2cba38f7b506).
  
  **CRediT author statement.** Outside of the position of the first, the second, and the last author, authorship position was determined by sorting authors' last names in alphabetical order. An overview of authorship contributions following the CRediT taxonomy can be viewed here: https://docs.google.com/spreadsheets/d/1jDvb0xL1U6YbXrpPZ1UyfyQ7yYK9aXo002UaArqy35U/edit?usp=sharing.

abstract: |
 Test-retest reliability --- establishing that measurements remain consistent across multiple testing sessions --- is critical to measuring, understanding, and predicting individual differences in infant language development. However, previous attempts to establish measurement reliability in infant speech perception tasks are limited, and reliability of frequently-used infant measures is largely unknown. The current study investigated the test-retest reliability of infants' preference for infant-directed speech (hereafter, IDS) over adult-directed speech (hereafter, ADS) in a large sample (*N*=158) in the context of the ManyBabies1 collaborative research project (hereafter, MB1; Frank et al., 2017; ManyBabies Consortium, 2020). Labs of the original MB1 study were asked to bring in participating infants for a second appointment retesting infants on their IDS preference. This approach allows us to estimate test-retest reliability across three different methods used to investigate preferential listening in infancy: the head-turn preference procedure, central fixation, and eye-tracking. Overall, we find no consistent evidence of test-retest reliability in measures of infants' speech preference (overall *r* = .09, 95% CI [-.06,.25]). While increasing the number of trials that infants needed to contribute for inclusion in the analysis revealed a numeric growth in test-retest reliability, it also considerably reduced the study's effective sample size.  Therefore, future research on infant development should take into account that not all experimental measures may be appropriate for assessing individual differences between infants.
  
  
keywords          : "language acquisition; speech perception; infant-directed speech; adult-directed speech; test-retest reliability"
wordcount         : "3998"

bibliography      : ["r-references.bib"]

figsintext        : yes
floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library(papaja)
library(dplyr)
library(tidyr)
library(readxl)
library(lme4)
library(readr)
library(langcog)
library(ggthemes)
library(cowplot)
library(tidyverse)
library(here)
library(knitr)
library(kableExtra)
library("langcog")
library(lmerTest)
library(metafor)
library(magick)
library(ggExtra)
library(psych)
library(pwr)

read_path <- here("data","processed")
write_path <- here("data","processed")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed, warning=F, message = F)
```

Obtaining a quantitative measure of infants' cognitive abilities is an extraordinarily difficult endeavor. 
The most frequent way to assess what infants know or prefer is to track overt behavior.
However, measuring overt behavior at early ages presents many challenges: participants' attention span is short, they do not follow instructions, their mood can change instantly, and their behavior is often inconsistent. 
Therefore, most measurements are noisy and the typical sample size of an infant study is small (around 20 infants per group), resulting in low power [@oakes2017sample]. 
In addition, there is individual and environmental variation that may add even more noise to the data [e.g., @johnson2010using]. 
Despite these demanding conditions, reliable and robust methods for assessing infants’ behavior are critical to understanding development.

In order to address these challenges, the ManyBabies collaborative research consortium was formed to conduct large-scale, conceptual, consensus-based replications of seminal findings to identify sources of variability and establish best practices for experimental studies in infancy [@frank2017collaborative]. 
The first ManyBabies collaborative research project [hereafter, MB1, @manybabies2020quantifying] explored the reproducibility of the well-studied phenomenon that infants prefer infant-directed speech (hereafter, IDS) over adult-directed speech [hereafter, ADS, @cooper1990preference]. 
Across many different cultures, infants are commonly addressed in IDS, which typically is characterized by higher pitch, greater pitch range, and shorter utterances, compared to the language used between interacting adults [@fernald1989cross]. 
A large body of behavioral studies finds that infants show increased looking times when hearing IDS compared to ADS stimuli across ages and methods [@cooper1990preference; see @dunst2012preference for a meta-analysis]. 
This attentional enhancement is also documented in neurophysiological studies showing increased neural activation during IDS compared to ADS exposure [@naoi2012cerebral; @zangl2007increased]. 
IDS has also been identified as facilitating early word learning. 
In particular, infants’ word segmentation abilities [@floccia2016british; @schreiner2017listen; @singh2009influences; @thiessen2005infant] and their learning of word-object associations [@graf2013infant; @ma2011word] are enhanced in the context of IDS. 
In sum, several lines of evidence suggest that IDS is beneficial for early language development.

Within MB1, 67 labs contributed data from 2,329 infants showing that babies generally prefer to listen to IDS over ADS. 
Nevertheless, the overall effect size of *d* = 0.35 was smaller than a previously reported meta-analytic effect size of *d* = 0.67 [@dunst2012preference].
The results revealed several additional factors that influenced the effect size. 
First, older infants showed a larger preference of IDS over ADS. 
Second, the stimulus language was linked to IDS preference, with North American English learning infants showing a larger IDS preference than infants learning other languages.
Third, comparing the different methods employed, the head-turn preference procedure yielded the highest effect size, while the central fixation paradigm and eye-tracking methods revealed smaller effects.
Finally, exploratory analyses assessed the effect of different inclusion criteria. 
Across methods, using stricter inclusion criteria led to an increase in effect sizes despite the larger proportion of excluded participants [see also @byers2021six].

However, there is a difference between a result being reliable in a large sample of infants and the measurement of an individual infant being reliable. 
In studies tracking individual differences, the measured behavior during an experimental setting is often used to predict a cognitive function or specific skill later in life. 
Individual differences research of this kind often has substantial implications for theoretical and applied work.
For example, research showing that infants’ behavior in speech perception tasks can be linked to later language development [see @cristia2014predicting for a meta-analysis] has the potential to identify infants at risk for later language delays or disorders. 
However, a necessary precondition for this link to be observable is that individual differences between infants can be measured with high reliability at these earlier stages, in order to ensure that measured inter-individual variation mainly reflects differences in children's abilities rather than measurement error. 
How reliable are the measures used in infancy research?

Previous attempts to address the reliability of measurements have typically been limited to adult populations [@hedge2018reliability;@oliveira_reliability_2023], or have been conducted with small sample sizes [e.g., @houston2007assessing; @colombo1988infant].
For example, @houston2007assessing tested 10 9-month-old infants' speech discrimination in a visual habituation procedure in two test sessions 1-3 days apart and found a large correlation (*r* = .7).
These data were subsequently included in a much larger systematic investigation of test-retest reliability in infant speech perception [@cristia2016test].
<!-- For example, @colombo1988infant used a paired-comparison task, in which infants were familiarized with a stimulus and presented with the familiarized and a novel stimulus side-by-side at test. 
Results indicated that infants’ novelty preference was extremely variable from task to task. 
Assessing infants' performance from one week to another revealed that infants' attention measures were moderately reliable.
However, reliability seemed to increase with the number of tasks infants completed in the younger age group, suggesting that reliability is influenced by the number of assessments. 
In addition, infants' performance from 4 to 7 months was longitudinally stable but somewhat smaller than week-to-week reliability. -->
@cristia2016test analyzed 13 different experiments assessing test-retest reliability in infant speech perception tasks, with the retest session occurring 0-18 days after the first session. 
The experiments were conducted at three different labs with different implementations of the individual studies.
Hence, it was only after completed data collection that the data was pooled together by the different labs revealing potential confounds.
Nevertheless, the results showed that reliability was extremely variable across the different experiments and labs and low overall (meta-analytic *r* = .07).

Against this background, the current study investigates test-retest reliability of infants' performance in a speech preference task. 
Within MB1, a multi-lab collaboration, we examine whether infants' preferential listening behavior to IDS and ADS is reliable across two different test sessions. 
We also investigate the influence of various moderators on the reliability of IDS preference (e.g., time between test and retest; infants’ language background).

Our study was faced with a critical design choice: what stimuli to use to assess test-retest reliability. 
One constraint on our study was that, since it was a follow-on to MB1, any stimulus we used would always be presented after the MB1 stimuli. 
One option would be simply to bring back infants and have them hear exactly the same stimulus materials. 
A weakness of this design would be the potential for stimulus familiarity effects, however, since infants would have heard the materials before. 
Further complicating matters, infants might show a preference for or against a familiar stimulus depending on their age [@hunter1988multifactor]. 
The ideal solution then would be to create a brand new stimulus set with the same characteristics.
Unfortunately, because of the process of how MB1 stimuli were created, we did not have enough normed raw recordings available to make brand new stimulus items that conformed to the same standards as the MB1 stimuli. We therefore chose an intermediate path: we reversed the ordering of MB1 stimuli. Average looking times in MB1 were always lower than 9s per trial, even for the youngest children on the earliest trials (the group who looked the longest on average), so most children in MB1 did not hear the second half of most trials. 
Thus, by reversing the order, we had a perfectly matched stimulus set that was relatively unfamiliar to most infants.
The disadvantage of this design was that infants who looked longer might be more likely to hear a familiar clip heard in the previous study. 
If infants then showed a familiarity preference --- an assumption which might not be true --- the end result could be to inflate our estimates of test-retest reliability slightly, since longer lookers would on average look longer at retest due to their familiarity preference. We view this risk as relatively low, but do note that it is a limitation of our design. 

The current study also explores whether there are any differences in test-retest reliability between three widely used methods: central fixation (CF), eye-tracking (ET), and the head-turn preference procedure (HPP).
Exploring differences in CF, ET, and HPP, @junge2020contrasting provide experimental and meta-analytic evidence in favor of using the HPP in speech segmentation tasks.
Similarly, the MB1 project reported an increase in the effect size for HPP compared to CF and ET [@manybabies2020quantifying].
HPP requires gross motor movements relative to other methods, such as CF and ET paradigms, for which subtle eye movements towards a monitor located in front of the child are sufficient.
One possible explanation for the stronger effects with HPP may be a higher sensitivity to the contingency of the presentation of auditory stimuli and infants’ head turns away from the typical forward-facing position.
While these findings suggest that HPP may be a more sensitive index of infant preference, they do not necessarily imply higher reliability for individual infants’ performance using HPP.
For example, @marimon2022 found no evidence for test-retest reliability when testing infants' prosodic preferences using the HPP method across three testing sessions, each 7-8 days apart on average.
It remains an open question whether the same measures that produce larger effect sizes at the group-level also have higher test-retest reliability for individual infants [@byers2021six]. 
Therefore, assessing the test-retest reliability of the different preference measures is crucial, so that researchers can make informed decisions about the appropriate methods for their particular research question. 
Critically, only measures with high test-retest reliability should be used for studies of individual differences.


# Method

## Preregistration

We preregistered the current study on the Open Science Framework (https://osf.io/v5f8t). Section S1 in the Supplementary Materials contains additional notes on the preregistration decisions and any deviations from the preregistered analytic plan.

## Data Collection

A call was issued to all labs participating in the original MB1 study on January 24th, 2018 [@manybabies2020quantifying]. The collection of retest session data was initially set to end on May 31st, 2018, one month after the end date of the original MB1 project. Due to the fact that the original MB1 project extended the time frame for data collection and the late start of data collection for the MB1 test-retest study, we also allowed participating labs to continue data collection past the scheduled end date.

## Participants

```{r,message=FALSE,warning=FALSE}
# importing dataset
df_all <- read_csv(here("data","processed","df_all.csv"))

# Calculating the variable days between the two test sessions
df_all <- df_all %>% 
  group_by(Subject,Subject_Unique,Lab) %>% 
  mutate(days_between_sessions=max(Age)-min(Age)) %>%
  ungroup() 

# Truncating the looking times per trial at 18 seconds
# consistent with decision from MB1
df_all <- df_all %>% 
  mutate(LT=ifelse(LT>18,18,LT)) 


#### Processing exclusions ####
# Excluding all children with session errors
# Based on the preregistration, participants must meet the inclusion criteria for both test and retest sessions, so we exclude participants who have a session error during either test session

##fix a few session error issues
#unify session error type and reclassify one instance as not a session error
df_all <- df_all %>%
  mutate(session_error_type = case_when(
    str_detect(session_error_type, "Coding") ~ "experimenter error",
    session_error_type %in% c("Retest-Stimuli used for first session","retest-timuli used for first session") ~ "experiment error",
    TRUE ~ session_error_type
  )
  ) %>%
  mutate(
    session_error = case_when(
      session_error_type == "Baby cried after second music trial, went in and asked mom if it's okay to continue, she tried to soothe him but baby cried again so we stopped" ~ "noerror",
      TRUE ~ session_error
    )
  )

excludeID <- unique(df_all$Subject_Unique[df_all$session_error=="error"])

#tabulate session errors
df_all_exclude <- df_all %>% 
  filter(Subject_Unique %in% excludeID)
  

df_all_exclude_type <- df_all_exclude %>% 
  filter(session_error=="error") %>%
  select(session_error_type) %>%
  distinct()

df_all_exclude_counts <- df_all_exclude %>%
  filter(session_error == "error") %>%
  group_by(Subject_Unique) %>%
  select(session_error,session_error_type) %>%
  distinct()

df_all <- df_all %>% 
  filter(!Subject_Unique %in% excludeID)

#exclude preterms
df_all_exclude_preterm <- df_all %>% 
  filter(preterm=="preterm") %>%
  select(Subject_Unique,preterm) %>%
  distinct()
df_all <- df_all %>%
  filter(preterm!="preterm")

#compute number of trial errors
df_all_exclude_trial_count <- df_all %>%
  filter(Condition!="training") %>%
  summarize(
    N=n(),
    trial_error = sum(trial_error=="error"),
    trial_error_percent = trial_error/N
  )

# exclude trials with trial error
df_all <- df_all %>% 
  filter(trial_error == "noerror")

#save data after removing exclusions
write_csv(df_all,here(write_path,"df_all_after_exclusions.csv"))

# number of labs
n_labs <- length(unique(df_all$Lab))
# number of excluded infants
n_exclude <- length(excludeID)
```

```{r,message=FALSE,warning=FALSE}
LT_MIN <- 2                # minumum LT for inclusion
MIN_TRIALS_PER_TYPE <- 1  # min trials per type for inclusion
LOOKING_MEASURE <- "raw" #how to compute aggregated looking time preferences per subject. raw = using raw (average) looking times. log = using log-transformed (average) looking times

# function for cleaning data 
# by using minimum LT for inclusion and minimum number of trials per condition,
# and removing training trials.
# also adding new columns: log-transformed looking data, number of looking trials below threshold
clean_data <- function(dataset, minimum_looking_time, minimum_trials_per_type, file_path) {
  data_clean <- dataset %>%
    #remove training trials
    filter(Condition != "training") %>%
    group_by(Subject, Subject_Unique,Session,Lab) %>%
    # log-transformation of LT
    mutate(log_lt = log(LT))  %>% 
    mutate(
      N_below_threshold_lt = sum(LT < minimum_looking_time)
      ) %>%
    #remove looking times below minimum looking time
    filter(LT >= minimum_looking_time) %>%
    #compute number of remaining trials of each type
    mutate(N_IDS = sum(Condition == "IDS"),
           N_ADS = sum(Condition == "ADS"),
           total_trial_n = N_IDS + N_ADS) %>%
    #remove rows where the number of IDS or ADS trials is below the minimum number of required trials
    filter(N_IDS >= minimum_trials_per_type & 
           N_ADS >= minimum_trials_per_type)
  
  #write the resulting data file
  write_csv(data_clean, file_path)
  
  return(data_clean)
}

#clean data based on minimum looking time and number of minimum trials
data_clean <- clean_data(df_all,
                         minimum_looking_time=LT_MIN,
                         minimum_trials_per_type=MIN_TRIALS_PER_TYPE,
                         file_path=here(write_path,"clean_data_minimum_trials_per_type_1.csv"))

#tally the count of trials removed for being below the looking time threshold
data_clean_exclude_trial_minlooking <- data_clean %>%
  distinct(Lab,Subject_Unique, Session,N_below_threshold_lt,total_trial_n) %>%
  ungroup() %>%
  summarize(
    sum_trials_below_threshold = sum(N_below_threshold_lt,na.rm=TRUE),
    sum_trial_n = sum(total_trial_n),
    percent_trials_below_threshold=sum_trials_below_threshold/sum_trial_n
  )

# function for aggregating LT and log_lt for each Subject, Session and Condition
aggregate_by_subject <- function(cleaned_data, file_path) {
  agg_subjects <- cleaned_data %>%
    group_by(Lab, Method, Subject, Subject_Unique,Language, Condition, Age,Session,total_trial_n,N_below_threshold_lt) %>%
    summarise(mean_log_lt = mean(log_lt),
              mean_lt = mean(LT)) %>%
    mutate(ConditionC = ifelse(Condition == "IDS", .5, -.5)) %>%
    mutate(language_recoded = case_when(
      Lab %in% c("InfantCog-UBC","infantll-madison") & Language == "English" ~ "American English",
      TRUE ~ Language
      )) %>%
    mutate(Native = ifelse(language_recoded == "American English", TRUE, FALSE))
  
  #write the resulting data file
  write_csv(agg_subjects, file_path)
  
  return(agg_subjects)
}

#aggregate data by-participant
agg_subjects <- aggregate_by_subject(data_clean,file_path=here(write_path,"agg_subjects_minimum_trials_per_type_1.csv"))

#summarize looking time across subjects
# calculating mean lt in seconds for Session and Condition
agg_subjects_lt <- agg_subjects %>%
  group_by(Condition, Session) %>%
  summarise(MeanLT = mean(mean_lt),
            SD_LT = sd(mean_lt)) %>%
  pivot_wider(names_from=Session,values_from=c(MeanLT, SD_LT),names_prefix="session_") %>%
  relocate(SD_LT_session_1, .before = MeanLT_session_2)

# function for creating a wide format of the dataset and calculating difference in lt and log_lt for IDS and ADS and the proportion of IDS looking
aggregate_by_subject_paired <- function(aggregated_subjects, file_path) {
  
  all_agg_subjects_paired <- aggregated_subjects %>%
    select(-ConditionC) %>%
    pivot_wider(names_from = Condition, values_from = c(mean_lt,mean_log_lt)) %>%
    rename(IDS = mean_lt_IDS,ADS = mean_lt_ADS) %>%
    mutate(Diff = IDS - ADS,
           Prop = IDS / (IDS + ADS),
           binary_pref = case_when(
             is.na(Diff) ~ NA_character_,
             Diff > 0 ~ "IDS_preference",
             Diff < 0 ~ "ADS_preference",
             Diff == 0 ~ "no_preference"
             ),
           Diff_log_lt = mean_log_lt_IDS - mean_log_lt_ADS,
           Prop_log_lt = mean_log_lt_IDS / (mean_log_lt_IDS+mean_log_lt_ADS)
           )
  
  tally_subject_sessions <- all_agg_subjects_paired %>%
    select(Subject_Unique,Diff) %>%
    group_by(Subject_Unique) %>%
    tally() 
  
  #remove subjects with less than 2 sessions
  subjects_to_remove <- tally_subject_sessions %>%
    filter(n<2) %>%
    pull(Subject_Unique)
  all_agg_subjects_paired <- all_agg_subjects_paired %>%
    filter(!(Subject_Unique %in% subjects_to_remove))
  
  #write data
  write_csv(all_agg_subjects_paired, file_path)
  
  return(all_agg_subjects_paired)
}

#create wide format summarizing data per participant
all_agg_subjects_paired <- aggregate_by_subject_paired(agg_subjects, file_path=here(write_path,"all_agg_subjects_paired_minimum_trials_per_type_1.csv"))

# function for creating a wide format of the dataset such that the difference scores for each Session become a separate variable, calculating the number of days between test and retest session, and mean Age for both testing sessions
aggregate_by_subject_paired_retest <- function(aggregated_subjects_paired, file_path) {
  
  all_agg_subjects_paired_retest <- aggregated_subjects_paired %>%
    group_by(Lab,Subject,Subject_Unique,Method,Language, language_recoded, Native) %>%
  pivot_wider(names_from=Session,values_from=c(Age,total_trial_n,N_below_threshold_lt,IDS,ADS,Diff,Prop,binary_pref,mean_log_lt_IDS,mean_log_lt_ADS,Diff_log_lt,Prop_log_lt),names_prefix="session_") %>% 
  mutate(days_between_sessions=Age_session_2-Age_session_1,
         Age = rowMeans(cbind(Age_session_2,Age_session_1),na.rm=TRUE)) %>%
  rename(Diff_1 = Diff_session_1, Diff_2 = Diff_session_2)
  
  #write data
  write_csv(all_agg_subjects_paired_retest, file_path)
  
  return(all_agg_subjects_paired_retest)
}

#create wide format summarizing differences scores by session per participant
all_agg_subjects_paired_retest <- aggregate_by_subject_paired_retest(all_agg_subjects_paired,file_path=here(write_path,"all_agg_subjects_paired_retest_minimum_trials_per_type_1.csv"))

# Getting the number of final participants
n <- length(unique(all_agg_subjects_paired$Subject_Unique)) # Final sample of participants
n_exclude_session <- length(unique(df_all$Subject_Unique))-n # Calculating Participants lost due to missing valid trial in either session 1 or session 2

# mean, minimum, and maximum age across both testing sessions
age_stats <- all_agg_subjects_paired_retest %>% 
  ungroup() %>%
  select(Age) %>% 
  summarise_at(vars(Age),funs(mean,min,max)) %>%
  mutate(
    age_in_months = round(mean/30.44,2)
  )

# Stats on number of days between test and retest
stats_daysbetween <- all_agg_subjects_paired_retest %>% 
  ungroup() %>% 
  select(days_between_sessions) %>% 
  summarise_at(vars(days_between_sessions),funs(mean,min,max))
```

Contributing labs were asked to re-recruit their monolingual participants between the ages of 6 to 12 months who had already participated in the MB1 project. 
If participating labs had not committed to testing either of these age groups, they were also allowed to re-recruit participants from the youngest age group of 3- to 6-month-olds and/or the oldest age group of 12- to 15-month-olds. 
Labs were asked to contribute half (*n*=16) or full samples (*n*=32); however, a lab's data was included in the study regardless of the number of included infants. 
The study was approved by each lab's respective ethics committee and parental consent was obtained for each infant prior to participation in the study.

Our final sample consisted of `r n` monolingual infants from `r n_labs` different labs (Table 1). 
In order to be included in the study, infants needed a minimum of 90% first language exposure, to be born full term with no known developmental disorders, and normal hearing and vision.
We excluded `r n_exclude+n_exclude_session` additional participants (see Data Exclusion section for details). 
The mean age of infants included in the study was `r round(age_stats$mean, 0)` days (range: `r age_stats$min` -- `r age_stats$max` days; approximately `r age_stats$age_in_months` months).

## Materials

### Visual stimuli.
The visual stimuli and instructions were identical to MB1.
For the CF paradigm and ET, labs used a multicolored static checkerboard as the fixation stimulus as well as a multicolored moving circle with a ringing sound as an attention-getter between trials.
For the HPP method, labs used their standard procedure, as in MB1.

### Speech stimuli.
We used the identical training stimuli of piano music from MB1.
A second set of naturalistic IDS and ADS recordings of mothers either talking to their infant or to an experimenter was created for the retest session by reversing the order of clips within each sequence of the original study.
This resulted in eight reordered sequences of natural IDS and eight reordered sequences of natural ADS with a length of 18 seconds each.
 
### Procedure.
Infants were retested using the identical procedure as during the first testing day: CF, HPP, or ET. 
Participating labs were asked to schedule test and retest sessions 7 days apart with a minimum number of 1 day and a maximum number of 31 days. 
However, infants whose time between test and retest exceeded 31 days were still included in the analyses (*n* = 3). 
The mean number of days between test and retest was `r round(stats_daysbetween$mean, 1)` (range: `r stats_daysbetween$min` - `r stats_daysbetween$max`).

A total of 18 trials, including two training, eight IDS, and eight ADS trials, were presented in one of four pseudo-randomized orders. 
Trial length was either infant-controlled or fixed depending on the lab’s standard procedure: a trial stopped either if the infant looked away for 2 seconds or after the total trial duration of 18 seconds. 
The online coding experimenter and the parent listened to music masked with the stimuli of the study via noise-cancelling headphones. 
If the experimenter was in an adjacent room separate from the testing location, listening to masking music was optional for the experimenter.
 
### Data exclusion.
In total, `r n_exclude+n_exclude_session` participants were excluded from the analysis.
`r length(df_all_exclude_preterm$Subject_Unique)` participants were excluded for being preterm (defined as a gestation time of less than 37 weeks).
`r length(df_all_exclude_counts$Subject_Unique)` participants were excluded due to session errors involving an experimenter error (e.g., inaccurate coding or presentation of retest stimuli on the first test session). 
Individual trials were excluded if they were marked as trial errors (`r round(df_all_exclude_trial_count$trial_error_percent,4) * 100`% of remaining trials), i.e., if the infant was reported as fussy, an experimental or equipment error occurred, or there was parental interference during the task (e.g., if the parent spoke with the infant during the trial).
Trials were also excluded if the minimum looking time of 2 s was not met (`r round(data_clean_exclude_trial_minlooking$percent_trials_below_threshold,4) * 100`% of the remaining trials). 
If a participant was unable to contribute at least one IDS and one ADS trial for either test or retest after trial-level exclusions, all data of that participant was excluded from the test-retest analyses (`r n_exclude_session` additional participants).

# Results

```{r labs, results="asis", message=FALSE, warning=FALSE}

# Summarize mean Age and number of Subjects for each lab, method, and language
lab_stats<-all_agg_subjects_paired_retest %>% select(Lab,Subject,Method,Age,Language) %>% group_by(Lab,Method,Language) %>% 
  summarise(Age=mean(Age),N=length(unique(Subject)))

# Assign column label to column 4
names(lab_stats)[4]<-"Mean age (days)"

# Create data set that includes language variable from main data set
#df_all_Language<-df_all %>% select(Lab,Method,Language) %>% group_by(Lab) %>% 
  #summarise(Method=as.character(unique(Method)[1]),Language=unique(Language[1]))

# Merge with lab stats
#lab_stats<-left_join(lab_stats,df_all_Language)

#slight edit to method naming
lab_stats <- lab_stats %>%
  mutate(Method = case_when(
    Method=="Eyetracking" ~ "eye-tracking",
    TRUE ~ Method
  ))

papaja::apa_table(lab_stats, 
                  caption = "Statistics of the included labs. n refers to the number of infants included in the final analysis.", 
                  format.args = list(digits = 0))

```

## IDS preference

```{r,message=FALSE,warning=FALSE}
# Linear mixed model assessing if session predicts differences in LT 
diff_session_1 <- lmer(Diff ~ Session + 
                (1 | Lab) + (1 | Subject), 
              data=all_agg_subjects_paired, REML=FALSE)

diff_session_2 <- lmer(Diff ~ Session + (1+Session|Lab) + (1|Subject),
                       data=all_agg_subjects_paired, REML=FALSE)
#this model yields a singular boundary fit but also yields similar results to the first model 

diff_session_coefs<-summary(diff_session_1)$coefficients
diff_session_coefs2<-summary(diff_session_2)$coefficients


# Subsetting to session 1 for t-test
all_agg_subjects_paired_s1 <- subset(all_agg_subjects_paired, all_agg_subjects_paired$Session=="1")

ttestSession1<-t.test(all_agg_subjects_paired_s1$Diff)

# Subsetting to session 2 for t-test

all_agg_subjects_paired_s2 <- subset(all_agg_subjects_paired, all_agg_subjects_paired$Session=="2")

ttestSession2 <- t.test(all_agg_subjects_paired_s2$Diff)

```

First, we conducted confirmatory analyses examining infants' preference for IDS in both sessions. Two-samples t-tests comparing the difference in average looking time between IDS and ADS to zero revealed that infants showed a preference of IDS over ADS in Session 1, `r apa_print(ttestSession1)$statistic`, and Session 2, `r apa_print(ttestSession2)$statistic`, replicating the main finding from MB1 (Table 2).
`r sum(all_agg_subjects_paired_s1$Diff>0)/length(all_agg_subjects_paired_s1$Diff)*100`% of infants in Session 1 and `r sum(all_agg_subjects_paired_s2$Diff>0)/length(all_agg_subjects_paired_s2$Diff)*100`% of infants in Session 2 showed a preference for IDS.
In order to test whether there was a difference in the strength of the preference effect across sessions, we fit a linear mixed-effects model predicting infants' average difference in looking time between IDS and ADS from test session (1 vs. 2), including by-lab and by-participant random intercepts.
There was no significant difference in the magnitude of infants’ preference between the two sessions, $\beta$=`r round(diff_session_coefs[2,1],2)`, *SE*=`r round(diff_session_coefs[2,2],2)`, *p*=`r printp(diff_session_coefs[2,5])`.

```{r descriptives_table1, results="asis",warning=FALSE,message=FALSE}

papaja::apa_table(agg_subjects_lt, 
                  caption = "Average looking times (in seconds) for each session and condition", 
                  format.args = list(digits = 2),
                  col.names=c("Trial type", "Session 1 Mean","Session 1 $SD$","Session 2 Mean", "Session 2 $SD$"))
```

## Reliability

```{r coef_table2, results="asis",warning=FALSE,message=FALSE}
# Linear mixed model predicting difference score of session 2 from difference score of session 1
mod_lmer <- lmer(Diff_2 ~ Diff_1 + (1|Lab),
              data=all_agg_subjects_paired_retest,REML=T)

#secondary preregistered model
# including an additional by-lab random slope for test session 1
# this model yields qualitatively equivalent results
mod_lmer_b <- lmer(Diff_2 ~ Diff_1 + (1+Diff_1|Lab),
              data=all_agg_subjects_paired_retest,REML=T)

# Coefficients for linear mixed model predicting difference score of session 2 from difference score of session 1
coefs <- summary(mod_lmer)$coef %>%
  as_data_frame %>%
  mutate_at(c("Estimate","Std. Error","df", "t value", "Pr(>|t|)"), 
            function (x) signif(x, digits = 3)) %>%
  rename(SE = `Std. Error`, 
         t = `t value`,
         p = `Pr(>|t|)`) %>%
  select(-df)
         
rownames(coefs) <- c("Intercept", "IDS Preference Session 1")

papaja::apa_table(coefs, 
                  caption = "Coefficient estimates from a linear mixed effects model predicting IDS preference in Session 2.", 
                  format.args = list(digits = 2))

# Preference Reversal
reversal <- all_agg_subjects_paired %>%
  ungroup() %>%
  select(Method,Subject_Unique, Session, binary_pref)

reversal <- reversal %>%
  group_by(Subject_Unique) %>%
  pivot_wider(names_from = Session, values_from = binary_pref)

names(reversal) <- c("Method", "Subject_Unique","Pref_Test" , "Pref_Retest")
  
reversal <- reversal %>%
  group_by(Subject_Unique) %>%
  mutate(reversal = case_when((Pref_Test == "IDS_preference" & Pref_Retest == "IDS_preference")  ~ 0,
                              (Pref_Test == "ADS_preference" & Pref_Retest == "ADS_preference")  ~ 0,
                              (Pref_Test == "IDS_preference" & Pref_Retest == "ADS_preference")  ~ 1,
                              (Pref_Test == "ADS_preference" & Pref_Retest == "IDS_preference")  ~ 1))

n_reversal <- length(reversal$reversal[reversal$reversal==1])
n_total <- length(reversal$reversal)

perc_reversal <- n_reversal/n_total*100

reversal$reversal <- as.factor(reversal$reversal)

# Figure for reversal (not shown)

reversal_plot <- left_join(all_agg_subjects_paired_retest, reversal)

reversal_plot$Diff <- reversal_plot$Diff_1 - reversal_plot$Diff_2

rev_plot <- ggplot(reversal_plot, aes(y=reversal_plot$Diff, x=reversal, fill=reversal))+
  geom_boxplot()+
  stat_summary(fun.y=mean, geom="point", shape=20, size=10, color="green", fill="green") +
  geom_jitter()

#t.test(reversal_plot$Diff[reversal_plot$reversal==0],reversal_plot$Diff[reversal_plot$reversal==1])

```

```{r}
# calculate simple correlation of difference score of session 1 and difference score of session 2
simplyOverallCorr<-cor.test(all_agg_subjects_paired_retest$Diff_1,
         all_agg_subjects_paired_retest$Diff_2)

#ICC
icc_data <- all_agg_subjects_paired_retest %>% ungroup() %>% select(Diff_1,Diff_2)
icc_results <- psych::ICC(icc_data,missing=FALSE, lmer=TRUE)
```

```{r fig.cap="Correlation between IDS Preference in Session 1 and Session 2 in each lab and method. Dots indicate individual participants. Error bands represent 95 percent confidence intervals. The dashed line indicates no preference (i.e., a value of zero) for the first and second session, respectively.", fig.align="center"}

all_agg_subjects_paired_retest <- all_agg_subjects_paired_retest  %>%
  mutate(
    Method_short=case_when(
      Method=="Eyetracking" ~ "ET",
      Method=="central fixation" ~ "CF",
      TRUE ~ Method
    )
  ) %>%
  mutate(
    Method=case_when(
      Method=="Eyetracking" ~ "eye-tracking",
      TRUE ~ Method
    )
  ) %>%
  unite(lab_method,Lab,Method_short,sep=", ", remove=FALSE) %>%
  left_join(lab_stats) %>%
  ungroup() %>%
  mutate(
    lab_method = fct_reorder(lab_method,`Mean age (days)`,.fun=max)
  )

ggplot(all_agg_subjects_paired_retest,aes(Diff_1,Diff_2,color=Method))+
  geom_vline(xintercept=0,linetype="dashed")+
  geom_hline(yintercept=0,linetype="dashed")+
  geom_point()+
  geom_smooth(method="lm")+
  xlab("IDS preference in first session")+
  ylab("IDS preference in second session")+
  facet_wrap(~lab_method,nrow=2)+
  theme(panel.background = element_rect(fill = "white",
                                colour = "black",
                                size = 0.5, linetype = "solid"),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  strip.background = element_blank())+
  scale_color_brewer(palette="Set1")+
  theme(legend.position=c(0.17,0.92),legend.title=element_text(size=8),legend.text=element_text(size=5), legend.key.height = unit(0.2, 'cm'), 
        legend.key.width = unit(0.2, 'cm'))+
  geom_label(aes(label=paste0("Mean Age: ",round(`Mean age (days)`,0)," days")),x=2.5,y=-7,size=2)
```

We assessed test-retest reliability in two planned, confirmatory analyses. 
First, we fit a linear mixed-effects model predicting IDS preference in Session 2 from IDS preference in Session 1, including a by-lab random intercept. 
The results revealed no significant relationship between IDS preference in Session 1 and 2 (Table 3). 
Second, we calculated the Pearson correlation coefficient. 
While a simple correlation coefficient might overestimate the test-retest reliability in our sample because it does not control for the differences between different labs and methods (HPP, CF, and ET), we felt it was important to also conduct a Pearson correlation as it is commonly used to assess reliability. 
The size of the correlation coefficient was not statistically different from zero and the estimate was small, `r apa_print(simplyOverallCorr)$full_result`. 
Moreover, no significant correlations emerged in each sample considered separately (Figure 1; see Supplementary Materials S3 for a meta-analytic approach).
 `r perc_reversal`% of the infants reversed their direction of preference for IDS versus ADS from the test to the retest session. 

```{r results="asis",warning=FALSE,message=FALSE}
#function for computing coefficient table from lmer model and correlation reliability
compute_coefs_reliability <- function(dataset) {
  #fit lmer reliability model
  mod_lmer <- lmer(Diff_2 ~ Diff_1 + (1|Lab),data=dataset,REML=T)
  #extract coefficients
  coef <- summary(mod_lmer)$coef %>%
    as_data_frame %>%
    mutate_at(c("Estimate","Std. Error","df", "t value", "Pr(>|t|)"), 
              function (x) signif(x, digits = 3)) %>%
    rename(SE = `Std. Error`, 
           t = `t value`,
           pvalue = `Pr(>|t|)`,
           estimate = Estimate) %>%
    #keep just main coefficient estimate
    slice(2) %>%
    #keep most important columns
    select(estimate, SE, pvalue)
  
  #compute correlation and add to summarized data
  correlation<-cor.test(dataset$Diff_1,dataset$Diff_2) 
  summarized_data <- coef %>%
    mutate(cor=as.numeric(correlation$estimate),
           df=as.numeric(correlation$parameter),
           pvalue2=as.numeric(correlation$p.value))
  
  #return
  summarized_data
}
coef_table <- all_agg_subjects_paired_retest %>%
  group_by(Method) %>%
  nest() %>%
  #compute coefficients
  mutate(coef_table = purrr::map(data, compute_coefs_reliability)) %>%
  #remove data
  select(-data) %>%
  #unpack
  unnest(cols=c(coef_table)) %>%
  select(-pvalue2,-df) %>%
  mutate(
    Method=case_when(
      Method=="Eyetracking" ~ "eye-tracking",
      Method=="central fixation" ~ "central fixation",
      TRUE ~ Method
    )
  )
  
papaja::apa_table(coef_table, 
                  caption = "Coefficient estimates from a linear mixed effects model predicting IDS preference in Session 2 and Pearson correlation coefficient for each method separately.", 
                  col.names = c("Method","beta","SE","p","Pearson r"),
                  format.args = list(digits = 2))
```

To investigate the test-retest reliability of each specific method, we computed Pearson correlation coefficients and the same mixed-effects model described above for HPP, CF, and ET separately (Table 4) in additional exploratory analyses. 
None of the three methods showed evidence of test-retest reliability. 
Neither the Pearson correlation coefficients nor the coefficients of the multilevel analysis were significant, all *p*-values > `r floor(min(coef_table$pvalue)*100)*0.01`.
In planned secondary analyses, we found that time between test sessions, participant age, method, and language background did not moderate the relationship between IDS preference in session 1 and session 2 (see Supplementary Materials S2).
Taken together, we find no significant evidence of test-retest reliability across our preregistered analyses.

## Exploratory analyses with different inclusion criteria

```{r,message=FALSE,warning=FALSE}

#helper function to combine all aggregation steps, with minimum number of trial pairs exclusion criterion applied
aggregate_complete <- function(dataset,min_trials) {
  
  cur_agg_subjects_paired_retest <- dataset %>%
    clean_data(minimum_looking_time=LT_MIN,minimum_trials_per_type=min_trials,file_path=here(write_path,paste0("clean_data_minimum_trials_per_type_",min_trials,".csv"))) %>%
    aggregate_by_subject(file_path=here(write_path,paste0("agg_subjects_minimum_trials_per_type_",min_trials,".csv"))) %>%
    aggregate_by_subject_paired(file_path=here(write_path,paste0("all_agg_subjects_paired_minimum_trials_per_type_",min_trials,".csv"))) %>%
    aggregate_by_subject_paired_retest(file_path=here(write_path,paste0("all_agg_subjects_paired_retest_minimum_trials_per_type_",min_trials,".csv")))
  
  return(cur_agg_subjects_paired_retest)
}

# Creating data sets for different number of minimum trial pairs included: 1, 2, 4, 6, and 8
min_trials<-c(1,2,4,6,8)

# create paired retest dataframes for each participant inclusion level based on the minimum number of trial pairs
paired_retest_df_by_trial_exclusions <- df_all %>%
  #expand and then nest data frame for each level of minimum trial pairs
  expand_grid(min_trials=min_trials) %>%
  group_by(min_trials) %>%
  nest() %>%
  #now aggregate data for each minimum trial exclusion level
  mutate(
    all_agg_subjects_paired_retest = purrr::map2(data,min_trials,aggregate_complete)
  ) %>%
  select(-data)

##### functions for computing and printing test-retest correlation
create_exclusion_dataset <- function(df, minimum_trials=1) {
  #create the dataset based on the minimum trial inclusion criterion
  filtered_data <- df %>%
    filter(min_trials==minimum_trials) %>%
    unnest(cols = c(all_agg_subjects_paired_retest))
  return(filtered_data)
}

compute_test_retest_correlation <- function(dataset,session_1_column="Diff_1",session_2_column="Diff_2") {
  #compute the test-retest correlation
  correlation <- cor.test(dataset[[session_1_column]],dataset[[session_2_column]])
  correlation
}

#compute correlations for various inclusion criteria
correlation_1 <- create_exclusion_dataset(paired_retest_df_by_trial_exclusions,minimum_trials=1) %>%
  compute_test_retest_correlation()
correlation_2 <- create_exclusion_dataset(paired_retest_df_by_trial_exclusions,minimum_trials=2) %>%
  compute_test_retest_correlation()
correlation_4 <- create_exclusion_dataset(paired_retest_df_by_trial_exclusions,minimum_trials=4) %>%
  compute_test_retest_correlation()
correlation_6 <- create_exclusion_dataset(paired_retest_df_by_trial_exclusions,minimum_trials=6) %>%
  compute_test_retest_correlation()
correlation_8 <- create_exclusion_dataset(paired_retest_df_by_trial_exclusions,minimum_trials=8) %>%
  compute_test_retest_correlation()
correlations <- c(correlation_1$estimate,correlation_2$estimate,correlation_4$estimate,correlation_6$estimate,correlation_8$estimate)

#ICC for restricted samples
icc_results_2 <- create_exclusion_dataset(paired_retest_df_by_trial_exclusions,minimum_trials=2) %>% ungroup() %>% select(Diff_1,Diff_2) %>% psych::ICC(missing=FALSE, lmer=TRUE)
icc_results_4 <- create_exclusion_dataset(paired_retest_df_by_trial_exclusions,minimum_trials=4) %>% ungroup() %>% select(Diff_1,Diff_2) %>% psych::ICC(missing=FALSE, lmer=TRUE)
icc_results_6 <- create_exclusion_dataset(paired_retest_df_by_trial_exclusions,minimum_trials=6) %>% ungroup() %>% select(Diff_1,Diff_2) %>% psych::ICC(missing=FALSE, lmer=TRUE)
icc_results_8 <- create_exclusion_dataset(paired_retest_df_by_trial_exclusions,minimum_trials=8) %>% ungroup() %>% select(Diff_1,Diff_2) %>% psych::ICC(missing=FALSE, lmer=TRUE)

```

To this point, all analyses were performed using the inclusion criteria from MB1, which required only that infants contribute at least one trial per condition for inclusion (i.e., one IDS and one ADS trial). 
However, more stringent inclusion criteria yielded larger effect sizes in MB1. 
We therefore conducted exploratory analyses assessing test-retest reliability after applying progressively stricter inclusion criteria, requiring two, four, six, and eight valid trials per condition. 
Applying stricter criteria --- and thereby increasing the number of test trials --- increased reliability numerically from *r* = `r round(min(correlations),2)` to *r* = `r round(max(correlations),2)` (Figure 2). 
In part due to a decrease in sample size, only one of these correlations was statistically significant (when requiring six trial pairs): two valid trial pairs, `r apa_print(correlation_2)$statistic`; four valid trial pairs, `r apa_print(correlation_4)$statistic`; six valid trial pairs, `r apa_print(correlation_6)$statistic`; eight valid trial pairs --- all trials in both sessions --- `r apa_print(correlation_8)$statistic`. 
The analyses provide tentative evidence that stricter inclusion criteria may lead to higher test-retest reliability, but at the cost of substantial decreases in sample size (see Supplementary Materials S4 for additional analyses, including moderator analyses using a more restricted sample).

```{r fig.cap="IDS preferences of both sessions plotted against each other for each inclusion criterion. \\textit{n} indicates the number of included infants, \\textit{r} is the Pearson correlation coefficient as the indicator for reliability.", fig.align="center",out.width = "5in",fig.height = 10, fig.width = 8}


# Creating scatterplot for each inclusion criterion
plot_exclusion_dataset <- function(current_dataset,minimum_trials=1,
                                   Limaxes=c(-4,10),
                                   session_1_column="Diff_1",
                                   session_2_column="Diff_2",
                                   xlab = "IDS preference in first session",
                                   ylab = "IDS preference in second session"
                                   ) {
  current_correlation <- cor.test(current_dataset[[session_1_column]],current_dataset[[session_2_column]])
  main_message <- paste0("Inclusion criterion: ",minimum_trials," trial pairs")
  plot(current_dataset[[session_1_column]],current_dataset[[session_2_column]],xlab = xlab,ylab=ylab,main = main_message,xlim = Limaxes,ylim = Limaxes)
abline(lm(current_dataset[[session_2_column]]~current_dataset[[session_1_column]]),col="red")
legend("topleft",legend = paste("r = ",round(current_correlation$estimate,2),"; n = ",current_correlation$parameter+2,sep=""))
}

par(mfrow = c(2, 2))
plot_exclusion_dataset(create_exclusion_dataset(paired_retest_df_by_trial_exclusions,minimum_trials=2),minimum_trials=2)
plot_exclusion_dataset(create_exclusion_dataset(paired_retest_df_by_trial_exclusions,minimum_trials=4),minimum_trials=4)
plot_exclusion_dataset(create_exclusion_dataset(paired_retest_df_by_trial_exclusions,minimum_trials=6),minimum_trials=6)
plot_exclusion_dataset(create_exclusion_dataset(paired_retest_df_by_trial_exclusions,minimum_trials=8),minimum_trials=8)
```

## Correlations between sessions for number of trials contributed and overall looking time 

In exploratory analyses, we also investigated whether there were stable individual differences in (a) the number of trials an infant contributed across the two test sessions and (b) infants' overall looking times.

### Number of trials contributed

```{r}
# importing dataset (after excluding session and trial errors)
df_all <- read_csv(here(read_path,"df_all_after_exclusions.csv"))

#some minor data cleaning
#not cleaning based on minimum number of trials in order to see even infants contributing at least one trial
df_all <- df_all %>%
  filter(Condition != "training") %>%
  filter(LT >= 2) # minimum LT for inclusion

# calculate number of trials that each participant contributes per session
df_all_trials <- df_all %>%
  group_by(Subject_Unique,Session, Method) %>%
  summarise(N_trials = length(trial_num)) %>%
  mutate(
    N_trials = as.numeric(N_trials), #probably not needed, just to be safe
    Method = as.factor(Method)) %>%
  mutate(
    Session = case_when(
      Session==1 ~ "Test",
      Session==2 ~ "Retest"
    )
  )

df_all_trials <- df_all_trials %>% 
  pivot_wider(names_from = Session,values_from = N_trials)

#compute correlation
cor_trials <- cor.test(df_all_trials$Test,df_all_trials$Retest)
cor_trials_out <- apa_print(cor_trials)
```

```{r sfig7, fig.cap = "Correlation between the number of trials contributed in Session 1 and Session 2. Each data point represents one infant. Colored lines represent linear fits for each method."}
# ordering levels of method as in language background plot (Figure 1 of Supplementary)
df_all_trials$Method <- factor(df_all_trials$Method, levels = c("central fixation", "Eyetracking", "HPP"))
# jittering individual data points in order to make overlapping data points visible
trials <- ggplot(df_all_trials, aes(x=Test, y=Retest))+
  geom_jitter(width=0.5,height=0.5,aes(colour = Method),alpha=0.5)+
  geom_smooth(aes(colour = Method,group=Method),method=lm, se=FALSE,alpha=0.3)+
  geom_smooth(method=lm, colour= "black")+
  scale_color_ptol(name = "Method") + 
  scale_x_continuous(seq(0,16,2),name="Number of Trials During Session 1")+
  scale_y_continuous(seq(0,16,2),name="Number of Trials During Session 2")+
  theme_cowplot()+
  theme(legend.position=c(0.02,0.8))
#plot
#trials
p1 <- ggMarginal(trials, groupColour = TRUE, groupFill = TRUE,alpha=0.3)
```

We found a strong positive correlation between number of trials contributed during the first and the second session `r cor_trials_out$full_result` (Figure 3A). 
In other words, if infants contributed a higher number of trials in one session, compared to other infants, they were likely to contribute a higher number of trials in their next session.
This finding is consistent with the hypothesis that how attentive infants are throughout an experiment (and hence how many trials they contribute) is a stable individual difference, at least for some infant looking time tasks.

### Overall looking times

```{r}
#Overall Correlations
agg_by_subj_paired <- data_clean %>%
  select(Subject,Subject_Unique,Age,Gender,Method, preterm,Session, Condition, trial_num,LT,Lab,days_between_sessions) %>% 
  group_by(Method, Lab,Subject, Subject_Unique, Age,Gender, preterm,days_between_sessions,Session) %>%
  summarize(mean_lt=mean(LT),
            N=n()) %>%
  pivot_wider(names_from = Session, values_from = c(mean_lt,N,Age)) %>%
  rowwise() %>%
  mutate(
    mean_age = mean(c(Age_1,Age_2),na.rm=TRUE)
  )

corr_lt_p1 <- ggplot(agg_by_subj_paired,aes(mean_lt_1,mean_lt_2))+
  geom_point()+
  geom_smooth(method="lm",color="black")+
  theme_cowplot()+
  theme(legend.position="none")+
  xlab("Average Looking Time (s) in first session")+
  ylab("Average Looking Time (s) in second session")

correlation_avg_looking <- cor.test(agg_by_subj_paired$mean_lt_1,agg_by_subj_paired$mean_lt_2)

#similar results when controlling for trial number
m <- lm(mean_lt_2~mean_lt_1+N_1+N_2,data=agg_by_subj_paired)
#car::vif(m) #no really worrying variance inflation here

#model controlling for age
m_overall_correlation_age <- lm(mean_lt_2~mean_lt_1+mean_age,data=agg_by_subj_paired)
results_overall_correlation_age  <- summary(m_overall_correlation_age)
```

```{r}
#Correlations by IDS/ADS
agg_by_subj_condition_paired <- data_clean %>%
  select(Subject,Subject_Unique,Age,Gender,Method, preterm,Session, Condition, LT,Lab,days_between_sessions) %>% 
  group_by(Method, Lab,Subject, Subject_Unique, Age,Gender, preterm,days_between_sessions,Condition,Session) %>%
  summarize(mean_lt=mean(LT)) %>%
  pivot_wider(names_from = Session, values_from =  c(mean_lt, Age)) %>%
  rename(LT_Test = mean_lt_1,LT_Retest=mean_lt_2) %>%
  rowwise() %>%
  mutate(
    mean_age = mean(c(Age_1,Age_2),na.rm=TRUE)
  )

corr_lt_p2 <- ggplot(agg_by_subj_condition_paired,aes(LT_Test,LT_Retest,color=Condition))+
  geom_point()+
  geom_smooth(method="lm",color="black")+
  facet_wrap(~Condition)+
  theme_cowplot()+
  theme(legend.position="none")+
  xlab("Average Looking Time (in s) in first session")+
  ylab("Average Looking Time (in s) in second session")

agg_by_subj_condition_paired_wide <- agg_by_subj_condition_paired %>%
  pivot_wider(names_from = Condition,values_from = c(LT_Test,LT_Retest))

#condition-specific correlations
correlation_IDS_LT <- cor.test(agg_by_subj_condition_paired_wide$LT_Retest_IDS,agg_by_subj_condition_paired_wide$LT_Test_IDS)
correlation_ADS_LT <- cor.test(agg_by_subj_condition_paired_wide$LT_Retest_ADS,agg_by_subj_condition_paired_wide$LT_Test_ADS)

#controlling for age
predicting_Retest_IDS_age <- lm(LT_Retest_IDS ~ LT_Test_IDS + mean_age,data=agg_by_subj_condition_paired_wide)
predicting_Retest_ADS_age <- lm(LT_Retest_ADS ~ LT_Test_ADS + mean_age,data=agg_by_subj_condition_paired_wide)

```

To what extent are participants looking times between the two sessions related?
To test this question, we investigated whether participants' overall looking times --- irrespective of condition --- were correlated between the first and second session.
There was a robust correlation between average looking time in Session 1 and Session 2: infants with longer looking times during their first session also tended to look longer during their second session, `r apa_print(correlation_avg_looking)$full_result` (Figure 3B).
This relationship held even after controlling for number of trials (`r apa_print(m)$full_result[["mean_lt_1"]]`) and participants' average age (`r apa_print(results_overall_correlation_age)$full_result[["mean_lt_1"]]`) across the two test sessions in linear regression models.
Finally, we found similar correlations in average looking time to IDS stimuli in Session 1 and 2, `r apa_print(correlation_IDS_LT)$full_result`, and ADS stimuli in Session 1 and 2, `r apa_print(correlation_ADS_LT)$full_result` (Figure 3C; see Supplementary Materials S9 and S10 for further details, including an investigation of item-level correlations).

```{r fig3, fig.cap="(A) Correlation between the number of trials contributed in Session 1 and Session 2. Each data point represents one infant. Colored lines represent linear fits for each method. (B) Overall correlations in average looking time (in s) between Session 1 and 2. (C) Correlations in average looking time (in s) between sessions, split by IDS/ADS condition.", fig.align="center",fig.height = 14, fig.width = 12}
bottom_row <- plot_grid(corr_lt_p1,corr_lt_p2, labels=c("B","C"),ncol=2)
plot_grid(p1,bottom_row,labels=c("A",""),nrow=2,rel_heights=c(1,1))
```

# General Discussion

The current study investigated the test-retest reliability of infants' preference for IDS over ADS. 
As part of the original MB1 project, we tested the IDS preference of infants in two separate test sessions to assess the extent to which their pattern of preference would remain consistent. 
While we replicated the original effect of infants’ speech preference for IDS over ADS for both the test and retest session on the group-level, we found that infants' speech preference measures showed no evidence of test-retest reliability. 
In other words, we were unable to detect stable individual differences in infants' preference for IDS. 
This finding is consistent with past research suggesting low test-retest reliability in other infant paradigms [@cristia2016test]. 
Given that most experimental procedures conducted in infant research are interested in the comparison of groups, individual differences between participants within a specific condition are usually minimized by the experimental procedure while differences between conditions are maximized. 
Therefore, infant preference measures may be a good approach for capturing group-level phenomena, but may be less appropriate for examining individual differences in development.

Consistent with general psychometric theory [e.g.,@debolt2020robust], stricter inclusion criteria --- and consequently a larger number of included test trials per participant --- tended to increase the magnitude of the correlation between test sessions. 
However, this association was based on exploratory analyses and was in part only observed descriptively, and hence should be interpreted with caution. 
A similar effect on the group-level was found in the MB1 project, where a stricter inclusion criterion led to bigger effect sizes [@manybabies2020quantifying]. 
As in MB1, higher reliability through strict exclusions came at a high cost. 
In particular, with the strictest criterion, only a small portion of the original sample size (24 out of 158 infants) could be included in the final sample.
In other words, applying stricter criteria leads to a higher drop-out rate and can dramatically reduce the sample size. 
In the case of studies in the field of developmental science, where there are many practical restrictions in collecting large samples of infants (e.g., birth rate in the area, restricted lab capacities, budget restrictions), a strict drop-out criterion may often be difficult to implement. 
Note that studies in developmental science already have above-average drop-out rates [@miller2017developmental]. 
In addition, drop out may not be random, and so having high drop-out rates can further limit the generalizability of a study. 
In fact, the number of trials individual infants contributed was highly correlated between test sessions in the current study (see Supplementary Materials S6).
Particularly in the context of turning individual differences measures into diagnostic tools, high drop-out rates have an additional limitation of not being broadly usable.

```{r}
spearman_brown <- function(r_true, rxx, ryy) {
  r_obs <-  r_true*sqrt(rxx*ryy)
  return(r_obs)
}

cur_alpha <- icc_results_6$results[5,2]

# Create dataframe for observable r's
rxx <- c(cur_alpha,cur_alpha)
true_r <- c(.7, .3)

reliability <- data.frame(rxx, true_r) %>%
  mutate(r_obs = spearman_brown(true_r, rxx, rxx))

# r = .25, n = 122
rel_pwr_results_r.7 <- pwr.r.test(r = .25, power = .8)

# r = .107, n = 682
rel_pwr_results_r.3 <- pwr.r.test(r = .107, power = .8)
```

Even under best-case scenarios, reliability remained quite low. For example, when restricting the sample to infants contributing at least 6 trials in each condition in both sessions, we obtained a correlation of *r* = `r round(correlation_6$estimate,2)` and an intra-class correlation coefficient of $\alpha$ = `r round(icc_results_6$results[5,2],2)`. As @byers2021six outline, low measurement reliability severely restricts power for detecting relationships between measures. Using the same approach as @byers2021six, we estimate that over `r floor(rel_pwr_results_r.3$n)` infants would be needed to have at least 80% power to observe a true correlation of *r* = .3 between two measurements, assuming an intra-class correlation coefficient as large as that observed in our restricted sample ($\alpha$ = `r round(icc_results_6$results[5,2],2)`). Even a very large true correlation of *r* = .7 would require a sample size of over 120 infants. In other words, even under optimistic estimates of reliability based on strict inclusion criteria, the low reliability of IDS preference measures would severely limit the feasibility of individual difference and longitudinal research using current methods.

An alternative approach to increasing the number of valid trials is to increase the number of experimental trials. 
This approach seeks to increase the likelihood that participants will contribute sufficient trials (after trial-level exclusions) to allow for precise individual-level estimates [@debolt2020robust; see also @silverstein2021infants]. 
While this approach is promising, it may not always be feasible, because the attention span of a typical infant participant is limited. 
Therefore, prolonging the experimental procedure to maximize the absolute number of trials is often challenging in practice. 
Other avenues for obtaining higher numbers of valid trials may include changes in the procedure [e.g., @egger2020improving] or implementing multi-day test sessions [@fernald2012individual].

As our results are only based on the phenomenon of IDS preference (albeit, with three widely used methods: HPP, CF, ET) it is essential to further assess the underlying reliability of preferential looking measures within other areas of speech perception [@marimon2022]. 
While most infants prefer IDS over ADS [@dunst2012preference], patterns of preferential looking in other tasks (e.g., speech segmentation) are often inconsistent and difficult to predict [@bergmann2016development].
These inconsistencies in looking behavior are especially important to consider in the context of relating a direction of preference to later language development, and can sometimes lead to seemingly contradictory findings. 
That is, both familiarity and novelty responses have been suggested to be predictive of infants’ later linguistic abilities [@depaolis2014infants; @newman2006infants; @newman2016input].
In light of our findings, researchers conducting longitudinal studies with experimental data from young infants predicting future outcomes should be cautious, as there may be large intra-individual variability affecting preference measurement.

While we observed limited evidence for test-retest reliability using preference measures, we observed robust correlations for average looking times between session 1 and 2, both overall and for IDS and ADS stimuli considered separately (see also Supplementary Materials S9 for an investigation of item-level correlations).
This finding is consistent with past results in infant looking time studies finding robust correlations in average looking times across multiple sessions [@marimon2022].
It also raises an apparent puzzle: why are overall looking times for ADS and IDS stimuli correlated, while difference scores are not?
One explanation is that infants have stable individual differences in how long they look to stimuli, but little or no stable individual differences in their preference for one stimulus type over another.
This only partially explains the current pattern of results, however, because IDS looking time in session 1 predicted IDS looking time in session 2 even when controlling for ADS looking time, and vice versa (see S9). 
In other words, the condition-specific looking time correlations are not fully explained by overall looking behavior.
Another long-established explanation is that difference scores tend to have poor measurement reliability, because difference scores combine error from individual measurements into a composite score and increasing the ratio of error relative to the variance between participants [@hedge2018reliability;@lord_measurement_growth].
Given the limitations of difference scores (and composite scores in general), one goal for future research will be to assess the use of trial-by-trial model-based approaches for estimating reliability [@rouder_psychometrics_2019;@haines_theoretically_2020].

## Limitations
While we had an above-average sample size for a study in infant research, we were unable to approach the number of participants collected within the original MB1 study. 
In addition to a delayed call, the extra effort of having to schedule a second lab visit for each participant and the fact that there were already other collaborative studies taking place simultaneously [MB1B, @byers2021multilab; MB1G, @byers2021development], might have contributed to a low participation rate.
A higher sample size and a larger number of participating labs from different countries would have enabled us to conduct a more highly-powered test of differences in test-retest reliability across different methods, language backgrounds, and participant age.

A further limitation concerns the stimuli. 
While the order of the audio recording clips presented to infants within a given trial differed between the first and second session, the exact same stimulus material as in MB1 was used in both sessions.
In particular, all children heard the exact same voices in Session 1 and in Session 2.
From a practical point of view, this was the most straightforward solution for coordinating the experiment within the larger MB1 project.
However, familiarity effects might have influenced infants’ looking behavior. 
Infants with longer looking times in their first session might have had more opportunity to recognize familiar audio clips in their second session.
For infants with short looking times, familiar audio clips would only occur towards the end of second-session trials, thus offering infants less opportunity to recognize voices from their first session.
Therefore, inconsistent familiarity with the stimulus material in the second session across infants might have artificially lowered test-retest reliability.
Moreoever, infants' experience with a testing paradigm has been found to systematically affect looking time to familiar stimuli [@santolin_experience_2021], further complicating the interpretation of infant familiarity preferences in retest sessions.
On the other hand, one factor that mitigates this concern is that infants' looking times generally declined in session 2 compared to session 1 [consistent with past work, e.g. @marimon2022], limiting opportunities for infants to encounter previously experienced stimulus material.

# Conclusion
Following the MB1 protocol, the current study could not detect test-retest reliability in measures of infants' preference for IDS over ADS.
Subsequent analyses provided tentative evidence that stricter criteria for the inclusion of participants may enhance test-retest reliability at the cost of high drop-out rates.
Developmental studies relying on stable individual differences between their participants need to consider the underlying reliability of their measures, and we recommend a broader assessment of test-retest reliability in infant research.

\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup

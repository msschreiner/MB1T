---
title             : "Assessing test-retest reliability of the infant preference measures"
shorttitle        : "test-retest reliability of the infant preference measures"

author: 
  - name          : "Melanie S. Schreiner"
    affiliation   : "1,2"
    corresponding : yes    # Define only one corresponding author
    address       : "Gosslerstr. 14, 37073 Göttingen"
    email         : "melanie.schreiner@psych.uni-goettingen.de"
  - name          : "the ManyBabies Test-Retest Consortium"

affiliation:
  - id            : "1"
    institution   : "University of Goettingen"
  - id            : "2"
    institution   : "Leibniz Science Campus PrimateCognition"

abstract: |
  The ManyBabies1 collaborative research project (hereafter, MB1; Frank et al., 2017; ManyBabies Consortium, 2020) explores the reproducibility of the well-studied and robust phenomenon of infants' preference of infant-directed speech (hereafter, IDS) over adult-directed speech (hereafter, ADS; Cooper & Aslin, 1990). The current study is a follow-on project aiming at further investigating the test-retest reliability of infant speech preference measures. In particular, labs of the original study were asked to bring in tested babies for a second appointment retesting infants on their IDS preference. This allows us to estimate test-retest reliability for the three different methods used to investigate preferential listening in infancy: The head-turn preference procedure, central fixation, and eye tracking. Our results suggest that the test-retest reliability of infants' speech preference measures is rather low. While increasing the number of trials that infants needed to contribute for inclusion in the analysis from 2 to 8 trial pairs revealed a growth in test-retest reliability, it also considerably reduced the study's effective sample size.  Therefore, future research on infant development should take into account that not all experimental measures might be appropriate to assess individual differences between infants, and hence, the interpretation of findings needs to be treated with caution.
  
  
keywords          : "language acquisition; speech perception; infant-directed speech; adult-directed speech, test-retest reliability"
wordcount         : "3070"

bibliography      : ["r-references.bib"]

figsintext        : yes
floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library(papaja)
library(dplyr)
library(tidyr)
library(readxl)
library(lme4)
library(readr)
library(langcog)
library(ggthemes)
library(tidyverse)
library(here)
library(knitr)
library(kableExtra)
library("langcog")
library(lmerTest)

```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

Studies on infant development are dependent on the robustness of their used measures. When applying a specific method to measure an underlying meaningful construct or a latent variable of a specific individual for multiple times, we would expect similar results. If the results, however, differ significantly on every other turn, we would assume that our measure is faulty and that we are unable to assess the underlying construct. In other words. high measurement reliability is necessary for the robustness of any behavioral science. Quantifying the behavior of young children or even infants is an extraordinarily difficult endeavor as a child's behavior may often be unstable with unforeseeable behavior.

Previous attempts that have addressed the reliability of measurements are either limited to adult populations investigating various tasks [@hedge2018reliability], or are conducted with a very small sample size [@houston2007assessing]. @colombo1988infant used a paired comparison task, in which infants were familiarized with a stimulus and for the test trials presented with the familiarized and a novel stimulus side-by-side. Results indicated that infants’ novelty preference was extremely variable from task to task. Assessing infants' performance from one week to another revealed that infants' attention measures were moderately reliable. However, reliability seemed to increase with the number of tasks infants were able to complete in the younger age group suggesting that reliability is influenced by the number of assessments. In addition, infants' performance was longitudinally stable but somewhat smaller than the week-to-week reliability. While @cristia2016test also retested infant populations, they independently conducted 12 different experiments on infant speech perception at three different labs, resulting in different implementations of the individual studies. Hence, it was only after completed data collection that the data was pooled together by the different labs revealing potential confounds. Nevertheless, the results showed that reliability was extremely variable across the different experiments or labs. Based on this finding, we cannot make specific predictions about test-retest reliability in the present study.

In line with the recent crisis in Psychology and the inability to reproduce different findings [@klein2014; @open2015estimating], the ManyBabies collaborative research project has been launched to assess power and replicability in the field of developmental psychology [@frank2017collaborative]. The first project of the ManyBabies1 collaborative research project (hereafter, MB1) confirmed the reproducibility of the well-studied phenomenon of infants' preference of infant-directed speech (hereafter, IDS) over adult-directed speech [hereafter, ADS; @cooper1990preference]. The current follow-on project is aimed at further investigating the test-retest reliability of infant preference measures. In addition to MB1, the different participating labs retested infants' IDS preference on a second appointment, allowing to compare test-retest reliability for three different methods that have been used to investigate preferential listening in infancy: The head-turn preference procedure, central fixation, and eye tracking. The current project will help to identify the reliability of infant preference measures to further explore the properties of experimental approaches in developmental research.

Paragraph on IDS/ADS

Against this background, the current study aims to explore how reliable infants' performance across two different test days is. In particular, we examine whether infants' preferential listening behavior to IDS and ADS sequences is reliable across two different test sessions. Using central fixation, eye-tracking, and head-turn preference procedure (HPP), the current study also explores whether there are any differences in test-retest reliability of the three widely used methods which would be crucial for future work in the field of early language acquisition. In addition, we aim to address if time between test and retest or infants’ language background influences the reliability of the preference measure.

## Disclosures

Prior to the start of data collection, we preregistered the current study on the Open Science Framework (OSF; see https://osf.io/v5f8t).


# Method

A call was issued to all labs participating in the original MB1 study on January 24th, 2018. The collection of retest session data was initially set to end on May 31st, 2018, one month after the end date of the original MB1 project. Due to the fact that the original MB1 project extended the time frame for data collection and the late start of data collection for the MB1 test-retest study, we also allowed participating labs to continue data collection past the scheduled end date.

## Participants

```{r,message=FALSE,warning=FALSE}
# importing dataset
df_all<-read_csv(here("data","processed","df_all.csv"))

# Calculating the variable days between the two test sessions
df_all<-df_all %>% group_by(Subject,Lab) %>% mutate(days_between_sessions=max(Age)-min(Age)) %>%
  ungroup() 

# Truncating the looking times per trial at 18 seconds

df_all<-df_all %>% mutate(LT=ifelse(LT>18,18,LT))

# Fixing different spelling issues for the method central fixation
df_all<-df_all %>% mutate(Method=as.character(Method),Method=ifelse(Method%in% c("single-screen","SingleScreen","single screen","Single Screen"),"central fixation",Method))


# Excluding all children with session errors
excludeID<-unique(df_all$Subject[df_all$session_error=="error"])

df_all<-df_all %>% filter(!Subject%in%excludeID)

# Relabeling preterm values "N" and "Y" into "term" and "preterm"
df_all<-df_all %>% mutate(preterm=as.character(preterm),
                          preterm=ifelse(preterm=="N","term",
                                          ifelse(preterm=="Y","preterm",preterm)))

#not needed as there are no premies? Checking preterm babies from Priya
#exclude preterms
#excludePreterm<-unique(df_all$Subject[df_all$preterm=="preterm"])
#df_all<-df_all %>% filter(preterm!="preterm")

# exclude trials with trial error

df_all<-df_all %>% filter(!trial_error%in%c("error","erorr"))

# number of labs
n_labs = length(unique(df_all$Lab))
# number of excluded infants
n_exlude = length(excludeID)

```

```{r,message=FALSE,warning=FALSE}

lt_min <- 2                # minumum LT for inclusion
z_threshold <- 3           # outlier threshold (sds)
min_trials_per_type <- 1  # min trials per type for inclusion

# cleaning data by using minimum LT for inclusion, outlier threshold and minimum number of 1 trial per condition
# log-transformation of LT
data_clean <- df_all %>%
  filter(LT >= lt_min) %>%
  group_by(Subject,Session,Lab) %>%
  mutate(log_lt = log(LT), 
         .scaled_log_lt = as.numeric(langcog::scale(log_lt))) %>%
  filter(abs(.scaled_log_lt) < z_threshold) %>%
  group_by(Subject,Session,Lab) %>%
  mutate(.N_IDS = sum(Condition == "IDS"),
         .N_ADS = sum(Condition == "ADS")) %>%  
  mutate(N_IDS = sum(Condition == "IDS"),
         N_ADS = sum(Condition == "ADS")) %>%
  filter(.N_IDS >= min_trials_per_type & 
           .N_ADS >= min_trials_per_type) %>%
  select(-starts_with("."))


# aggregating log_lt for each Subject, Session and Condition
agg_subjects <- data_clean %>%
  group_by(Lab, Method, Subject, Language, Condition, Age,Session) %>%
  summarise(MeanLogLT = mean(log_lt)) %>%
  mutate(ConditionC = ifelse(Condition == "IDS", .5, -.5)) %>%
  mutate(Native = ifelse(Language == "American English", TRUE, FALSE))

# creating a wide format of the dataset and calculating difference in log_lt for IDS and ADS and the proportion of IDS looking
all_agg_subjects_paired <- agg_subjects %>%
  select(-ConditionC) %>% 
  spread(Condition,MeanLogLT) %>%
  mutate(Diff = IDS - ADS, 
         Prop = IDS / (IDS + ADS))

# creating a wide format of the dataset that the difference scores for each Session become a separate variable, calculating the number of days between test and retest session, and mean Age for both testing sessions
all_agg_subjects_paired_retest<-all_agg_subjects_paired%>%
  group_by(Lab,Subject,Method,Language) %>%  spread(Session,Diff) %>% 
  mutate(days_between_sessions=max(Age)-min(Age)) %>% 
  summarise(Diff_1=mean(`1`,na.rm=T),Diff_2=mean(`2`,na.rm=T),days_between_sessions=mean(days_between_sessions),Age=mean(Age,na.rm=T))

# filtering subjects that have no data for the difference scores
all_agg_subjects_paired_retest<-all_agg_subjects_paired_retest %>% 
  filter(!is.na(Diff_1)) %>% filter(!is.na(Diff_2))

# filtering out those subjects in the paired data frame
all_agg_subjects_paired<-all_agg_subjects_paired %>% filter(Subject %in%all_agg_subjects_paired_retest$Subject )

# Getting the number of final participants
n = length(unique(all_agg_subjects_paired$Subject)) # Final sample of participants
n_exlude_session = length(unique(df_all$Subject))-n # Calculating Participants lost due to missing valid trial in either session 1 or session 2

# mean, minimum, and maximum age across both testing sessions
age_stats<-all_agg_subjects_paired_retest %>% ungroup() %>%select(Age) %>% summarise_at(vars(Age),funs(mean,min,max))

# Stats on number of days between test and retest
stats_daysbetween <- all_agg_subjects_paired_retest %>% ungroup() %>% select(days_between_sessions) %>% summarise_at(vars(days_between_sessions),funs(mean,min,max))

```

Contributing labs were asked to re-recruit their monolingual participants between the ages of 6 to 12 months who had already participated in the MB1 project. If participating labs had not committed to testing either of these age groups, they were also allowed to re-recruit participants from the youngest age group of 3- to 6-month-olds and/or the oldest age group of 12- to 15-month-olds. Labs were asked to contribute half (*n*=16) or full blocks (*n*=32), however, a lab's data was included in the study regardless of the number of non-discard kids .The study was approved by each lab's respective ethics committee and parental consent was obtained for each infant prior to participation in the study.
Our final sample consisted of `r n` monolingual infants from `r n_labs` different labs. In order to be included in the study, infants needed a minimum of 90% first language exposure, to be born full term with no known developmental disorders, and normal hearing and vision. We had to exclude `r n_exlude` participants due to session errors and `r n_exlude_session` participants did not have at least one valid trial at their first or second session. The mean age of infants included in the study was `r round(age_stats$mean, 0)` days (range: `r age_stats$min` -- `r age_stats$max` days). Further information on labs and participants are provided in Table 1.

## Materials

### Visual stimuli.
The visual stimuli are identical to MB1. For the central fixation paradigm and eye-tracking, labs were asked to use a multicolored static checkerboard as fixation stimulus as well as a multicolored moving circle with a ringing sound as an attention getter to reorient infants toward the screen in between trials. Labs using the HPP method were instructed to use whatever was part of their common procedure.

### Speech stimuli.
We used the identical training stimuli of piano music from MB1. However, a second set of naturalistic IDS and ADS recordings of mothers either talking to their infant or to an experimenter was created for the retest session. Eight sequences of natural IDS and eight sequences of natural ADS with a length of 18 seconds each were constructed by reversing the order of clips within each sequence of the original study. This way we wanted to prevent infants who still remembered the stimuli from their first test session from easily getting bored.
 
### Procedure.

Infants were retested using the identical procedure as during the first testing day: central fixation, HPP, or eye-tracking. Participating labs were asked to ideally schedule test and retest session 7 days apart with a minimum number of 1 day and a maximum number of 31 days. Three infants whose time between test and retest exceeded 31 days were also included in the analyses. The mean number of days between test and retest was `r round(stats_daysbetween$mean, 1)` (range: `r stats_daysbetween$min` to `r stats_daysbetween$max` days).
A total of 18 trials, including two training, eight IDS, and eight ADS trials, were presented in one of four pseudo-randomized orders. Trial length was either infant-controlled or fixed depending on the lab's standard procedure. A trial started after the infant had fixated the screen for 2 seconds. A trial stopped either if the infant looked away for 2 seconds or after the total trial duration of 18 seconds. The experimenter and the parent were blinded through music masked with the stimuli of the study via noise-cancelling headphones. If the experimenter was in an adjacent room, blinding was optional for the online coding experimenter.
 
### Data exclusion.
A child was excluded if they had a session error. Trials were excluded if they were marked as trial error or if the minimum looking time of 2 s was not met. If a participant was unable to contribute at least one IDS and one ADS trial for either test or retest, all data of that participant was excluded from the test-retest analyses.

# Results

All analyses were performed on data of infants who contributed at least 2 non-discard trials, i.e., one non-discard IDS and one non-discard ADS trial.


```{r labs, results="asis", message=FALSE, warning=FALSE}

# Summarize mean Age and number of Subjects for each lab, method, and language
lab_stats<-all_agg_subjects_paired_retest %>% select(Lab,Subject,Method,Age,Language) %>% group_by(Lab,Method,Language) %>% 
  summarise(Age=mean(Age),N=length(unique(Subject)))

# Assign column label to column 4
names(lab_stats)[4]<-"Mean age (days)"

# Create data set that includes language variable from main data set
#df_all_Language<-df_all %>% select(Lab,Method,Language) %>% group_by(Lab) %>% 
  #summarise(Method=as.character(unique(Method)[1]),Language=unique(Language[1]))

# Merge with lab stats
#lab_stats<-left_join(lab_stats,df_all_Language)

papaja::apa_table(lab_stats, 
                  caption = "Statistics of the included labs. n refers to the number of infants included in the final analysis.", 
                  format.args = list(digits = 0))

```

## IDS preference

```{r,message=FALSE,warning=FALSE}
# Linear mixed model assessing if session predicts differences in LT 
diff_session <- lmer(Diff ~ Session + 
                (1+Session | Lab), 
              data=all_agg_subjects_paired, REML=FALSE)

diff_session_coefs<-summary(diff_session)$coefficients

# Subsetting to session 1 for t-test
all_agg_subjects_paired_s1 <- subset(all_agg_subjects_paired, all_agg_subjects_paired$Session=="1")

ttestSession1<-t.test(all_agg_subjects_paired_s1$Diff)

# Subsetting to session 2 for t-test

all_agg_subjects_paired_s2 <- subset(all_agg_subjects_paired, all_agg_subjects_paired$Session=="2")

ttestSession2 = t.test(all_agg_subjects_paired_s2$Diff)

# Hier berechnen wir das Durchschnittsalter auf Grundlage beider Testzeitpunkte... Kann raus, oder?
#age_stats_paired<-all_agg_subjects_paired %>% ungroup() %>% select(Age) %>% #summarise_at(vars(Age),funs(mean,min,max))

```

First, we examined infants' preferences of IDS in both sessions. Two two-samples t-tests revealed that the children in Session 1, `r apa_print(ttestSession1)$statistic`, and in Session 2, `r apa_print(ttestSession2)$statistic`, showed a preference of IDS over ADS. In the first session, `r sum(all_agg_subjects_paired_s1$Diff>0)/length(all_agg_subjects_paired_s1$Diff)*100` percent of the children showed a preference for IDS, and in the second session, `r sum(all_agg_subjects_paired_s2$Diff>0)/length(all_agg_subjects_paired_s2$Diff)*100` percent of the children showed a preference for IDS. In other words, we replicated the previous finding from the main MB1 study. There was no difference in the strength of the preference effect, as a multilevel analysis with a random slope and random intercept for session on the lab level revealed no impact of session on infants' preference, $\beta$=`r round(diff_session_coefs[2,1],3)`, SE=`r round(diff_session_coefs[2,2],2)`, *p*=`r printp(diff_session_coefs[2,5])`.

## Reliability

```{r coef_table2, results="asis",warning=FALSE,message=FALSE}
# Linear mixed model predicting difference score of session 2 from difference score of session 1
mod_lmer <- lmer(Diff_2 ~ Diff_1 + (1|Lab),
              data=all_agg_subjects_paired_retest,REML=T)

# Adding days between sessions to model
mod_lmer2 <- lmer(Diff_2 ~ Diff_1*days_between_sessions+ (1|Lab), # Effect of days between sessions
              data=all_agg_subjects_paired_retest,REML=T)

days_coefs<-summary(mod_lmer2)$coefficients

# Differences in test-retest reliability between English and non-English learning infants
# Assigning 1 to UBC and Madison as NAE labs. Remaining labs are non-NAE and are coded as 0.
all_agg_subjects_paired_retest$NAE<-ifelse(all_agg_subjects_paired_retest$Lab=="InfantCog-UBC"| all_agg_subjects_paired_retest$Lab=="infantll-madison", 1, 0)

# Adding NAE to model
mod_lmer3 <- lmer(Diff_2 ~ Diff_1*NAE+(1|Lab),
              data=all_agg_subjects_paired_retest,REML=T) 

nae_coefs<-summary(mod_lmer3)$coefficients

# Coefficients for linear mixed model predicting difference score of session 2 from difference score of session 1
coefs <- summary(mod_lmer)$coef %>%
  as_data_frame %>%
  mutate_at(c("Estimate","Std. Error","df", "t value", "Pr(>|t|)"), 
            function (x) signif(x, digits = 3)) %>%
  rename(SE = `Std. Error`, 
         t = `t value`,
         p = `Pr(>|t|)`) %>%
  select(-df)
         
rownames(coefs) <- c("Intercept", "Session One")

papaja::apa_table(coefs, 
                  caption = "Coefficient estimates from a linear mixed effects model predicting IDS preference in Session 2.", 
                  format.args = list(digits = 3))

```

```{r}
# calculate simple correlation of difference score of session 1 and difference score of session 2
simplyOverallCorr<-cor.test(all_agg_subjects_paired_retest$Diff_1,
         all_agg_subjects_paired_retest$Diff_2)
```

We assessed test-retest reliability in two ways. First, we conducted a multilevel analysis, with Lab as random intercept, predicting the IDS preference in Session 2 based on the IDS preference in Session 1. The results revealed that we could not predict the preference score in Session 2 based on Session 1 (see Table 2). Second, we calculated the Pearson correlation coefficient. While a simple correlation coefficient might overestimate the test-retest reliability in our sample because it does not control for the differences between different labs, we felt it was important to also conduct a Pearson correlation as it is commonly used to assess reliability. Again, the size of the correlation coefficient was not statistically different from zero, `r apa_print(simplyOverallCorr)$full_result`. Taken together, our results reveal no overall test-retest reliability for the three infant preference measures used within the current study.
To test whether the results were different for a specific method, we calculated the Pearson correlation coefficients and the multilevel analyses for the three different methods, HPP, central fixation and eye-tracking, separately (see Table 3). Splitting the data per method, did also lead to no different results. Neither the Pearson correlation coefficients nor the coefficients of the multilevel analysis were significant, all *p-values* > .143.
We also tested for the possibility that the Time between sessions might have an impact on the reliability. The subsequent multilevel analysis, with Lab as random intercept, predicting the IDS preference in Session 2 based on the IDS preference in Session 1, the number of days between Session 1 and Session 2 and the interaction of these two variables, did not indicate that Time between sessions had an effect. Neither the main effect of Time between sessions, $\beta$=`r round(days_coefs[3,1],3)`, SE=`r round(days_coefs[3,2],2)`, *p*=`r printp(days_coefs[3,5])`, nor the interaction term, $\beta$=`r round(days_coefs[4,1],3)`, SE=`r round(days_coefs[4,2],2)`, *p*=`r printp(days_coefs[4,5])`, showed significant effects.
As NAE-learning infants showed a greater IDS preferences than their non-NAE counterparts in the original study, we also assessed if test-retest reliability interacted with children's native language. A multilevel analysis with Lab as random intercept, predicting the IDS preference in Session 2 based on the IDS preference in Session 1, NAE and the interaction of these two variables, revealed no main effect of NAE, $\beta$=`r round(nae_coefs[3,1],3)`,  SE=`r round(nae_coefs[3,2],2)`, *p*=`r printp(nae_coefs[3,5])`, and no interaction, $\beta$=`r round(nae_coefs[4,1],3)`, SE=`r round(nae_coefs[4,2],2)`, *p*=`r printp(nae_coefs[4,5])` (see Figure 1).

```{r results="asis",warning=FALSE,message=FALSE}

# Computing lmm for each method separately
Method<-unique(all_agg_subjects_paired_retest$Method)
estimate<-rep(NA,3)
SE<-rep(NA,3)
pvalue<-rep(NA,3)
cor<-rep(NA,3)
pvalue2<-rep(NA,3)

df<-as.data.frame(cbind(Method,estimate,SE,pvalue,cor,pvalue2))
df[,2:6]<-as.numeric(as.character(df[,2:6]))

counter = 0
for (M in unique(all_agg_subjects_paired_retest$Method)){
  
  counter=counter+1

  data<-all_agg_subjects_paired_retest[all_agg_subjects_paired_retest$Method==M,]
  mod_lmer <- lmer(Diff_2 ~ Diff_1 + (1|Lab),
              data=data,REML=T)


coefs <- summary(mod_lmer)$coef %>%
  as_data_frame %>%
  mutate_at(c("Estimate","Std. Error","df", "t value", "Pr(>|t|)"), 
            function (x) signif(x, digits = 3)) %>%
  rename(SE = `Std. Error`, 
         t = `t value`,
         p = `Pr(>|t|)`) %>%
  select(-df)
         
rownames(coefs) <- c("Intercept", "Session One")
  
  correlation<-cor.test(data$Diff_1,data$Diff_2)
  
  df$estimate[counter]<-as.numeric(coefs$Estimate)[2]  
  df$SE[counter]<-as.numeric(coefs$SE)[2]
  df$pvalue[counter]<-as.numeric(coefs$p)[2]
  df$cor[counter]<-as.numeric(correlation$estimate)
  df$pvalue2[counter]<-as.numeric(correlation$p.value)
}

papaja::apa_table(df, 
                  caption = "Coefficient estimates from a linear mixed effects model predicting IDS preference in Session 2 for each method separately.", 
                  format.args = list(digits = 3))

```
```{r fig1, fig.cap = "Infants' preference in Session 1 and Session 2 with individual data points and regression lines color-coded by method (central fixation, eye tracking, or HPP). Results are plotted separately for North American English-learning infants (right panel) and infants learning other languages and dialects (right panel)."}

# Plotting IDS preference in Session 1 and Session 2 split by NAE
NAE.labs <- c("non-NAE", "NAE")
names(NAE.labs) <- c("0", "1")

all_agg_subjects_paired_retest$NAE <- as.factor(all_agg_subjects_paired_retest$NAE)

ggplot(all_agg_subjects_paired_retest, aes(x = Diff_1, y = Diff_2, col = Method)) + 
  geom_point() +
  geom_smooth(method = "lm", se=FALSE) + 
 # geom_hline(yintercept = 0, lty = 2) + 
  facet_grid(~NAE, labeller=labeller(NAE=NAE.labs))+
  ylab("IDS preference in second session") + 
  scale_color_ptol(name = "Method") + 
  #scale_linetype(name = "North American English") + 
  xlab("IDS preference in first session") + 
 # ylim(-3, 5) + 
  theme(legend.position = "bottom", legend.box = "vertical",panel.background = element_rect(fill = "white",
                                colour = "black",
                                size = 0.5, linetype = "solid"),
  panel.grid.major = element_blank(), 
  panel.grid.minor = element_blank(),
  strip.background = element_blank())

```

```{r,message=FALSE,warning=FALSE}

# Creating data sets for different number of minimum trial pairs included: 2, 4, 6, and 8
min_trials<-c(2,4,6,8) # define min_trials for Loop

for (i in min_trials){
  
  lt_min <- 2                # minumum LT for inclusion
  z_threshold <- 3           # outlier threshold (sds)
  min_trials_per_type <- i  # min trials per type for inclusion


data_clean <- df_all %>%
  filter(LT >= lt_min) %>%
  group_by(Subject,Session,Lab) %>%
  mutate(log_lt = log(LT), 
         .scaled_log_lt = as.numeric(langcog::scale(log_lt))) %>%
  filter(abs(.scaled_log_lt) < z_threshold) %>%
  group_by(Subject,Session,Lab) %>%
  mutate(.N_IDS = sum(Condition == "IDS"),
         .N_ADS = sum(Condition == "ADS")) %>%  
  mutate(N_IDS = sum(Condition == "IDS"),
         N_ADS = sum(Condition == "ADS")) %>%
  filter(.N_IDS >= min_trials_per_type & 
           .N_ADS >= min_trials_per_type) %>%
  select(-starts_with("."))

agg_subjects <- data_clean %>%
  group_by(Lab, Method, Subject, Language, Condition, Age,Session) %>%
  summarise(MeanLogLT = mean(log_lt)) %>%
  mutate(ConditionC = ifelse(Condition == "IDS", .5, -.5)) %>%
  mutate(Native = ifelse(Language == "American English", TRUE, FALSE))

agg_subjects_paired <- agg_subjects %>%
  select(-ConditionC) %>% 
  spread(Condition,MeanLogLT) %>%
  mutate(Diff = IDS - ADS, 
         Prop = IDS / (IDS + ADS))

all_agg_subjects <- data_clean %>%
  group_by(Lab, Method, Session, Subject, Language, Condition, Age) %>%
  summarise(MeanLogLT = mean(log_lt)) %>%
  mutate(ConditionC = ifelse(Condition == "IDS", .5, -.5)) %>%
  mutate(Native = ifelse(Language == "American English", TRUE, FALSE))

all_agg_subjects_paired <- all_agg_subjects %>%
  select(-ConditionC) %>%
  spread(Condition, MeanLogLT) %>%
  mutate(Diff = IDS - ADS, 
         Prop = IDS / (IDS + ADS))

# notperfect, but works
all_agg_subjects_paired_retest<-all_agg_subjects_paired %>% spread(Session,Diff) %>% 
  group_by(Lab,Subject,Method,Language) %>%
  mutate(days_between_sessions=max(Age)-min(Age)) %>% 
  summarise(Diff_1=mean(`1`,na.rm=T),Diff_2=mean(`2`,na.rm=T),days_between_sessions=mean(days_between_sessions))

all_agg_subjects_paired_retest<-all_agg_subjects_paired_retest %>% 
  filter(!is.na(Diff_1)) %>% filter(!is.na(Diff_2))

all_agg_subjects_paired<-all_agg_subjects_paired %>% filter(Subject %in%all_agg_subjects_paired_retest$Subject )

all_agg_subjects_pairedsum<-all_agg_subjects_paired %>% group_by(Lab) %>% 
  summarise(N=n())


overallCorrelation<-cor.test(all_agg_subjects_paired_retest$Diff_1,
         all_agg_subjects_paired_retest$Diff_2)

eval(parse(text = paste('simplyOverallCorr',i, ' = overallCorrelation', sep='')))
eval(parse(text = paste('all_agg_subjects_paired_retest',i, ' = all_agg_subjects_paired_retest', sep='')))
  
}

```

## Results with different inclusion criteria
To this point, all analyses were performed on data with the inclusion criteria from MB1. For this, infants needed only 1 out of 8 valid trial pairs to be included in the analyses. Given that the use of more stringent inclusion criteria yielded larger effects sizes within the original MB1 study, we also assessed test-retest reliability by applying stricter inclusion criteria and thereby increasing test length to 2, 4, 6, and 8 non-discard test trials. Applying a stricter criterion and thereby increasing test length, increased descriptively the reliability (Figure 2). In particular, while neither the correlation coefficient based on the inclusion criterion of 2 valid trials, `r apa_print(simplyOverallCorr2)$statistic `, the correlation coefficient based on the inclusion criterion of 4 valid trials, `r apa_print(simplyOverallCorr4)$statistic `, nor the correlation coefficient based on the inclusion criterion of 6 valid trials, `r apa_print(simplyOverallCorr6)$statistic `, were significant, the correlation coefficient based on the inclusion criterion of 8 valid trials - meaning a child had to have no discard trials on both testing days - revealed a significant result, `r apa_print(simplyOverallCorr8)$statistic `. Due to the small sample size and a missing type-1 error correction, we note that this result needs to be treated with caution. Nevertheless, the analyses show that a more stricter inclusion criteria might have led to higher test-retest reliability but at the same time comes with tremendous decreases in sample size.

```{r fig.cap="IDS preferences of both sessions plotted against each other for each inclusion criterion. n indicates the number of included infants, r is the Pearson correlation coefficient as the indicator for reliability.", fig.align="center",out.width = "5in",fig.height = 12, fig.width = 8}

# Creating scatterplot for each inclusion criterion

Limaxes<-c(-0.5,1.5)

par(mfrow = c(2, 2))
plot(all_agg_subjects_paired_retest2$Diff_1,all_agg_subjects_paired_retest2$Diff_2,xlab = "IDS preference in first session",ylab= "IDS preference in second session",main = "Inclusion criterion: 2 trial pairs",xlim = Limaxes,ylim = Limaxes)
abline(lm(all_agg_subjects_paired_retest2$Diff_2~all_agg_subjects_paired_retest2$Diff_1),col="red")
legend("topleft",legend = paste("r = ",round(simplyOverallCorr2$estimate,2),"; n = ",simplyOverallCorr2$parameter+2,sep=""))


plot(all_agg_subjects_paired_retest4$Diff_1,all_agg_subjects_paired_retest4$Diff_2,xlab = "IDS preference in first session",ylab= "IDS preference in second session",main = "Inclusion criterion: 4 trial pairs",xlim = Limaxes,ylim = Limaxes)
abline(lm(all_agg_subjects_paired_retest4$Diff_2~all_agg_subjects_paired_retest4$Diff_1),col="red")
legend("topleft",legend = paste("r = ",round(simplyOverallCorr4$estimate,2),"; n = ",simplyOverallCorr4$parameter+2,sep=""))

plot(all_agg_subjects_paired_retest6$Diff_1,all_agg_subjects_paired_retest6$Diff_2,xlab = "IDS preference in first session",ylab= "IDS preference in second session",main = "Inclusion criterion: 6 trial pairs",xlim = Limaxes,ylim = Limaxes)
abline(lm(all_agg_subjects_paired_retest6$Diff_2~all_agg_subjects_paired_retest6$Diff_1),col="red")
legend("topleft",legend = paste("r = ",round(simplyOverallCorr6$estimate,2),"; n = ",simplyOverallCorr6$parameter+2,sep=""))

plot(all_agg_subjects_paired_retest8$Diff_1,all_agg_subjects_paired_retest8$Diff_2,xlab = "IDS preference in first session",ylab= "IDS preference in second session",main = "Inclusion criterion: 8 trial pairs",xlim = Limaxes,ylim = Limaxes)
abline(lm(all_agg_subjects_paired_retest8$Diff_2~all_agg_subjects_paired_retest8$Diff_1),col="red")
legend("topleft",legend = paste("r = ",round(simplyOverallCorr8$estimate,2),"; n = ",simplyOverallCorr8$parameter+2,sep=""))

```

# General Discussion

The current study set out to explore test-retest reliability of infant preference measures which are common experimental paradigms used to study infant cognition. Infants of the original MB1 project were retested on a reversed order of stimuli in order to assess if their listening pattern would be similar to that of their initial assessment. While we replicated the original effect of infants' speech preference for IDS over ADS in the current MB1 follow-up study for both test and retest session using the same MB1 protocol, we did find that infants' speech preference measures had no test-retest reliability. In other words, we were unable to detect any stable individual differences of infants' speech preference. This finding is in line with other research indicating a rather low test-reliability for different developmental paradigms [@cristia2016test]. Given that most experimental procedures conducted in developmental research are interested in the comparison of groups, individual differences between participants within a specific condition are usually minimized by the experimental procedure while differences between conditions are maximized. Therefore, infant preference measures may be a good approach to capture universal phenomena but do not seem to be appropriate for examining factors that may lead to differences in development.
Nevertheless, we also detect the effect that increasing test length enhances reliability of the measures, which in turn leads to higher test-retest reliability. A similar effect was found in the MB1 project, where a stricter inclusion criterion led to bigger effect sizes [@manybabies2020quantifying]. As in the MB1 original study, higher reliability came at high costs. In particular, due to this strict criterion, only a small portion of the original sample size that is 21 out of 154 infants could be included in the final sample for the analyses. In other words, applying an even stricter criterion leads to an even higher drop out rate and reduces the actual sample sizes enormously. In the case of studies in the field of developmental science, where there are many practical restrictions to collect large samples of infants (e.g., birth rate in the area, restricted lab capacities, budget restrictions), a strict drop out criterion might not be easy - if even possible at all - to implement. Note that studies in developmental science already have above average drop out rates [@miller2017developmental].
An alternative approach to increase the number of valid trials might be to also increase the number of collected trials. In this case, a participant can have a high number/proportion of invalid trials and still be included into the final sample as the absolute number of trials is high and thereby decreasing trial-to-trial variability [@debolt2020robust]. While this approach might sound promising, it must be seen if this is realistic, because the attention span of a typical participant of a developmental study is rather short. Therefore, prolonging the experimental procedure to maximize the absolute number of trials might also be practically challenging.
As our results are only based on specific experimental procedure, still with three widely used methods (HPP, central fixation; eye-tracking), developmental studies. need to test the underlining reliability of their measures. Especially, researchers conducting longitudinal studies with experimental data from young infants should be cautious. 
One potential explanation for the different behavioral responses to IDS and ADS sequences on the two different testing days may be related to infants' previous experience with laboratory work. As @santolin2021experience found out, the amount of studies that infants had already participated in may impact their looking patterns with a typical familiarity response for first time visitors moving to preferences of novel items with increasing number of visits.

## Limitations
While we had an above average sample size for a study in developmental research, we were unable to reach the number of participants collected within the original MB1 study. In addition to a delayed call, the extra effort of having to schedule a second lab visit for each participant and the fact that there were already other collaborative studies taking place simultaneously [@byers2021multilab; @byers2021development], might have contributed to the rather low turnout. A higher sample size and a larger number of participating labs from different countries might have enabled us to test for possible differences of the test-retest reliability of the different methods (HPP, central fixation, eye-tracking) and NAE versus non-NAE language backgrounds. Further, a larger sample size might have enabled us to conduct meaningful moderator effects such as age of the child or the effect of native vs. non-native children on the test-retest reliability.
A further limitation concerns the stimuli. While the order of the stimuli presented to the participating children in the second session was different than in the first session, the exact same stimuli as in MB1 were used in both sessions. In particular, all children heard the exact same voices in Session 1 and in Session 2. From the practical point of view, it was the easiest solution. However, familiarity effects might have played a role for the children. Assuming that only some children might have recognized the voices in Session 2 from their session a week ago, some children might not have done so, for some children a familiarity effect might have happened.

# Conclusion
Following the MB1 protocol, the current study could not detect test-retest reliability of infants' preference measures for IDS over ADS. Subsequent analyses showed that a stricter criterion for the inclusion of data points enhances the test-retest reliability at the cost of high drop out rates. Developmental studies, which rely on stable individual differences of their participants, need to consider the underlying reliability of their measures.

\newpage

# Data and materials availability statement

The data and materials that support the findings of the current study are openly available on OSF at https://osf.io/ZEQKA/.

\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
